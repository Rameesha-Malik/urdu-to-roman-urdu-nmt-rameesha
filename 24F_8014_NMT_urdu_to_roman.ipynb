{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Project1: Neural Machine Translation – Urdu to Roman Urdu"
      ],
      "metadata": {
        "id": "CVD11YwbiHvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective**\n",
        "\n",
        "---\n",
        "Build\ta\tsequence-to-sequence\tmodel\tusing\ta\tbidirectional\tLSTM\t(BiLSTM)\tencoder-decoder\tto\ttranslate\tUrdu\ttext\tinto\tits\tRoman\tUrdu transliteration.\tThe\tgoal\tis\tto\texperiment\twith data\tfrom\turdu_ghazals_rekhta\tand\tpush\tthe\tlimits\tof\twhat\tBiLSTM-based\tNMT\tcan achieve for\tlow-resource,\tpoetic\ttext."
      ],
      "metadata": {
        "id": "yLNmZBcuimdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**\n",
        "\n",
        "---\n",
        "You\twill\tuse\tthe\t*urdu_ghazals_rekhta*\tdataset:\n",
        "https://github.com/amir9ume/urdu_ghazals_rekhta"
      ],
      "metadata": {
        "id": "SrmITYMejP_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Drive**"
      ],
      "metadata": {
        "id": "9vLiMzebM1iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6Ivy3AhzEWD",
        "outputId": "2d224f2f-945e-4d2e-841a-6246b39f18bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**view dataset folders**\n",
        "\n",
        "mainfolders -> Subfolders"
      ],
      "metadata": {
        "id": "w6SLpI5pN0_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**poets**"
      ],
      "metadata": {
        "id": "JzrlOX-1OSvj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZN4wQJVbgXS",
        "outputId": "dd27cd8c-4035-4328-c2a2-9f064d98e29f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 135\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[0m\u001b[01;34mahmad-faraz\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34makbar-allahabadi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mallama-iqbal\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34maltaf-hussain-hali\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mameer-khusrau\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mbahadur-shah-zafar\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mdagh-dehlvi\u001b[0m/\n",
            "-rw------- 1 root root 14340 Oct 11 10:15 .DS_Store\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mfahmida-riaz\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mfaiz-ahmad-faiz\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mfiraq-gorakhpuri\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mgulzar\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mhabib-jalib\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mjaan-nisar-akhtar\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mjaun-eliya\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mjaved-akhtar\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mjigar-moradabadi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mkaifi-azmi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mmeer-anees\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mmeer-taqi-meer\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mmirza-ghalib\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mmohsin-naqvi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mnaji-shakir\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mnaseer-turabi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mnazm-tabatabai\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mnida-fazli\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mnoon-meem-rashid\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mparveen-shakir\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34msahir-ludhianvi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mwali-mohammad-wali\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mwaseem-barelvi\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls -la /content/drive/MyDrive/ANLP/project_1/dataset | head -n 40\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**langauge**\n",
        "according to our project requiremnt we only need **en** and **ur**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   en->roman urdu\n",
        "*   ur->actual urdu\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H44j-aVTOWa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls -la /content/drive/MyDrive/ANLP/project_1/dataset/ahmad-faraz | head -n 40\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ngRCAY_4KTk",
        "outputId": "eb187792-f56d-49a6-e791-d14bd362b314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 19\n",
            "-rw------- 1 root root 6148 Oct 11 10:15 .DS_Store\n",
            "drwx------ 2 root root 4096 Oct 11 10:16 \u001b[0m\u001b[01;34men\u001b[0m/\n",
            "drwx------ 2 root root 4096 Oct 11 10:16 \u001b[01;34mhi\u001b[0m/\n",
            "drwx------ 2 root root 4096 Oct 11 10:16 \u001b[01;34mur\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**poets -> en-ghazal**"
      ],
      "metadata": {
        "id": "nfQEnz0KO-8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls -la /content/drive/MyDrive/ANLP/project_1/dataset/ahmad-faraz/en | head -n 40\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cTJjlke4YcN",
        "outputId": "92cc8d12-42c0-4000-e983-7348df4640eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 49\n",
            "-rw------- 1 root root  476 Oct 11 10:15 aankh-se-duur-na-ho-dil-se-utar-jaaegaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  709 Oct 11 10:15 aashiqii-men-miir-jaise-khvaab-mat-dekhaa-karo-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  473 Oct 11 10:15 ab-aur-kyaa-kisii-se-maraasim-badhaaen-ham-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  977 Oct 11 10:15 abhii-kuchh-aur-karishme-gazal-ke-dekhte-hain-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  650 Oct 11 10:15 ab-ke-ham-bichhde-to-shaayad-kabhii-khvaabon-men-milen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1408 Oct 11 10:15 ab-ke-tajdiid-e-vafaa-kaa-nahiin-imkaan-jaanaan-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  640 Oct 11 10:15 ab-kyaa-sochen-kyaa-haalaat-the-kis-kaaran-ye-zahr-piyaa-hai-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  574 Oct 11 10:15 ab-shauq-se-ki-jaan-se-guzar-jaanaa-chaahiye-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  780 Oct 11 10:15 agarche-zor-havaaon-ne-daal-rakkhaa-hai-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  690 Oct 11 10:15 aisaa-hai-ki-sab-khvaab-musalsal-nahiin-hote-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  598 Oct 11 10:15 aise-chup-hain-ki-ye-manzil-bhii-kadii-ho-jaise-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  677 Oct 11 10:15 ajab-junuun-e-masaafat-men-ghar-se-niklaa-thaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  784 Oct 11 10:15 avval-avval-kii-dostii-hai-abhii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  866 Oct 11 10:15 dost-ban-kar-bhii-nahiin-saath-nibhaane-vaalaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  545 Oct 11 10:15 dukh-fasaana-nahiin-ki-tujh-se-kahen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  843 Oct 11 10:15 guftuguu-achchhii-lagii-zauq-e-nazar-achchhaa-lagaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  680 Oct 11 10:15 har-koii-dil-kii-hathelii-pe-hai-sahraa-rakkhe-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  788 Oct 11 10:15 havaa-ke-zor-se-pindaar-e-baam-o-dar-bhii-gayaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  605 Oct 11 10:15 huii-hai-shaam-to-aankhon-men-bas-gayaa-phir-tuu-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  792 Oct 11 10:15 is-qadar-musalsal-thiin-shiddaten-judaaii-kii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  684 Oct 11 10:15 is-se-pahle-ki-be-vafaa-ho-jaaen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  417 Oct 11 10:15 jab-bhii-dil-khol-ke-roe-honge-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  993 Oct 11 10:15 jis-samt-bhii-dekhuun-nazar-aataa-hai-ki-tum-ho-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  555 Oct 11 10:15 jo-gair-the-vo-isii-baat-par-hamaare-hue-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1035 Oct 11 10:15 juz-tire-koii-bhii-din-raat-na-jaane-mere-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  598 Oct 11 10:15 karuun-na-yaad-magar-kis-tarah-bhulaauun-use-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  594 Oct 11 10:15 kathin-hai-raahguzar-thodii-duur-saath-chalo-ahmad-faraz-ghazals-1\n",
            "-rw------- 1 root root  598 Oct 11 10:15 khaamosh-ho-kyuun-daad-e-jafaa-kyuun-nahiin-dete-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  541 Oct 11 10:15 kyaa-aise-kam-sukhan-se-koii-guftuguu-kare-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  615 Oct 11 10:15 main-to-maqtal-men-bhii-qismat-kaa-sikandar-niklaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  426 Oct 11 10:15 phir-usii-rahguzaar-par-shaayad-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  620 Oct 11 10:15 qurbat-bhii-nahiin-dil-se-utar-bhii-nahiin-jaataa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  562 Oct 11 10:15 qurbaton-men-bhii-judaaii-ke-zamaane-maange-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  576 Oct 11 10:15 ranjish-hii-sahii-dil-hii-dukhaane-ke-liye-aa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  602 Oct 11 10:15 rog-aise-bhii-gam-e-yaar-se-lag-jaate-hain-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  953 Oct 11 10:15 saamne-us-ke-kabhii-us-kii-sataaish-nahiin-kii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  560 Oct 11 10:15 saaqiyaa-ek-nazar-jaam-se-pahle-pahle-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  490 Oct 11 10:15 shoala-thaa-jal-bujhaa-huun-havaaen-mujhe-na-do-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  467 Oct 11 10:15 silsile-tod-gayaa-vo-sabhii-jaate-jaate-ahmad-faraz-ghazals\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**poets -> ur-ghazal**"
      ],
      "metadata": {
        "id": "Wrd_laNxPLUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls -la /content/drive/MyDrive/ANLP/project_1/dataset/ahmad-faraz/ur | head -n 40\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qviNoXD4fIq",
        "outputId": "8a944f2d-876c-416f-928c-31abfc889656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 59\n",
            "-rw------- 1 root root  648 Oct 11 10:15 aankh-se-duur-na-ho-dil-se-utar-jaaegaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  935 Oct 11 10:15 aashiqii-men-miir-jaise-khvaab-mat-dekhaa-karo-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  641 Oct 11 10:15 ab-aur-kyaa-kisii-se-maraasim-badhaaen-ham-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1348 Oct 11 10:15 abhii-kuchh-aur-karishme-gazal-ke-dekhte-hain-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  837 Oct 11 10:15 ab-ke-ham-bichhde-to-shaayad-kabhii-khvaabon-men-milen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1832 Oct 11 10:15 ab-ke-tajdiid-e-vafaa-kaa-nahiin-imkaan-jaanaan-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  868 Oct 11 10:15 ab-kyaa-sochen-kyaa-haalaat-the-kis-kaaran-ye-zahr-piyaa-hai-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  741 Oct 11 10:15 ab-shauq-se-ki-jaan-se-guzar-jaanaa-chaahiye-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  987 Oct 11 10:15 agarche-zor-havaaon-ne-daal-rakkhaa-hai-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  897 Oct 11 10:15 aisaa-hai-ki-sab-khvaab-musalsal-nahiin-hote-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  805 Oct 11 10:15 aise-chup-hain-ki-ye-manzil-bhii-kadii-ho-jaise-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  894 Oct 11 10:15 ajab-junuun-e-masaafat-men-ghar-se-niklaa-thaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1009 Oct 11 10:15 avval-avval-kii-dostii-hai-abhii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1139 Oct 11 10:15 dost-ban-kar-bhii-nahiin-saath-nibhaane-vaalaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  714 Oct 11 10:15 dukh-fasaana-nahiin-ki-tujh-se-kahen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1076 Oct 11 10:15 guftuguu-achchhii-lagii-zauq-e-nazar-achchhaa-lagaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  911 Oct 11 10:15 har-koii-dil-kii-hathelii-pe-hai-sahraa-rakkhe-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1044 Oct 11 10:15 havaa-ke-zor-se-pindaar-e-baam-o-dar-bhii-gayaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  732 Oct 11 10:15 huii-hai-shaam-to-aankhon-men-bas-gayaa-phir-tuu-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1025 Oct 11 10:15 is-qadar-musalsal-thiin-shiddaten-judaaii-kii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  927 Oct 11 10:15 is-se-pahle-ki-be-vafaa-ho-jaaen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  599 Oct 11 10:15 jab-bhii-dil-khol-ke-roe-honge-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1323 Oct 11 10:15 jis-samt-bhii-dekhuun-nazar-aataa-hai-ki-tum-ho-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  760 Oct 11 10:15 jo-gair-the-vo-isii-baat-par-hamaare-hue-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1388 Oct 11 10:15 juz-tire-koii-bhii-din-raat-na-jaane-mere-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  746 Oct 11 10:15 karuun-na-yaad-magar-kis-tarah-bhulaauun-use-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  757 Oct 11 10:15 kathin-hai-raahguzar-thodii-duur-saath-chalo-ahmad-faraz-ghazals-1\n",
            "-rw------- 1 root root  780 Oct 11 10:15 khaamosh-ho-kyuun-daad-e-jafaa-kyuun-nahiin-dete-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  708 Oct 11 10:15 kyaa-aise-kam-sukhan-se-koii-guftuguu-kare-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  777 Oct 11 10:15 main-to-maqtal-men-bhii-qismat-kaa-sikandar-niklaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  542 Oct 11 10:15 phir-usii-rahguzaar-par-shaayad-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  797 Oct 11 10:15 qurbat-bhii-nahiin-dil-se-utar-bhii-nahiin-jaataa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  750 Oct 11 10:15 qurbaton-men-bhii-judaaii-ke-zamaane-maange-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  748 Oct 11 10:15 ranjish-hii-sahii-dil-hii-dukhaane-ke-liye-aa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  800 Oct 11 10:15 rog-aise-bhii-gam-e-yaar-se-lag-jaate-hain-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1221 Oct 11 10:15 saamne-us-ke-kabhii-us-kii-sataaish-nahiin-kii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  748 Oct 11 10:15 saaqiyaa-ek-nazar-jaam-se-pahle-pahle-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  644 Oct 11 10:15 shoala-thaa-jal-bujhaa-huun-havaaen-mujhe-na-do-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  628 Oct 11 10:15 silsile-tod-gayaa-vo-sabhii-jaate-jaate-ahmad-faraz-ghazals\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Preprocessing**\n",
        " ---\n",
        "* Clean\tthe\tUrdu\ttext:normalize\tcharacters,\tremove\textraneous\tpunctuation\tas\tneeded.\n",
        "* Define\tor\tcollect\trules\tfor\tconverting\tUrdu\tinto\tRoman\tUrdu\t(if\tRoman\tUrdu\tis\tnot\tdirectly\n",
        "given\tin\tthe\tdataset).\n",
        "* Tokenization:\tchoose\tproper\ttokenization\tstrategy\tfor\tboth\tsource\t(Urdu)\tand\ttarget\n",
        "(Roman\tUrdu).Consider\tsubword\tmethods\t(e.g.\tByte-Pair\tEncoding,\tWordPiece)\tif\thelpful."
      ],
      "metadata": {
        "id": "90V7kyujcbFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**count dataset**\n",
        "* total poets in dataset\n",
        "* total pairs of urdu & roman urdu lines"
      ],
      "metadata": {
        "id": "uxfdx52pPQt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "datapath = '/content/drive/MyDrive/ANLP/project_1/dataset'\n",
        "\n",
        "# get all poets\n",
        "poets = os.listdir(datapath)\n",
        "poets = [p for p in poets if os.path.isdir(os.path.join(datapath, p))]\n",
        "print(f\"total no of poets in dataset: {len(poets)}\")\n",
        "\n",
        "# load data from each poets\n",
        "actual_urdu = []\n",
        "roman_urdu = []\n",
        "\n",
        "for poet in poets:\n",
        "    urdu_data_path = os.path.join(datapath, poet, 'ur')\n",
        "    roman_data_path = os.path.join(datapath, poet, 'en')\n",
        "\n",
        "    if not os.path.exists(urdu_data_path) or not os.path.exists(roman_data_path):\n",
        "        continue\n",
        "\n",
        "    files = os.listdir(urdu_data_path)\n",
        "\n",
        "    for file in files:\n",
        "        f = open(os.path.join(urdu_data_path, file), 'r', encoding='utf-8')\n",
        "        urdudata_lines = f.read().split('\\n')\n",
        "        f.close()\n",
        "\n",
        "        f = open(os.path.join(roman_data_path, file), 'r', encoding='utf-8')\n",
        "        romandata_lines = f.read().split('\\n')\n",
        "        f.close()\n",
        "\n",
        "        for u, r in zip(urdudata_lines, romandata_lines):\n",
        "            if u.strip() and r.strip():\n",
        "                actual_urdu.append(u.strip())\n",
        "                roman_urdu.append(r.strip())\n",
        "\n",
        "print(f\"total pairs: {len(actual_urdu)}\")\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"urdu: {actual_urdu[i]}\")\n",
        "    print(f\"roman: {roman_urdu[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBlwbWC84pKk",
        "outputId": "40d12eb1-4270-4e1f-a465-91487dbba061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total no of poets in dataset: 30\n",
            "total pairs: 21003\n",
            "\n",
            "1.\n",
            "urdu: دل سے تری نگاہ جگر تک اتر گئی\n",
            "roman: dil se tirī nigāh jigar tak utar ga.ī\n",
            "\n",
            "2.\n",
            "urdu: دونوں کو اک ادا میں رضامند کر گئی\n",
            "roman: donoñ ko ik adā meñ razā-mand kar ga.ī\n",
            "\n",
            "3.\n",
            "urdu: شق ہو گیا ہے سینہ خوشا لذت فراغ\n",
            "roman: shaq ho gayā hai siina ḳhushā lazzat-e-farāġh\n",
            "\n",
            "4.\n",
            "urdu: تکلیف پردہ داری زخم جگر گئی\n",
            "roman: taklīf-e-parda-dāri-e-zaḳhm-e-jigar ga.ī\n",
            "\n",
            "5.\n",
            "urdu: وہ بادۂ شبانہ کی سرمستیاں کہاں\n",
            "roman: vo bāda-e-shabāna kī sarmastiyāñ kahāñ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data cleaning"
      ],
      "metadata": {
        "id": "2SVdTCtgZKdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**urdu data cleaning**"
      ],
      "metadata": {
        "id": "G3NhMGy1ZPgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# urdu data cleaniing\n",
        "clean_urdudata = []\n",
        "\n",
        "# set different types of urdu haraf(alphabet)\n",
        "for txt in actual_urdu:\n",
        "    #alif\n",
        "    txt = txt.replace('ﺍ', 'ا')\n",
        "    txt = txt.replace('ﺎ', 'ا')\n",
        "    txt = txt.replace('ﺁ', 'آ')\n",
        "    txt = txt.replace('ﺃ', 'أ')\n",
        "\n",
        "    #ye\n",
        "    txt = txt.replace('ﯼ', 'ی')\n",
        "    txt = txt.replace('ﯽ', 'ی')\n",
        "    txt = txt.replace('ﯾ', 'ی')\n",
        "    txt = txt.replace('ﯿ', 'ی')\n",
        "    txt = txt.replace('ﮮ', 'ی')\n",
        "    txt = txt.replace('ئ', 'ی')\n",
        "    txt = txt.replace('ي', 'ی')\n",
        "\n",
        "    #hey\n",
        "    txt = txt.replace('ﮨ', 'ہ')\n",
        "    txt = txt.replace('ﮩ', 'ہ')\n",
        "    txt = txt.replace('ﮪ', 'ہ')\n",
        "    txt = txt.replace('ﮫ', 'ہ')\n",
        "    txt = txt.replace('ھ', 'ہ')\n",
        "    txt = txt.replace('ه', 'ہ')\n",
        "\n",
        "    #kaaf\n",
        "    txt = txt.replace('ﮐ', 'ک')\n",
        "    txt = txt.replace('ﮑ', 'ک')\n",
        "    txt = txt.replace('ك', 'ک')\n",
        "\n",
        "    #gaaf\n",
        "    txt = txt.replace('ﮒ', 'گ')\n",
        "    txt = txt.replace('ﮓ', 'گ')\n",
        "\n",
        "    #wao\n",
        "    txt = txt.replace('ؤ', 'و')\n",
        "\n",
        "    # remove urdu punctuation\n",
        "    txt = txt.replace('۔', '')\n",
        "    txt = txt.replace('؍', '')\n",
        "    txt = txt.replace('،', '')\n",
        "\n",
        "    # remove zabar zer pesh\n",
        "    txt = re.sub(r'[\\u064B-\\u065F]', '', txt)\n",
        "\n",
        "    # remove extra spaces\n",
        "    txt = re.sub(r'\\s+', ' ', txt)\n",
        "    txt = txt.strip()\n",
        "\n",
        "    if txt:\n",
        "        clean_urdudata.append(txt)\n",
        "\n",
        "print(f\"total urdu data(cleaned): {len(clean_urdudata)}\")\n",
        "\n",
        "for i in range(1,10,2):\n",
        "    print(f\"\\n{i+1}. {clean_urdudata[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UQmXVMj5OiM",
        "outputId": "4f4a4703-1b74-4e07-9e28-49b72d0cdaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total urdu data(cleaned): 21003\n",
            "\n",
            "2. دونوں کو اک ادا میں رضامند کر گیی\n",
            "\n",
            "4. تکلیف پردہ داری زخم جگر گیی\n",
            "\n",
            "6. اٹہیے بس اب کہ لذت خواب سحر گیی\n",
            "\n",
            "8. بارے اب اے ہوا ہوس بال و پر گیی\n",
            "\n",
            "10. موج خرام یار بہی کیا گل کتر گیی\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**roman urdu data cleaning**"
      ],
      "metadata": {
        "id": "oAXqW6EnZYP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# roman data cleaning\n",
        "clean_roman_data = []\n",
        "\n",
        "for txt in roman_urdu:\n",
        "    # convert to lowercase\n",
        "    txt = txt.lower()\n",
        "\n",
        "    # remove special urdu letters\n",
        "    txt = txt.replace('ā', 'a')\n",
        "    txt = txt.replace('ī', 'i')\n",
        "    txt = txt.replace('ū', 'u')\n",
        "    txt = txt.replace('ñ', 'n')\n",
        "    txt = txt.replace('ṭ', 't')\n",
        "    txt = txt.replace('ḍ', 'd')\n",
        "    txt = txt.replace('ṛ', 'r')\n",
        "    txt = txt.replace('ṇ', 'n')\n",
        "\n",
        "    # keep only english letters and spaces\n",
        "    txt = re.sub(r'[^a-z\\s]', '', txt)\n",
        "\n",
        "    # remove extra spaces\n",
        "    txt = re.sub(r'\\s+', ' ', txt)\n",
        "    txt = txt.strip()\n",
        "\n",
        "    if txt:\n",
        "        clean_roman_data.append(txt)\n",
        "\n",
        "print(f\"total roman data(cleaned): {len(clean_roman_data)}\")\n",
        "\n",
        "for i in range(1,10,2):\n",
        "    print(f\"\\n{i+1}. {clean_roman_data[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zx9aWmJAoJy",
        "outputId": "0ba4692d-ead0-4f0b-f9e2-3c6802c87045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total roman data(cleaned): 21003\n",
            "\n",
            "2. donon ko ik ada men razamand kar gai\n",
            "\n",
            "4. taklifepardadariezahmejigar gai\n",
            "\n",
            "6. uthiye bas ab ki lazzatehvabesahar gai\n",
            "\n",
            "8. baare ab ai hava havasebalopar gai\n",
            "\n",
            "10. maujehirameyar bhi kya gul katar gai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**combine view of data cleaning**"
      ],
      "metadata": {
        "id": "qgVD3tg5Zf_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"total urdu data(cleaned): {len(clean_urdudata)}\")\n",
        "print(f\"total roman data(cleaned): {len(clean_roman_data)}\")\n",
        "\n",
        "print(\"\\nrandom clean data samples pairs:\")\n",
        "for i in range(1,20,4):\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"actual urdu : {clean_urdudata[i]}\")\n",
        "    print(f\"roman urdu: {clean_roman_data[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw0ZVGTPBZsG",
        "outputId": "6e0a35e6-1f66-447a-c3ee-8b5dc69cf1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total urdu data(cleaned): 21003\n",
            "total roman data(cleaned): 21003\n",
            "\n",
            "random clean data samples pairs:\n",
            "\n",
            "2.\n",
            "actual urdu : دونوں کو اک ادا میں رضامند کر گیی\n",
            "roman urdu: donon ko ik ada men razamand kar gai\n",
            "\n",
            "6.\n",
            "actual urdu : اٹہیے بس اب کہ لذت خواب سحر گیی\n",
            "roman urdu: uthiye bas ab ki lazzatehvabesahar gai\n",
            "\n",
            "10.\n",
            "actual urdu : موج خرام یار بہی کیا گل کتر گیی\n",
            "roman urdu: maujehirameyar bhi kya gul katar gai\n",
            "\n",
            "14.\n",
            "actual urdu : مستی سے ہر نگہ ترے رخ پر بکہر گیی\n",
            "roman urdu: masti se har nigah tire ruh par bikhar gai\n",
            "\n",
            "18.\n",
            "actual urdu : وہ ولولے کہاں وہ جوانی کدہر گیی\n",
            "roman urdu: vo valvale kahan vo javani kidhar gai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**merge all data in one file**"
      ],
      "metadata": {
        "id": "xInTXx_WKI7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "class DatasetAggregator:\n",
        "    #get all data from each poet and create combine file\n",
        "\n",
        "    def __init__(self, dataset_path):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.urdu_lines = []\n",
        "        self.roman_lines = []\n",
        "\n",
        "    def is_letter(self, ch):\n",
        "        #checck if char is a letter\n",
        "        return ch.isalpha()\n",
        "\n",
        "    def remove_inner_dots(self, text):\n",
        "        #remove dots that show between letters\n",
        "        if not text:\n",
        "            return text\n",
        "\n",
        "        result = []\n",
        "        for i, ch in enumerate(text):\n",
        "            if ch == '.':\n",
        "                # Check if previous and next are letters\n",
        "                prev_is_letter = (i > 0 and self.is_letter(text[i-1]))\n",
        "                next_is_letter = (i < len(text) - 1 and self.is_letter(text[i+1]))\n",
        "\n",
        "                # only remove if surrounded by letters\n",
        "                if prev_is_letter and next_is_letter:\n",
        "                    continue\n",
        "                else:\n",
        "                    result.append(ch)\n",
        "            else:\n",
        "                result.append(ch)\n",
        "\n",
        "        return ''.join(result)\n",
        "\n",
        "    def normalize_roman(self, text):\n",
        "        #Normalize Roman txt\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Normalize unicode\n",
        "        text = unicodedata.normalize(\"NFKD\", text)\n",
        "\n",
        "        # Remove non-ASCII char\n",
        "        text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "\n",
        "        # Remove inner dots\n",
        "        text = self.remove_inner_dots(text)\n",
        "\n",
        "        # Remove quotes\n",
        "        text = re.sub(r\"['''\\\"`]\", \"\", text)\n",
        "\n",
        "        # Clean whitespace\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def normalize_urdu(self, text):\n",
        "        #Normalize Urdu txt\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Normalize unicode\n",
        "        text = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "        # Remove diacritical marks (Arabic Presentation Forms)\n",
        "        cleaned = []\n",
        "        for ch in text:\n",
        "            cp = ord(ch)\n",
        "            # Skip Arabic Presentation Forms\n",
        "            if (0xFB50 <= cp <= 0xFDFF) or (0xFE70 <= cp <= 0xFEFF):\n",
        "                continue\n",
        "            cleaned.append(ch)\n",
        "\n",
        "        text = ''.join(cleaned)\n",
        "\n",
        "        # Remove special char\n",
        "        text = text.replace(\"ؔ\", \"\")\n",
        "\n",
        "        # Remove inner dots\n",
        "        text = self.remove_inner_dots(text)\n",
        "\n",
        "        # Clean whitespace\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def process_poet_folder(self, poet_name):\n",
        "        #Process single poets folder\n",
        "        poet_path = os.path.join(self.dataset_path, poet_name)\n",
        "\n",
        "        en_dir = os.path.join(poet_path, \"en\")\n",
        "        ur_dir = os.path.join(poet_path, \"ur\")\n",
        "\n",
        "        # Check if directories exist\n",
        "        if not os.path.isdir(en_dir) or not os.path.isdir(ur_dir):\n",
        "            return 0\n",
        "\n",
        "        # Get file lists\n",
        "        en_files = sorted([f for f in os.listdir(en_dir) if os.path.isfile(os.path.join(en_dir, f))])\n",
        "        ur_files = sorted([f for f in os.listdir(ur_dir) if os.path.isfile(os.path.join(ur_dir, f))])\n",
        "\n",
        "        pairs_added = 0\n",
        "\n",
        "        # Process each file pair\n",
        "        for en_file, ur_file in zip(en_files, ur_files):\n",
        "            en_path = os.path.join(en_dir, en_file)\n",
        "            ur_path = os.path.join(ur_dir, ur_file)\n",
        "\n",
        "            try:\n",
        "                # Read Roman file\n",
        "                with open(en_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "                    en_lines = f.readlines()\n",
        "\n",
        "                # Read Urdu file\n",
        "                with open(ur_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "                    ur_lines = f.readlines()\n",
        "\n",
        "                # Process each line pair\n",
        "                for en_line, ur_line in zip(en_lines, ur_lines):\n",
        "                    en_line = en_line.strip()\n",
        "                    ur_line = ur_line.strip()\n",
        "\n",
        "                    if en_line and ur_line:\n",
        "                        # Normalize both\n",
        "                        en_norm = self.normalize_roman(en_line)\n",
        "                        ur_norm = self.normalize_urdu(ur_line)\n",
        "\n",
        "                        # Only add if both are non-empty after normalization\n",
        "                        if en_norm and ur_norm:\n",
        "                            self.roman_lines.append(en_norm)\n",
        "                            self.urdu_lines.append(ur_norm)\n",
        "                            pairs_added += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {poet_name}/{en_file}: {e}\")\n",
        "\n",
        "        return pairs_added\n",
        "\n",
        "    def aggregate_all(self):\n",
        "        #Process all poet folders\n",
        "        print(\"start data merging\")\n",
        "        print(f\"dataset path: {self.dataset_path}\\n\")\n",
        "\n",
        "        # Get all poet directories\n",
        "        poets = sorted([d for d in os.listdir(self.dataset_path)\n",
        "                       if os.path.isdir(os.path.join(self.dataset_path, d))\n",
        "                       and not d.startswith('.')])\n",
        "\n",
        "        print(f\"total : {len(poets)} poets folders\\n\")\n",
        "\n",
        "        total_pairs = 0\n",
        "\n",
        "        for poet_name in poets:\n",
        "            pairs = self.process_poet_folder(poet_name)\n",
        "            if pairs > 0:\n",
        "                print(f\"{poet_name}: {pairs} pairs added\")\n",
        "                total_pairs += pairs\n",
        "\n",
        "        print(f\"\\nTotal pairs: {total_pairs}\")\n",
        "        print(f\"Total Urdu lines: {len(self.urdu_lines)}\")\n",
        "        print(f\"Total Roman lines: {len(self.roman_lines)}\")\n",
        "\n",
        "        return total_pairs\n",
        "\n",
        "    def save_files(self, urdu_output, roman_output):\n",
        "        #Save merged data to files\n",
        "        print(f\"\\nSave merge files\")\n",
        "\n",
        "        # Save Urdu\n",
        "        with open(urdu_output, 'w', encoding='utf-8') as f:\n",
        "            for line in self.urdu_lines:\n",
        "                f.write(line + '\\n')\n",
        "        print(f\"Saved {len(self.urdu_lines)} Urdu lines-> {urdu_output}\")\n",
        "\n",
        "        # Save Roman\n",
        "        with open(roman_output, 'w', encoding='utf-8') as f:\n",
        "            for line in self.roman_lines:\n",
        "                f.write(line + '\\n')\n",
        "        print(f\"Saved {len(self.roman_lines)} Roman lines-> {roman_output}\")\n",
        "\n",
        "    def show_samples(self, num_samples=5):\n",
        "        #show sample pairs\n",
        "\n",
        "        print(\"sample data pair\")\n",
        "\n",
        "        for i in range(min(num_samples, len(self.urdu_lines))):\n",
        "            print(f\"Pair no {i+1}:\")\n",
        "            print(f\"  Urdu:  {self.urdu_lines[i]}\")\n",
        "            print(f\"  Roman: {self.roman_lines[i]}\")\n",
        "            print()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # UPDATE THIS PATH to your actual dataset location\n",
        "    DATASET_PATH = \"/content/drive/MyDrive/ANLP/project_1/dataset\"\n",
        "\n",
        "    # Create merger\n",
        "    aggregator = DatasetAggregator(DATASET_PATH)\n",
        "\n",
        "    # Process all poets\n",
        "    total = aggregator.aggregate_all()\n",
        "\n",
        "    # Save files\n",
        "    aggregator.save_files(\"/content/drive/MyDrive/ANLP/project_1/urdu.txt\", \"/content/drive/MyDrive/ANLP/project_1/roman.txt\")\n",
        "\n",
        "    # Show samples\n",
        "    aggregator.show_samples(num_samples=5)\n",
        "\n",
        "\n",
        "    print(\"Dataset merging done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CWFZz-MKG6g",
        "outputId": "9a4d37b7-b745-44b0-9011-ba8706a3662e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start data merging\n",
            "dataset path: /content/drive/MyDrive/ANLP/project_1/dataset\n",
            "\n",
            "total : 30 poets folders\n",
            "\n",
            "ahmad-faraz: 748 pairs added\n",
            "akbar-allahabadi: 776 pairs added\n",
            "allama-iqbal: 738 pairs added\n",
            "altaf-hussain-hali: 582 pairs added\n",
            "ameer-khusrau: 10 pairs added\n",
            "bahadur-shah-zafar: 878 pairs added\n",
            "dagh-dehlvi: 1414 pairs added\n",
            "fahmida-riaz: 84 pairs added\n",
            "faiz-ahmad-faiz: 564 pairs added\n",
            "firaq-gorakhpuri: 1502 pairs added\n",
            "gulzar: 432 pairs added\n",
            "habib-jalib: 608 pairs added\n",
            "jaan-nisar-akhtar: 550 pairs added\n",
            "jaun-eliya: 946 pairs added\n",
            "javed-akhtar: 564 pairs added\n",
            "jigar-moradabadi: 1088 pairs added\n",
            "kaifi-azmi: 174 pairs added\n",
            "meer-anees: 168 pairs added\n",
            "meer-taqi-meer: 842 pairs added\n",
            "mirza-ghalib: 3717 pairs added\n",
            "mohsin-naqvi: 632 pairs added\n",
            "naji-shakir: 116 pairs added\n",
            "naseer-turabi: 234 pairs added\n",
            "nazm-tabatabai: 650 pairs added\n",
            "nida-fazli: 532 pairs added\n",
            "noon-meem-rashid: 62 pairs added\n",
            "parveen-shakir: 754 pairs added\n",
            "sahir-ludhianvi: 498 pairs added\n",
            "wali-mohammad-wali: 552 pairs added\n",
            "waseem-barelvi: 588 pairs added\n",
            "\n",
            "Total pairs: 21003\n",
            "Total Urdu lines: 21003\n",
            "Total Roman lines: 21003\n",
            "\n",
            "Save merge files\n",
            "Saved 21003 Urdu lines-> /content/drive/MyDrive/ANLP/project_1/urdu.txt\n",
            "Saved 21003 Roman lines-> /content/drive/MyDrive/ANLP/project_1/roman.txt\n",
            "sample data pair\n",
            "Pair no 1:\n",
            "  Urdu:  آنکھ سے دور نہ ہو دل سے اتر جائے گا\n",
            "  Roman: aankh se duur na ho dil se utar jaega\n",
            "\n",
            "Pair no 2:\n",
            "  Urdu:  وقت کا کیا ہے گزرتا ہے گزر جائے گا\n",
            "  Roman: vaqt ka kya hai guzarta hai guzar jaega\n",
            "\n",
            "Pair no 3:\n",
            "  Urdu:  اتنا مانوس نہ ہو خلوت غم سے اپنی\n",
            "  Roman: itna manus na ho khalvat-e-gham se apni\n",
            "\n",
            "Pair no 4:\n",
            "  Urdu:  تو کبھی خود کو بھی دیکھے گا تو ڈر جائے گا\n",
            "  Roman: tu kabhi khud ko bhi dekhega to dar jaega\n",
            "\n",
            "Pair no 5:\n",
            "  Urdu:  ڈوبتے ڈوبتے کشتی کو اچھالا دے دوں\n",
            "  Roman: dubte dubte kashti ko uchhala de duun\n",
            "\n",
            "Dataset merging done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tokenization on sample data**"
      ],
      "metadata": {
        "id": "tU9SGQwAdQHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "#bpe\n",
        "class BPETokenizer:\n",
        "   #why vocab size 1000 nd num merge 500\n",
        "    def __init__(self, vocab_size=1000, num_merges=500):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_merges = num_merges\n",
        "        self.merges = []  # List of merge operations: [('r','a'), ('ra','m'), nd so on]\n",
        "        self.vocab = {}\n",
        "        self.special_tokens = ['<pad>', '<unk>', '<strt>', '<end>']\n",
        "\n",
        "    def train(self, texts):\n",
        "        print(\"Start training bpe \")\n",
        "\n",
        " #build initial word freq dict nd store each word as space-separated char with </w> marker\n",
        "        word_freq = defaultdict(int)\n",
        "\n",
        "        for text in texts:\n",
        "            text = text.strip().lower()\n",
        "            words = text.split()\n",
        "            for word in words:\n",
        "                if word:\n",
        "                    # changee \"meesha\" to \"m e e s h a </w>\"\n",
        "                    char_word = ' '.join(list(word)) + ' </w>'\n",
        "                    word_freq[char_word] += 1\n",
        "\n",
        "        print(f\"Initial words: {len(word_freq)}\")\n",
        "\n",
        "        # bpe merge\n",
        "        for merge_step in range(self.num_merges):\n",
        "            #count all adjacent pairs\n",
        "            pairs = defaultdict(int)\n",
        "\n",
        "            for word_str, freq in word_freq.items():\n",
        "                symbols = word_str.split()\n",
        "                for i in range(len(symbols) - 1):\n",
        "                    pair = (symbols[i], symbols[i + 1])\n",
        "                    pairs[pair] += freq\n",
        "\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            #find most common pair\n",
        "            best = max(pairs, key=pairs.get)\n",
        "\n",
        "            # Merge pair in all words\n",
        "            new_word_freq = defaultdict(int)\n",
        "            for word_str, freq in word_freq.items():\n",
        "                symbols = word_str.split()\n",
        "                new_symbols = []\n",
        "                i = 0\n",
        "                while i < len(symbols):\n",
        "                    if i < len(symbols) - 1 and symbols[i] == best[0] and symbols[i+1] == best[1]:\n",
        "                        new_symbols.append(best[0] + best[1])\n",
        "                        i += 2\n",
        "                    else:\n",
        "                        new_symbols.append(symbols[i])\n",
        "                        i += 1\n",
        "                new_word_freq[' '.join(new_symbols)] += freq\n",
        "\n",
        "            word_freq = new_word_freq\n",
        "            self.merges.append(best)\n",
        "\n",
        "            if (merge_step + 1) % 50 == 0:\n",
        "                print(f\"Merge {merge_step + 1}: merged {best}\")\n",
        "\n",
        "        print(f\"Total no of merges : {len(self.merges)}\")\n",
        "\n",
        "        # build vocabulary from final words\n",
        "        self.vocab = {}\n",
        "        idx = 0\n",
        "\n",
        "        #add special tokens\n",
        "        for token in self.special_tokens:\n",
        "            self.vocab[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "        #collect all tokens that show in final words\n",
        "        all_tokens = set()\n",
        "        for word_str in word_freq.keys():\n",
        "            for token in word_str.split():\n",
        "                all_tokens.add(token)\n",
        "\n",
        "        #add tokens to vocab\n",
        "        for token in sorted(all_tokens):\n",
        "            self.vocab[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "        print(f\"final vocablry size: {len(self.vocab)}\")\n",
        "        return self.vocab\n",
        "\n",
        "    def encode(self, text):\n",
        "        #convert text -> token IDs\n",
        "        text = text.strip().lower()\n",
        "        words = text.split()\n",
        "\n",
        "        token_ids = []\n",
        "\n",
        "        for word in words:\n",
        "            #convert word -> tokens\n",
        "            tokens = self._encode_word(word)\n",
        "            for token in tokens:\n",
        "                if token in self.vocab:\n",
        "                    token_ids.append(self.vocab[token])\n",
        "                else:\n",
        "                    token_ids.append(self.vocab['<unk>'])\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def _encode_word(self, word):\n",
        "        #encode a single word using trained BPE\n",
        "        word = word.lower()\n",
        "\n",
        "        #start with char: \"meesha\" -> ['m', 'e', 'e', 's', 'h', 'a' ,'</w>']\n",
        "        tokens = list(word) + ['</w>']\n",
        "\n",
        "        #apply each merge operation\n",
        "        for pair in self.merges:\n",
        "            new_tokens = []\n",
        "            i = 0\n",
        "            while i < len(tokens):\n",
        "                if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
        "                    #found pair nd merge it\n",
        "                    new_tokens.append(pair[0] + pair[1])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1\n",
        "            tokens = new_tokens\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        #convert token IDs -> text\n",
        "        id_to_token = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "        tokens = []\n",
        "        for tid in token_ids:\n",
        "            if tid in id_to_token:\n",
        "                tokens.append(id_to_token[tid])\n",
        "            else:\n",
        "                tokens.append('<unk>')\n",
        "\n",
        "        # Join nd recover spaces\n",
        "        text = ''.join(tokens)\n",
        "        text = text.replace('</w>', ' ')\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def save(self, filename):\n",
        "        #Save tokenizer\n",
        "        data = {\n",
        "            'vocablry': self.vocab,\n",
        "            'merges': self.merges,\n",
        "            'special tokens': self.special_tokens\n",
        "        }\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"saved to {filename}\")\n",
        "\n",
        "    def load(self, filename):\n",
        "      #Load tokenizer\n",
        "        with open(filename, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.vocab = data['vocablry']\n",
        "        self.merges = data['merges']\n",
        "        self.special_tokens = data['special tokens']\n",
        "        print(f\"loaded from {filename}\")\n",
        "\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    #word-level tokenizer\n",
        "    #splits text -> words - maps each word -> ID\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word_to_id = {}\n",
        "        self.id_to_word = {}\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        #build vocabulary\n",
        "        words = set()\n",
        "        for text in texts:\n",
        "            for word in text.lower().split():\n",
        "                words.add(word)\n",
        "\n",
        "        idx = 0\n",
        "        for token in ['<pad>', '<unk>', '<strt>', '<end>']:\n",
        "            self.word_to_id[token] = idx\n",
        "            self.id_to_word[idx] = token\n",
        "            idx += 1\n",
        "\n",
        "        for word in sorted(words):\n",
        "            self.word_to_id[word] = idx\n",
        "            self.id_to_word[idx] = word\n",
        "            idx += 1\n",
        "\n",
        "        print(f\"Word level vocablry size: {len(self.word_to_id)}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        #Text -> IDs\n",
        "        text = text.strip().lower()\n",
        "        words = text.split()\n",
        "        return [self.word_to_id.get(w, self.word_to_id['<unk>']) for w in words]\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        #IDs -> text\n",
        "        words = [self.id_to_word.get(tid, '<unk>') for tid in token_ids]\n",
        "        return ' '.join(words)\n",
        "\n",
        "#main\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # We need seperate tokenizers for Urdu nd Roman Urdu\n",
        "    # Bcz both use different char sets\n",
        "\n",
        "    urdu_texts = [\n",
        "\"چاندنی رات میں تیرا خیال آتا ہے\",\n",
        "\"دل کے ویران کو پھر سے جمال آتا ہے\",\n",
        "\"خواب بن کر تری صورت نظر آتی ہے\",\n",
        "\"یوں لگتا ہے کہ تُو میرے حال آتا ہے\" ]\n",
        "\n",
        "    roman_texts = [\n",
        "\"chaandni raat mein tera khayal aata hai\",\n",
        "\"dil ke veeran ko phir se jamal aata hai\",\n",
        "\"khaab ban kar teri soorat nazar aati hai\",\n",
        "\"yun lagta hai ke tu mere haal aata hai\"\n",
        "\n",
        "   ]\n",
        "\n",
        "    print(\"training urdu bpe tokenizer\")\n",
        "\n",
        "    urdu_bpe = BPETokenizer(vocab_size=500, num_merges=50)\n",
        "    urdu_bpe.train(urdu_texts)\n",
        "\n",
        "    test_urdu = \"وہ جو خوابوں میں آیا کبھی حقیقت بن جائے\"\n",
        "    enc_urdu = urdu_bpe.encode(test_urdu)\n",
        "    dec_urdu = urdu_bpe.decode(enc_urdu)\n",
        "\n",
        "    print(f\"\\nOriginal urdu txt:  {test_urdu}\")\n",
        "    print(f\"Encoded urdu txt:   {enc_urdu}\")\n",
        "    print(f\"Decoded urdu txt:   {dec_urdu}\")\n",
        "    print(f\"cross check:     {test_urdu == dec_urdu}\")\n",
        "\n",
        "    print(\"training roman bpe tokenizer\")\n",
        "\n",
        "    roman_bpe = BPETokenizer(vocab_size=500, num_merges=50)\n",
        "    roman_bpe.train(roman_texts)\n",
        "\n",
        "    test_roman = \"woh jo khwabon mein aaya kabhi haqeeqat ban jaye\"\n",
        "    enc_roman = roman_bpe.encode(test_roman)\n",
        "    dec_roman = roman_bpe.decode(enc_roman)\n",
        "\n",
        "    print(f\"\\nOriginal roman txt:  {test_roman}\")\n",
        "    print(f\"Encoded roman txt:   {enc_roman}\")\n",
        "    print(f\"Decoded roman txt:   {dec_roman}\")\n",
        "    print(f\"cross check:     {test_roman == dec_roman}\")\n",
        "\n",
        "    print(\"\\ncomparison\")\n",
        "\n",
        "    print(f\"Urdu vocablry size:   {len(urdu_bpe.vocab)} :::\" + f\"Roman vocablry size:  {len(roman_bpe.vocab)}\" )\n",
        "    print(f\"Urdu seq length:   {len(enc_urdu)} :::\" + f\"Roman seq length:  {len(enc_roman)}\")\n",
        "\n",
        "    print(\"word level comparison\")\n",
        "\n",
        "    urdu_word = SimpleTokenizer()\n",
        "    urdu_word.build_vocab(urdu_texts)\n",
        "\n",
        "    roman_word = SimpleTokenizer()\n",
        "    roman_word.build_vocab(roman_texts)\n",
        "\n",
        "    print(f\"Urdu word vocablry:   {len(urdu_word.word_to_id)} :::\" + f\"Roman word vocablry:  {len(roman_word.word_to_id)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeDZEIHlE4s-",
        "outputId": "c36641cb-629e-4ccf-a69e-3166f0b9630e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training urdu bpe tokenizer\n",
            "Start training bpe \n",
            "Initial words: 27\n",
            "Merge 50: merged ('ن', 'ظ')\n",
            "Total no of merges : 50\n",
            "final vocablry size: 44\n",
            "\n",
            "Original urdu txt:  وہ جو خوابوں میں آیا کبھی حقیقت بن جائے\n",
            "Encoded urdu txt:   [26, 38, 4, 1, 27, 1, 26, 37, 24, 1, 40, 7, 32, 1, 1, 41, 14, 1, 40, 1, 1, 9, 1, 1, 1, 43]\n",
            "Decoded urdu txt:   وہ <unk>و <unk>وں میں <unk>یا ک<unk><unk>ی ح<unk>ی<unk><unk>بن <unk><unk><unk>ے\n",
            "cross check:     False\n",
            "training roman bpe tokenizer\n",
            "Start training bpe \n",
            "Initial words: 26\n",
            "Merge 50: merged ('b', 'an</w>')\n",
            "Total no of merges : 50\n",
            "final vocablry size: 45\n",
            "\n",
            "Original roman txt:  woh jo khwabon mein aaya kabhi haqeeqat ban jaye\n",
            "Encoded roman txt:   [1, 32, 1, 4, 1, 32, 4, 21, 1, 1, 5, 1, 32, 31, 29, 5, 5, 43, 6, 21, 5, 1, 1, 19, 1, 1, 1, 1, 1, 10, 4, 11, 1, 43, 14]\n",
            "Decoded roman txt:   <unk>o<unk> <unk>o k<unk><unk>a<unk>on mein aaya ka<unk><unk>i <unk><unk><unk><unk><unk>at ban <unk>ye\n",
            "cross check:     False\n",
            "\n",
            "comparison\n",
            "Urdu vocablry size:   44 :::Roman vocablry size:  45\n",
            "Urdu seq length:   26 :::Roman seq length:  35\n",
            "word level comparison\n",
            "Word level vocablry size: 31\n",
            "Word level vocablry size: 30\n",
            "Urdu word vocablry:   31 :::Roman word vocablry:  30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tokenization on actual data**"
      ],
      "metadata": {
        "id": "qmmvgtKXERlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class BPETokenizer:\n",
        "   #why vocab size 1000 nd num merge 500\n",
        "    def __init__(self, vocab_size=1000, num_merges=500):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_merges = num_merges\n",
        "        self.merges = []  # List of merge operations: [('r','a'), ('ra','m'), nd so on]\n",
        "        self.vocab = {}\n",
        "        self.special_tokens = ['<pad>', '<unk>', '<strt>', '<end>']\n",
        "\n",
        "    def train(self, texts):\n",
        "        print(\"Start training bpe \")\n",
        "\n",
        " #build initial word freq dict nd store each word as space-separated char with </w> marker\n",
        "        word_freq = defaultdict(int)\n",
        "\n",
        "        for text in texts:\n",
        "            text = text.strip().lower()\n",
        "            words = text.split()\n",
        "            for word in words:\n",
        "                if word:\n",
        "                    # changee \"meesha\" to \"m e e s h a </w>\"\n",
        "                    char_word = ' '.join(list(word)) + ' </w>'\n",
        "                    word_freq[char_word] += 1\n",
        "\n",
        "        print(f\"Initial words: {len(word_freq)}\")\n",
        "\n",
        "\n",
        "        # bpe merge\n",
        "        for merge_step in range(self.num_merges):\n",
        "            #count all adjacent pairs\n",
        "            pairs = defaultdict(int)\n",
        "\n",
        "            for word_str, freq in word_freq.items():\n",
        "                symbols = word_str.split()\n",
        "                for i in range(len(symbols) - 1):\n",
        "                    pair = (symbols[i], symbols[i + 1])\n",
        "                    pairs[pair] += freq\n",
        "\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            #find most common pair\n",
        "            best = max(pairs, key=pairs.get)\n",
        "\n",
        "            # Merge pair in all words\n",
        "            new_word_freq = defaultdict(int)\n",
        "            for word_str, freq in word_freq.items():\n",
        "                symbols = word_str.split()\n",
        "                new_symbols = []\n",
        "                i = 0\n",
        "                while i < len(symbols):\n",
        "                    if i < len(symbols) - 1 and symbols[i] == best[0] and symbols[i+1] == best[1]:\n",
        "                        new_symbols.append(best[0] + best[1])\n",
        "                        i += 2\n",
        "                    else:\n",
        "                        new_symbols.append(symbols[i])\n",
        "                        i += 1\n",
        "                new_word_freq[' '.join(new_symbols)] += freq\n",
        "\n",
        "            word_freq = new_word_freq\n",
        "            self.merges.append(best)\n",
        "\n",
        "            if (merge_step + 1) % 50 == 0:\n",
        "                print(f\"Merge {merge_step + 1}: merged {best}\")\n",
        "\n",
        "        print(f\"Total no of merges : {len(self.merges)}\")\n",
        "\n",
        "        # build vocabulary from final words\n",
        "        self.vocab = {}\n",
        "        idx = 0\n",
        "\n",
        "        #add special tokens\n",
        "        for token in self.special_tokens:\n",
        "            self.vocab[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "        #collect all tokens that show in final words\n",
        "        all_tokens = set()\n",
        "        for word_str in word_freq.keys():\n",
        "            for token in word_str.split():\n",
        "                all_tokens.add(token)\n",
        "\n",
        "        #add tokens to vocab\n",
        "        for token in sorted(all_tokens):\n",
        "            self.vocab[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "        print(f\"final vocablry size: {len(self.vocab)}\")\n",
        "        return self.vocab\n",
        "\n",
        "    def encode(self, text):\n",
        "        #convert text -> token IDs\n",
        "        text = text.strip().lower()\n",
        "        words = text.split()\n",
        "\n",
        "        token_ids = []\n",
        "\n",
        "        for word in words:\n",
        "            #convert word -> tokens\n",
        "            tokens = self._encode_word(word)\n",
        "            for token in tokens:\n",
        "                if token in self.vocab:\n",
        "                    token_ids.append(self.vocab[token])\n",
        "                else:\n",
        "                    token_ids.append(self.vocab['<unk>'])\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def _encode_word(self, word):\n",
        "        #encode a single word using trained BPE\n",
        "        word = word.lower()\n",
        "\n",
        "        #start with char: \"meesha\" -> ['m', 'e', 'e', 's', 'h', 'a' ,'</w>']\n",
        "        tokens = list(word) + ['</w>']\n",
        "\n",
        "        #apply each merge operation\n",
        "        for pair in self.merges:\n",
        "            new_tokens = []\n",
        "            i = 0\n",
        "            while i < len(tokens):\n",
        "                if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
        "                    #found pair nd merge it\n",
        "                    new_tokens.append(pair[0] + pair[1])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1\n",
        "            tokens = new_tokens\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        #convert token IDs -> text\n",
        "        id_to_token = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "        tokens = []\n",
        "        for tid in token_ids:\n",
        "            if tid in id_to_token:\n",
        "                tokens.append(id_to_token[tid])\n",
        "            else:\n",
        "                tokens.append('<unk>')\n",
        "\n",
        "        # Join nd recover spaces\n",
        "        text = ''.join(tokens)\n",
        "        text = text.replace('</w>', ' ')\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def save(self, filename):\n",
        "        #Save tokenizer\n",
        "        data = {\n",
        "            'vocablry': self.vocab,\n",
        "            'merges': self.merges,\n",
        "            'special tokens': self.special_tokens\n",
        "        }\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"saved to {filename}\")\n",
        "\n",
        "    def load(self, filename):\n",
        "      #Load tokenizer\n",
        "        with open(filename, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.vocab = data['vocablry']\n",
        "        self.merges = data['merges']\n",
        "        self.special_tokens = data['special tokens']\n",
        "        print(f\"loaded from {filename}\")\n",
        "\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    #word-level tokenizer\n",
        "    #splits text -> words - maps each word -> ID\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word_to_id = {}\n",
        "        self.id_to_word = {}\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        #build vocabulary\n",
        "        words = set()\n",
        "        for text in texts:\n",
        "            for word in text.lower().split():\n",
        "                words.add(word)\n",
        "\n",
        "        idx = 0\n",
        "        for token in ['<pad>', '<unk>', '<strt>', '<end>']:\n",
        "            self.word_to_id[token] = idx\n",
        "            self.id_to_word[idx] = token\n",
        "            idx += 1\n",
        "\n",
        "        for word in sorted(words):\n",
        "            self.word_to_id[word] = idx\n",
        "            self.id_to_word[idx] = word\n",
        "            idx += 1\n",
        "\n",
        "        print(f\"Word level vocablry size: {len(self.word_to_id)}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        #Text -> IDs\n",
        "        text = text.strip().lower()\n",
        "        words = text.split()\n",
        "        return [self.word_to_id.get(w, self.word_to_id['<unk>']) for w in words]\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        #IDs -> text\n",
        "        words = [self.id_to_word.get(tid, '<unk>') for tid in token_ids]\n",
        "        return ' '.join(words)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load from your ACTUAL cleaned dataset files\n",
        "\n",
        "    print(\"Loading full dataset\")\n",
        "\n",
        "    # Load all Urdu texts (21,003 sentences)\n",
        "    with open('/content/drive/MyDrive/ANLP/project_1/urdu.txt', 'r', encoding='utf-8') as f:\n",
        "        urdu_texts = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "    # Load all Roman texts (21,003 sentences)\n",
        "    with open('/content/drive/MyDrive/ANLP/project_1/roman.txt', 'r', encoding='utf-8') as f:\n",
        "        roman_texts = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "    print(f\"Loaded {len(urdu_texts)} Urdu sentences\")\n",
        "    print(f\"Loaded {len(roman_texts)} Roman sentences\")\n",
        "\n",
        "    print(\"training urdu bpe tokenizer on actual data\")\n",
        "\n",
        "    urdu_bpe = BPETokenizer(vocab_size=1000, num_merges=500)\n",
        "    urdu_bpe.train(urdu_texts)\n",
        "    urdu_bpe.save('/content/drive/MyDrive/ANLP/project_1/urdu_tokenizer.pkl')\n",
        "\n",
        "    # Test with one example\n",
        "    test_urdu = urdu_texts[0]\n",
        "    enc_urdu = urdu_bpe.encode(test_urdu)\n",
        "    dec_urdu = urdu_bpe.decode(enc_urdu)\n",
        "\n",
        "    print(f\"\\nTest example:\")\n",
        "    print(f\"actual:  {test_urdu}\")\n",
        "    print(f\"decoded:   {dec_urdu}\")\n",
        "    print(f\"cross check:     {test_urdu == dec_urdu}\")\n",
        "\n",
        "    print(\"training roman bpe tokenizer on actual data\")\n",
        "\n",
        "    roman_bpe = BPETokenizer(vocab_size=1000, num_merges=500)\n",
        "    roman_bpe.train(roman_texts)\n",
        "    roman_bpe.save('/content/drive/MyDrive/ANLP/project_1/roman_tokenizer.pkl')\n",
        "\n",
        "    # Test with one example\n",
        "    test_roman = roman_texts[0]\n",
        "    enc_roman = roman_bpe.encode(test_roman)\n",
        "    dec_roman = roman_bpe.decode(enc_roman)\n",
        "\n",
        "    print(f\"\\nTest example:\")\n",
        "    print(f\"actual:  {test_roman}\")\n",
        "    print(f\"decoded:   {dec_roman}\")\n",
        "    print(f\"cross check:     {test_roman == dec_roman}\")\n",
        "\n",
        "    print(\"Fianl comparison\\n\")\n",
        "    print(f\"Urdu vocab size:         {len(urdu_bpe.vocab)} ::: \" + f\"Roman vocab size:        {len(roman_bpe.vocab)}\")\n",
        "    print(f\"Urdu total merges:       {len(urdu_bpe.merges)} ::: \" +f\"Roman total merges:      {len(roman_bpe.merges)}\")\n",
        "    print(f\"\\nTokenizers saved:\")\n",
        "    print(f\"- urdu_tokenizer.pkl\"  + f\"- roman_tokenizer.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-fj2lT_IdhK",
        "outputId": "5024b03e-5e52-47b1-f1de-860ab210edee"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading full dataset\n",
            "Loaded 21003 Urdu sentences\n",
            "Loaded 21003 Roman sentences\n",
            "training urdu bpe tokenizer on actual data\n",
            "Start training bpe \n",
            "Initial words: 10340\n",
            "Merge 50: merged ('ا', 'س</w>')\n",
            "Merge 100: merged ('ت', 'ی')\n",
            "Merge 150: merged ('ن', 'ظ')\n",
            "Merge 200: merged ('ا', 'ل</w>')\n",
            "Merge 250: merged ('پ', 'و')\n",
            "Merge 300: merged ('م', 'ی</w>')\n",
            "Merge 350: merged ('س', 'تی</w>')\n",
            "Merge 400: merged ('ت', 'و')\n",
            "Merge 450: merged ('گ', 'ا')\n",
            "Merge 500: merged ('کھ', 'ی')\n",
            "Total no of merges : 500\n",
            "final vocablry size: 560\n",
            "saved to /content/drive/MyDrive/ANLP/project_1/urdu_tokenizer.pkl\n",
            "\n",
            "Test example:\n",
            "actual:  آنکھ سے دور نہ ہو دل سے اتر جائے گا\n",
            "decoded:   آنکھ سے دور نہ ہو دل سے اتر جائے گا\n",
            "cross check:     True\n",
            "training roman bpe tokenizer on actual data\n",
            "Start training bpe \n",
            "Initial words: 16917\n",
            "Merge 50: merged ('s', 'h')\n",
            "Merge 100: merged ('g', 'u')\n",
            "Merge 150: merged ('a', 'n')\n",
            "Merge 200: merged ('p', 'u')\n",
            "Merge 250: merged ('aa', 'm</w>')\n",
            "Merge 300: merged ('yaa', 'd</w>')\n",
            "Merge 350: merged ('ha', 's')\n",
            "Merge 400: merged ('ya', 't</w>')\n",
            "Merge 450: merged ('sh', '-e-')\n",
            "Merge 500: merged ('jaa', 'na</w>')\n",
            "Total no of merges : 500\n",
            "final vocablry size: 534\n",
            "saved to /content/drive/MyDrive/ANLP/project_1/roman_tokenizer.pkl\n",
            "\n",
            "Test example:\n",
            "actual:  aankh se duur na ho dil se utar jaega\n",
            "decoded:   aankh se duur na ho dil se utar jaega\n",
            "cross check:     True\n",
            "Fianl comparison\n",
            "\n",
            "Urdu vocab size:         560 ::: Roman vocab size:        534\n",
            "Urdu total merges:       500 ::: Roman total merges:      500\n",
            "\n",
            "Tokenizers saved:\n",
            "- urdu_tokenizer.pkl- roman_tokenizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Tokenizers | Encoding Text | Creating PyTorch Datasets**"
      ],
      "metadata": {
        "id": "Dnax2j2fN1GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the trained tokenizers\n",
        "print(\"Loading tokenizers...\")\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/urdu_tokenizer.pkl', 'rb') as f:\n",
        "    urdu_data = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/roman_tokenizer.pkl', 'rb') as f:\n",
        "    roman_data = pickle.load(f)\n",
        "\n",
        "print(\"Tokenizers loaded successfully!\")\n",
        "\n",
        "# Extract vocabulary nd merges\n",
        "urdu_vocab = urdu_data['vocablry']\n",
        "urdu_merges = urdu_data['merges']\n",
        "\n",
        "roman_vocab = roman_data['vocablry']\n",
        "roman_merges = roman_data['merges']\n",
        "\n",
        "print(f\"Urdu vocab size: {len(urdu_vocab)}, Merges: {len(urdu_merges)}\")\n",
        "print(f\"Roman vocab size: {len(roman_vocab)}, Merges: {len(roman_merges)}\")\n",
        "\n",
        "# Load the data\n",
        "print(\"\\nLoad dataset\")\n",
        "urdu_sentences = []\n",
        "roman_sentences = []\n",
        "\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/urdu.txt', 'r', encoding='utf-8') as f:\n",
        "    urdu_sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/roman.txt', 'r', encoding='utf-8') as f:\n",
        "    roman_sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Loaded {len(urdu_sentences)} Urdu sentences\")\n",
        "print(f\"Loaded {len(roman_sentences)} Roman sentences\")\n",
        "\n",
        "# Function to encode words using BPE merges\n",
        "def encode_with_bpe(word, vocab, merges):\n",
        "    #Encode a single word using BPE vocablty nd merges\n",
        "    word = word.lower()\n",
        "\n",
        "    tokens = list(word) + ['</w>']\n",
        "\n",
        "    # Apply each merge operation\n",
        "    for pair in merges:\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
        "                # Found pair, merge it\n",
        "                new_tokens.append(pair[0] + pair[1])\n",
        "                i += 2\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "        tokens = new_tokens\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function to encode full text\n",
        "def encode_text(text, vocab, merges):\n",
        "    #convert text -> token IDs using BPE\n",
        "    text = text.strip().lower()\n",
        "    words = text.split()\n",
        "\n",
        "    token_ids = []\n",
        "    for word in words:\n",
        "        tokens = encode_with_bpe(word, vocab, merges)\n",
        "        for token in tokens:\n",
        "            if token in vocab:\n",
        "                token_ids.append(vocab[token])\n",
        "            else:\n",
        "                token_ids.append(vocab.get('<unk>', 1))  # Use unk token ID\n",
        "\n",
        "    return token_ids\n",
        "\n",
        "# Tokenize all sentences using BPE\n",
        "print(\"\\nTokenizing sentences using BPE\")\n",
        "urdu_tokens = []\n",
        "roman_tokens = []\n",
        "\n",
        "for i, (urdu_sent, roman_sent) in enumerate(zip(urdu_sentences, roman_sentences)):\n",
        "    if i % 5000 == 0:\n",
        "        print(f\"Tokenized {i} sentences\")\n",
        "\n",
        "    # Encode using BPE -> returns list of token IDs\n",
        "    urdu_seq = encode_text(urdu_sent, urdu_vocab, urdu_merges)\n",
        "    roman_seq = encode_text(roman_sent, roman_vocab, roman_merges)\n",
        "\n",
        "    urdu_tokens.append(urdu_seq)\n",
        "    roman_tokens.append(roman_seq)\n",
        "\n",
        "print(f\"Total sequences tokenized: {len(urdu_tokens)}\")\n",
        "\n",
        "# Create train/val/test splits 50%, 25%, 25%\n",
        "print(\"Creating train/val/test splits\")\n",
        "\n",
        "# split train 50% nd temp 50%\n",
        "train_urdu, temp_urdu, train_roman, temp_roman = train_test_split(\n",
        "    urdu_tokens, roman_tokens, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# split temp into val 25% nd test 25%\n",
        "val_urdu, test_urdu, val_roman, test_roman = train_test_split(\n",
        "    temp_urdu, temp_roman, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train set size: {len(train_urdu)}\")\n",
        "print(f\"Validation set size: {len(val_urdu)}\")\n",
        "print(f\"Test set size: {len(test_urdu)}\")\n",
        "\n",
        "# Custom PyTorch Dataset class\n",
        "class UrduRomanDataset(Dataset):\n",
        "    def __init__(self, urdu_sequences, roman_sequences):\n",
        "        self.urdu_sequences = urdu_sequences\n",
        "        self.roman_sequences = roman_sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.urdu_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        urdu_seq = self.urdu_sequences[idx]\n",
        "        roman_seq = self.roman_sequences[idx]\n",
        "\n",
        "        return {\n",
        "            'urdu': torch.tensor(urdu_seq, dtype=torch.long),\n",
        "            'roman': torch.tensor(roman_seq, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create Dataset obj\n",
        "print(\"\\nCreating Dataset obj\")\n",
        "train_dataset = UrduRomanDataset(train_urdu, train_roman)\n",
        "val_dataset = UrduRomanDataset(val_urdu, val_roman)\n",
        "test_dataset = UrduRomanDataset(test_urdu, test_roman)\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Val dataset: {len(val_dataset)} samples\")\n",
        "print(f\"Test dataset: {len(test_dataset)} samples\")\n",
        "\n",
        "# Padding function for DataLoader\n",
        "def pad_batch(batch):\n",
        "    urdu_seqs = [item['urdu'] for item in batch]\n",
        "    roman_seqs = [item['roman'] for item in batch]\n",
        "\n",
        "    # find max length in batch\n",
        "    urdu_max_len = max(len(seq) for seq in urdu_seqs)\n",
        "    roman_max_len = max(len(seq) for seq in roman_seqs)\n",
        "\n",
        "    # Pad sequences\n",
        "    urdu_padded = []\n",
        "    roman_padded = []\n",
        "\n",
        "    for seq in urdu_seqs:\n",
        "        padded = torch.cat([seq, torch.zeros(urdu_max_len - len(seq), dtype=torch.long)])\n",
        "        urdu_padded.append(padded)\n",
        "\n",
        "    for seq in roman_seqs:\n",
        "        padded = torch.cat([seq, torch.zeros(roman_max_len - len(seq), dtype=torch.long)])\n",
        "        roman_padded.append(padded)\n",
        "\n",
        "    urdu_batch = torch.stack(urdu_padded)\n",
        "    roman_batch = torch.stack(roman_padded)\n",
        "\n",
        "    return {\n",
        "        'urdu': urdu_batch,\n",
        "        'roman': roman_batch,\n",
        "        'urdu_lengths': torch.tensor([len(seq) for seq in urdu_seqs], dtype=torch.long),\n",
        "        'roman_lengths': torch.tensor([len(seq) for seq in roman_seqs], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "# Create DataLoaders\n",
        "print(\"creating DataLoaders\")\n",
        "\n",
        "BATCH_SIZE = 32  # You can change this to 64 or 128\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=pad_batch\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=pad_batch\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=pad_batch\n",
        ")\n",
        "\n",
        "print(f\"Train loader batches: {len(train_loader)}\")\n",
        "print(f\"Val loader batches: {len(val_loader)}\")\n",
        "print(f\"Test loader batches: {len(test_loader)}\")\n",
        "\n",
        "# Test one batch\n",
        "print(\"Testing one batch from train loader\")\n",
        "\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"Urdu batch shape: {sample_batch['urdu'].shape} ::: \" + f\"Roman batch shape: {sample_batch['roman'].shape}\" )\n",
        "print(f\"Urdu lengths: {sample_batch['urdu_lengths'][:5]} ::: \" + f\"Roman lengths: {sample_batch['roman_lengths'][:5]}\")\n",
        "\n",
        "print(\"\\nData preparation done next step -> model training\")"
      ],
      "metadata": {
        "id": "hNPUVl3JN2w1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "463d701a-74dc-4b5c-f3fd-7737f270d3e7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizers...\n",
            "Tokenizers loaded successfully!\n",
            "Urdu vocab size: 560, Merges: 500\n",
            "Roman vocab size: 534, Merges: 500\n",
            "\n",
            "Load dataset\n",
            "Loaded 21003 Urdu sentences\n",
            "Loaded 21003 Roman sentences\n",
            "\n",
            "Tokenizing sentences using BPE\n",
            "Tokenized 0 sentences\n",
            "Tokenized 5000 sentences\n",
            "Tokenized 10000 sentences\n",
            "Tokenized 15000 sentences\n",
            "Tokenized 20000 sentences\n",
            "Total sequences tokenized: 21003\n",
            "Creating train/val/test splits\n",
            "Train set size: 10501\n",
            "Validation set size: 5251\n",
            "Test set size: 5251\n",
            "\n",
            "Creating Dataset obj\n",
            "Train dataset: 10501 samples\n",
            "Val dataset: 5251 samples\n",
            "Test dataset: 5251 samples\n",
            "creating DataLoaders\n",
            "Train loader batches: 329\n",
            "Val loader batches: 165\n",
            "Test loader batches: 165\n",
            "Testing one batch from train loader\n",
            "Urdu batch shape: torch.Size([32, 19]) ::: Roman batch shape: torch.Size([32, 21])\n",
            "Urdu lengths: tensor([17, 16, 11, 13, 15]) ::: Roman lengths: tensor([21, 15, 11, 14, 16])\n",
            "\n",
            "Data preparation done next step -> model training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Model Architecture**\n",
        "\n",
        "* Build\ta\tseq2seq\tmodel\twith\ta\tBiLSTM\n",
        "encoder\tand\tan\tLSTM\tdecoder.\n",
        "* Use\t2\tlayers\tin\tthe\tencoder\tand\t4\tlayers\tin\tthe\tdecod"
      ],
      "metadata": {
        "id": "1NNHqjlcL3YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# ENCODER BiLSTM with 2 layers\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "\n",
        "    #Takes Urdu text (token IDs) and encodes it -> context vector\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=2, dropout=0.3):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer converts token IDs -> vectors\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0  # Treat padding as special token\n",
        "        )\n",
        "\n",
        "        # BiLSTM processes sequence in both directions\n",
        "        # bidirectional=True means output size will be hidden_size * 2\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, lengths):\n",
        "\n",
        "        #Forward pass of encoder\n",
        "\n",
        "        # Convert token IDs-> embedding vectors\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pack padded sequence to handle variable lengths\n",
        "        packed_embedded = pack_padded_sequence(\n",
        "            embedded,\n",
        "            lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Pass through BiLSTM\n",
        "        packed_outputs, (hidden, cell) = self.bilstm(packed_embedded)\n",
        "\n",
        "        # Unpack sequence back to padded format\n",
        "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "\n",
        "# DECODER - LSTM with 4 layers\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "\n",
        "   # Takes the encoder output and generates Roman Urdu text one token at a time\n",
        "\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=4, dropout=0.3):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer converts token IDs ->vectors\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        # LSTM unidirectional 4 layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output layer converts hidden state to vocabulry probabilities\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_id, hidden, cell):\n",
        "\n",
        "        #Forward pass of decoder (one step at a time)\n",
        "\n",
        "\n",
        "        # Embed the input token\n",
        "        embedded = self.embedding(input_id)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # Pass through LSTM\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        logits = self.fc_out(output.squeeze(1))\n",
        "\n",
        "        return logits, hidden, cell\n",
        "\n",
        "\n",
        "# seq2seq MODEL\n",
        "\n",
        "class Seq2SeqNMT(nn.Module):\n",
        "\n",
        "\n",
        "    #Combines BiLSTM Encoder (2 layers) and LSTM Decoder (4 layers) for translating Urdu to Roman Urdu\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2SeqNMT, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src_ids, src_lengths, tgt_ids, teacher_forcing_ratio=0.5):\n",
        "\n",
        "\n",
        "        batch_size = src_ids.size(0)\n",
        "        tgt_seq_length = tgt_ids.size(1)\n",
        "        tgt_vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        # Encode the Urdu text\n",
        "        encoder_outputs, encoder_hidden, encoder_cell = self.encoder(src_ids, src_lengths)\n",
        "\n",
        "\n",
        "        # Initialize decoder hidden and cell states from encoder\n",
        "\n",
        "        decoder_hidden, decoder_cell = self._init_decoder_states(\n",
        "            encoder_hidden, encoder_cell, batch_size\n",
        "        )\n",
        "\n",
        "        # Prepare storage for outputs\n",
        "        outputs = torch.zeros(batch_size, tgt_seq_length, tgt_vocab_size).to(self.device)\n",
        "\n",
        "        #  Start with <strt> token usually token ID 2\n",
        "        decoder_input = torch.full((batch_size, 1), 2, dtype=torch.long).to(self.device)\n",
        "\n",
        "        #  Decode step by step\n",
        "        for t in range(tgt_seq_length):\n",
        "            # Get decoder output for current step\n",
        "            logits, decoder_hidden, decoder_cell = self.decoder(\n",
        "                decoder_input, decoder_hidden, decoder_cell\n",
        "            )\n",
        "\n",
        "            # Store the output\n",
        "            outputs[:, t, :] = logits\n",
        "\n",
        "            # Decide what to feed to decoder for next step\n",
        "            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n",
        "\n",
        "            if use_teacher_forcing:\n",
        "                # Use ground truth token as next input\n",
        "                decoder_input = tgt_ids[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                # Use models best prediction\n",
        "                decoder_input = logits.argmax(1).unsqueeze(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _init_decoder_states(self, encoder_hidden, encoder_cell, batch_size):\n",
        "\n",
        "\n",
        "        # Average bidirectional hidden states\n",
        "        encoder_hidden = encoder_hidden.view(\n",
        "            self.encoder.num_layers,\n",
        "            2,\n",
        "            batch_size,\n",
        "            self.encoder.hidden_size\n",
        "        )\n",
        "\n",
        "        encoder_hidden = encoder_hidden.mean(dim=1)\n",
        "\n",
        "        encoder_cell = encoder_cell.view(\n",
        "            self.encoder.num_layers,\n",
        "            2,\n",
        "            batch_size,\n",
        "            self.encoder.hidden_size\n",
        "        )\n",
        "        encoder_cell = encoder_cell.mean(dim=1)\n",
        "\n",
        "        # Expand from 2 encoder layers to 4 decoder layers\n",
        "        decoder_hidden_list = []\n",
        "        decoder_cell_list = []\n",
        "\n",
        "        for layer_idx in range(self.decoder.num_layers):\n",
        "            if layer_idx < encoder_hidden.size(0):\n",
        "                h = encoder_hidden[layer_idx]  # (batch_size, hidden_size)\n",
        "                c = encoder_cell[layer_idx]    # (batch_size, hidden_size)\n",
        "            else:\n",
        "                # Repeat last encoder layer for remaining decoder layers\n",
        "                h = encoder_hidden[-1]\n",
        "                c = encoder_cell[-1]\n",
        "\n",
        "            decoder_hidden_list.append(h.unsqueeze(0))\n",
        "            decoder_cell_list.append(c.unsqueeze(0))\n",
        "\n",
        "        # Concatenate all layers\n",
        "        decoder_hidden = torch.cat(decoder_hidden_list, dim=0)\n",
        "        decoder_cell = torch.cat(decoder_cell_list, dim=0)\n",
        "\n",
        "        return decoder_hidden, decoder_cell\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing Model Architecture...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Device setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Hyperparameters\n",
        "    URDU_VOCAB_SIZE = 560\n",
        "    ROMAN_VOCAB_SIZE = 534\n",
        "    EMBEDDING_DIM = 256\n",
        "    HIDDEN_SIZE = 256\n",
        "    DROPOUT = 0.3\n",
        "\n",
        "    print(f\"\\nHyperparameters:\")\n",
        "    print(f\"Urdu Vocab Size: {URDU_VOCAB_SIZE}\")\n",
        "    print(f\"Roman Vocab Size: {ROMAN_VOCAB_SIZE}\")\n",
        "    print(f\"Embedding Dimension: {EMBEDDING_DIM}\")\n",
        "    print(f\"Hidden Size: {HIDDEN_SIZE}\")\n",
        "    print(f\"Dropout: {DROPOUT}\")\n",
        "\n",
        "    # Create encoder\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Encoder (BiLSTM with 2 layers)...\")\n",
        "    encoder = Encoder(\n",
        "        vocab_size=URDU_VOCAB_SIZE,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=2,\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "    print(f\"Encoder created successfully\")\n",
        "    print(f\"Encoder parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
        "\n",
        "    # Create decoder\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Decoder (LSTM with 4 layers)...\")\n",
        "    decoder = Decoder(\n",
        "        vocab_size=ROMAN_VOCAB_SIZE,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=4,\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "    print(f\"Decoder created successfully\")\n",
        "    print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters()):,}\")\n",
        "\n",
        "    # Create full seq2seq model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Complete Seq2Seq Model...\")\n",
        "    model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "    print(f\"Seq2Seq model created successfully\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nTotal model parameters: {total_params:,}\")\n",
        "\n",
        "    # Test forward pass\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Testing forward pass...\")\n",
        "\n",
        "    batch_size = 4\n",
        "    src_seq_len = 15\n",
        "    tgt_seq_len = 18\n",
        "\n",
        "    # Create dummy input\n",
        "    src_ids = torch.randint(0, URDU_VOCAB_SIZE, (batch_size, src_seq_len)).to(device)\n",
        "    src_lengths = torch.tensor([15, 12, 14, 13]).to(device)\n",
        "    tgt_ids = torch.randint(0, ROMAN_VOCAB_SIZE, (batch_size, tgt_seq_len)).to(device)\n",
        "\n",
        "    print(f\"Input shape - src_ids: {src_ids.shape}, src_lengths: {src_lengths.shape}\")\n",
        "    print(f\"Target shape - tgt_ids: {tgt_ids.shape}\")\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(src_ids, src_lengths, tgt_ids, teacher_forcing_ratio=0.5)\n",
        "\n",
        "    print(f\"Output shape: {outputs.shape}\")\n",
        "    print(f\"Expected shape: ({batch_size}, {tgt_seq_len}, {ROMAN_VOCAB_SIZE})\")\n",
        "\n",
        "    if outputs.shape == (batch_size, tgt_seq_len, ROMAN_VOCAB_SIZE):\n",
        "        print(\"\\n✓ Forward pass successful!\")\n",
        "    else:\n",
        "        print(\"\\n✗ Forward pass failed!\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Model Architecture Testing Complete!\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "cqFIIUCEN38V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6e19ffc-65a5-4dd5-ae05-240d372d8c38"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Model Architecture...\n",
            "============================================================\n",
            "Device: cpu\n",
            "\n",
            "Hyperparameters:\n",
            "Urdu Vocab Size: 560\n",
            "Roman Vocab Size: 534\n",
            "Embedding Dimension: 256\n",
            "Hidden Size: 256\n",
            "Dropout: 0.3\n",
            "\n",
            "============================================================\n",
            "Creating Encoder (BiLSTM with 2 layers)...\n",
            "Encoder created successfully\n",
            "Encoder parameters: 2,772,992\n",
            "\n",
            "============================================================\n",
            "Creating Decoder (LSTM with 4 layers)...\n",
            "Decoder created successfully\n",
            "Decoder parameters: 2,379,286\n",
            "\n",
            "============================================================\n",
            "Creating Complete Seq2Seq Model...\n",
            "Seq2Seq model created successfully\n",
            "\n",
            "Total model parameters: 5,152,278\n",
            "\n",
            "============================================================\n",
            "Testing forward pass...\n",
            "Input shape - src_ids: torch.Size([4, 15]), src_lengths: torch.Size([4])\n",
            "Target shape - tgt_ids: torch.Size([4, 18])\n",
            "Output shape: torch.Size([4, 18, 534])\n",
            "Expected shape: (4, 18, 534)\n",
            "\n",
            "✓ Forward pass successful!\n",
            "\n",
            "============================================================\n",
            "Model Architecture Testing Complete!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# ============================================================\n",
        "# ENCODER - BiLSTM with 2 layers\n",
        "# ============================================================\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM Encoder\n",
        "\n",
        "    Takes Urdu text (token IDs) and encodes it into context vectors\n",
        "    Uses 2 layers as per project requirement\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=2, dropout=0.3):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer - converts token IDs to vectors\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        # BiLSTM - processes sequence in both directions\n",
        "        # Output size will be hidden_size * 2 (forward + backward)\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, lengths):\n",
        "        \"\"\"\n",
        "        Forward pass of encoder\n",
        "\n",
        "        Args:\n",
        "            input_ids: shape (batch_size, seq_length) - tokenized Urdu text\n",
        "            lengths: shape (batch_size,) - actual lengths before padding\n",
        "\n",
        "        Returns:\n",
        "            outputs: shape (batch_size, seq_length, hidden_size*2) - all hidden states\n",
        "            hidden: tuple (hidden, cell) - final states from BiLSTM\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Embed tokens\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Step 2: Pack for efficient processing\n",
        "        packed_embedded = pack_padded_sequence(\n",
        "            embedded,\n",
        "            lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Step 3: BiLSTM forward pass\n",
        "        packed_outputs, (hidden, cell) = self.bilstm(packed_embedded)\n",
        "\n",
        "        # Step 4: Unpack back to padded format\n",
        "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ATTENTION MECHANISM\n",
        "# ============================================================\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Bahdanau Attention Mechanism\n",
        "\n",
        "    Allows decoder to focus on relevant parts of encoder output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, encoder_output_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.attention = nn.Linear(hidden_size + encoder_output_size, 1)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs, src_lengths):\n",
        "        \"\"\"\n",
        "        Calculate attention weights\n",
        "\n",
        "        Args:\n",
        "            decoder_hidden: shape (batch_size, hidden_size)\n",
        "            encoder_outputs: shape (batch_size, src_seq_len, encoder_output_size)\n",
        "            src_lengths: shape (batch_size,) - for masking padding\n",
        "\n",
        "        Returns:\n",
        "            context: shape (batch_size, encoder_output_size) - weighted encoder output\n",
        "            attention_weights: shape (batch_size, src_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, src_seq_len, encoder_output_size = encoder_outputs.size()\n",
        "\n",
        "        # Expand decoder hidden to match encoder sequence length\n",
        "        # decoder_hidden: (batch_size, hidden_size) -> (batch_size, src_seq_len, hidden_size)\n",
        "        decoder_hidden_expanded = decoder_hidden.unsqueeze(1).expand(\n",
        "            batch_size, src_seq_len, -1\n",
        "        )\n",
        "\n",
        "        # Concatenate decoder hidden with encoder outputs\n",
        "        combined = torch.cat(\n",
        "            [decoder_hidden_expanded, encoder_outputs],\n",
        "            dim=2\n",
        "        )  # (batch_size, src_seq_len, hidden_size + encoder_output_size)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        energy = self.attention(combined).squeeze(2)\n",
        "        # energy: (batch_size, src_seq_len)\n",
        "\n",
        "        # Create mask for padding\n",
        "        mask = torch.arange(src_seq_len).unsqueeze(0) < src_lengths.unsqueeze(1)\n",
        "        mask = mask.to(encoder_outputs.device)\n",
        "\n",
        "        # Apply mask (set padding positions to very negative value)\n",
        "        energy = energy.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = torch.softmax(energy, dim=1)\n",
        "\n",
        "        # Handle any NaN values from -inf\n",
        "        attention_weights = attention_weights.masked_fill(~mask, 0)\n",
        "\n",
        "        # Calculate context vector as weighted sum of encoder outputs\n",
        "        context = torch.bmm(\n",
        "            attention_weights.unsqueeze(1),\n",
        "            encoder_outputs\n",
        "        ).squeeze(1)\n",
        "        # context: (batch_size, encoder_output_size)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DECODER - LSTM with 4 layers and Attention\n",
        "# ============================================================\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM Decoder with 4 layers and Attention\n",
        "\n",
        "    Generates Roman Urdu text using encoder outputs and attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size,\n",
        "                 encoder_hidden_size, num_layers=4, dropout=0.3):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        # Encoder outputs have size encoder_hidden_size * 2 (bidirectional)\n",
        "        self.attention = Attention(hidden_size, encoder_hidden_size * 2)\n",
        "\n",
        "        # LSTM takes embedding + context from attention\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim + encoder_hidden_size * 2,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_id, hidden, cell, encoder_outputs, src_lengths):\n",
        "        \"\"\"\n",
        "        Forward pass of decoder (one step at a time)\n",
        "\n",
        "        Args:\n",
        "            input_id: shape (batch_size, 1) - single token\n",
        "            hidden: shape (num_layers, batch_size, hidden_size)\n",
        "            cell: shape (num_layers, batch_size, hidden_size)\n",
        "            encoder_outputs: shape (batch_size, src_seq_len, encoder_hidden_size*2)\n",
        "            src_lengths: shape (batch_size,)\n",
        "\n",
        "        Returns:\n",
        "            logits: shape (batch_size, vocab_size)\n",
        "            hidden: updated hidden state\n",
        "            cell: updated cell state\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Embed input token\n",
        "        embedded = self.embedding(input_id)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # embedded: (batch_size, 1, embedding_dim)\n",
        "\n",
        "        # Step 2: Calculate attention\n",
        "        # Use the top layer hidden state for attention\n",
        "        decoder_hidden_for_attention = hidden[-1]  # (batch_size, hidden_size)\n",
        "        context, attention_weights = self.attention(\n",
        "            decoder_hidden_for_attention,\n",
        "            encoder_outputs,\n",
        "            src_lengths\n",
        "        )\n",
        "        # context: (batch_size, encoder_hidden_size*2)\n",
        "\n",
        "        # Step 3: Concatenate embedding with attention context\n",
        "        lstm_input = torch.cat([embedded.squeeze(1), context], dim=1)\n",
        "        # lstm_input: (batch_size, embedding_dim + encoder_hidden_size*2)\n",
        "        lstm_input = lstm_input.unsqueeze(1)  # Add sequence dimension back\n",
        "        # lstm_input: (batch_size, 1, embedding_dim + encoder_hidden_size*2)\n",
        "\n",
        "        # Step 4: LSTM forward pass\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "        # output: (batch_size, 1, hidden_size)\n",
        "\n",
        "        # Step 5: Project to vocabulary\n",
        "        logits = self.fc_out(output.squeeze(1))\n",
        "        # logits: (batch_size, vocab_size)\n",
        "\n",
        "        return logits, hidden, cell\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# COMPLETE SEQ2SEQ MODEL WITH ATTENTION\n",
        "# ============================================================\n",
        "\n",
        "class Seq2SeqNMT(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Sequence-to-Sequence Model with Attention\n",
        "\n",
        "    BiLSTM Encoder (2 layers) + LSTM Decoder (4 layers) + Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2SeqNMT, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src_ids, src_lengths, tgt_ids, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            src_ids: shape (batch_size, src_seq_len) - Urdu tokens\n",
        "            src_lengths: shape (batch_size,) - actual Urdu lengths\n",
        "            tgt_ids: shape (batch_size, tgt_seq_len) - Roman tokens (ground truth)\n",
        "            teacher_forcing_ratio: probability to use teacher forcing\n",
        "\n",
        "        Returns:\n",
        "            outputs: shape (batch_size, tgt_seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = src_ids.size(0)\n",
        "        tgt_seq_length = tgt_ids.size(1)\n",
        "        tgt_vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        # Step 1: Encode source\n",
        "        encoder_outputs, encoder_hidden, encoder_cell = self.encoder(src_ids, src_lengths)\n",
        "        # encoder_outputs: (batch_size, src_seq_len, hidden_size*2)\n",
        "        # encoder_hidden: (num_layers*2, batch_size, hidden_size)\n",
        "\n",
        "        # Step 2: Initialize decoder states from encoder\n",
        "        decoder_hidden, decoder_cell = self._init_decoder_states(\n",
        "            encoder_hidden, encoder_cell, batch_size\n",
        "        )\n",
        "\n",
        "        # Step 3: Prepare output storage\n",
        "        outputs = torch.zeros(batch_size, tgt_seq_length, tgt_vocab_size).to(self.device)\n",
        "\n",
        "        # Step 4: Start token (usually 2)\n",
        "        decoder_input = torch.full(\n",
        "            (batch_size, 1),\n",
        "            2,\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Step 5: Decode step by step\n",
        "        for t in range(tgt_seq_length):\n",
        "            logits, decoder_hidden, decoder_cell = self.decoder(\n",
        "                decoder_input,\n",
        "                decoder_hidden,\n",
        "                decoder_cell,\n",
        "                encoder_outputs,\n",
        "                src_lengths\n",
        "            )\n",
        "            # logits: (batch_size, vocab_size)\n",
        "\n",
        "            outputs[:, t, :] = logits\n",
        "\n",
        "            # Teacher forcing decision\n",
        "            if torch.rand(1).item() < teacher_forcing_ratio:\n",
        "                decoder_input = tgt_ids[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                decoder_input = logits.argmax(1).unsqueeze(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _init_decoder_states(self, encoder_hidden, encoder_cell, batch_size):\n",
        "        \"\"\"\n",
        "        Initialize decoder states from encoder states using learned projection\n",
        "\n",
        "        This is better than just repeating states\n",
        "        \"\"\"\n",
        "\n",
        "        # encoder_hidden: (num_layers*2, batch_size, hidden_size)\n",
        "        # We need: (decoder_layers, batch_size, hidden_size)\n",
        "\n",
        "        # Average bidirectional states for each encoder layer\n",
        "        encoder_hidden = encoder_hidden.view(\n",
        "            self.encoder.num_layers,\n",
        "            2,\n",
        "            batch_size,\n",
        "            self.encoder.hidden_size\n",
        "        )\n",
        "        encoder_hidden = encoder_hidden.mean(dim=1)\n",
        "        # Now: (num_encoder_layers, batch_size, hidden_size)\n",
        "\n",
        "        encoder_cell = encoder_cell.view(\n",
        "            self.encoder.num_layers,\n",
        "            2,\n",
        "            batch_size,\n",
        "            self.encoder.hidden_size\n",
        "        )\n",
        "        encoder_cell = encoder_cell.mean(dim=1)\n",
        "\n",
        "        # Expand to decoder layers\n",
        "        decoder_hidden_list = []\n",
        "        decoder_cell_list = []\n",
        "\n",
        "        num_encoder_layers = encoder_hidden.size(0)\n",
        "\n",
        "        for layer_idx in range(self.decoder.num_layers):\n",
        "            # Cycle through encoder layers instead of repeating last\n",
        "            encoder_layer_idx = layer_idx % num_encoder_layers\n",
        "\n",
        "            h = encoder_hidden[encoder_layer_idx]  # (batch_size, hidden_size)\n",
        "            c = encoder_cell[encoder_layer_idx]\n",
        "\n",
        "            decoder_hidden_list.append(h.unsqueeze(0))\n",
        "            decoder_cell_list.append(c.unsqueeze(0))\n",
        "\n",
        "        decoder_hidden = torch.cat(decoder_hidden_list, dim=0)\n",
        "        decoder_cell = torch.cat(decoder_cell_list, dim=0)\n",
        "\n",
        "        return decoder_hidden, decoder_cell\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TEST\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing Fixed Model Architecture...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Hyperparameters\n",
        "    URDU_VOCAB_SIZE = 560\n",
        "    ROMAN_VOCAB_SIZE = 534\n",
        "    EMBEDDING_DIM = 256\n",
        "    HIDDEN_SIZE = 256\n",
        "    DROPOUT = 0.3\n",
        "\n",
        "    print(f\"\\nHyperparameters:\")\n",
        "    print(f\"Urdu Vocab Size: {URDU_VOCAB_SIZE}\")\n",
        "    print(f\"Roman Vocab Size: {ROMAN_VOCAB_SIZE}\")\n",
        "    print(f\"Embedding Dimension: {EMBEDDING_DIM}\")\n",
        "    print(f\"Hidden Size: {HIDDEN_SIZE}\")\n",
        "    print(f\"Dropout: {DROPOUT}\")\n",
        "\n",
        "    # Create encoder\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Encoder (BiLSTM with 2 layers)...\")\n",
        "    encoder = Encoder(\n",
        "        vocab_size=URDU_VOCAB_SIZE,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=2,\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "    print(f\"Encoder parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
        "\n",
        "    # Create decoder with attention\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Decoder (LSTM with 4 layers + Attention)...\")\n",
        "    decoder = Decoder(\n",
        "        vocab_size=ROMAN_VOCAB_SIZE,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        encoder_hidden_size=HIDDEN_SIZE,  # FIX: Properly pass encoder hidden size\n",
        "        num_layers=4,\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "    print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters()):,}\")\n",
        "\n",
        "    # Create full model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Complete Seq2Seq Model with Attention...\")\n",
        "    model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total model parameters: {total_params:,}\")\n",
        "\n",
        "    # Test forward pass\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Testing forward pass...\")\n",
        "\n",
        "    batch_size = 4\n",
        "    src_seq_len = 15\n",
        "    tgt_seq_len = 18\n",
        "\n",
        "    src_ids = torch.randint(0, URDU_VOCAB_SIZE, (batch_size, src_seq_len)).to(device)\n",
        "    src_lengths = torch.tensor([15, 12, 14, 13]).to(device)\n",
        "    tgt_ids = torch.randint(0, ROMAN_VOCAB_SIZE, (batch_size, tgt_seq_len)).to(device)\n",
        "\n",
        "    print(f\"Input shape - src_ids: {src_ids.shape}, src_lengths: {src_lengths.shape}\")\n",
        "    print(f\"Target shape - tgt_ids: {tgt_ids.shape}\")\n",
        "\n",
        "    outputs = model(src_ids, src_lengths, tgt_ids, teacher_forcing_ratio=0.5)\n",
        "\n",
        "    print(f\"\\nOutput shape: {outputs.shape}\")\n",
        "    print(f\"Expected shape: ({batch_size}, {tgt_seq_len}, {ROMAN_VOCAB_SIZE})\")\n",
        "\n",
        "    if outputs.shape == (batch_size, tgt_seq_len, ROMAN_VOCAB_SIZE):\n",
        "        print(\"✓ Forward pass successful!\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Model Testing Complete!\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "75-j4tM4g0Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# ============================================================\n",
        "# ENCODER - BiLSTM with 2 layers\n",
        "# ============================================================\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM Encoder\n",
        "\n",
        "    Takes Urdu text (token IDs) and encodes it into context vectors\n",
        "    Uses 2 layers as per project requirement\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=2, dropout=0.3):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer - converts token IDs to vectors\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        # BiLSTM - processes sequence in both directions\n",
        "        # Output size will be hidden_size * 2 (forward + backward)\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, lengths):\n",
        "        \"\"\"\n",
        "        Forward pass of encoder\n",
        "\n",
        "        Args:\n",
        "            input_ids: shape (batch_size, seq_length) - tokenized Urdu text\n",
        "            lengths: shape (batch_size,) - actual lengths before padding\n",
        "\n",
        "        Returns:\n",
        "            outputs: shape (batch_size, seq_length, hidden_size*2) - all hidden states\n",
        "            hidden: tuple (hidden, cell) - final states from BiLSTM\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Embed tokens\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Step 2: Pack for efficient processing\n",
        "        packed_embedded = pack_padded_sequence(\n",
        "            embedded,\n",
        "            lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Step 3: BiLSTM forward pass\n",
        "        packed_outputs, (hidden, cell) = self.bilstm(packed_embedded)\n",
        "\n",
        "        # Step 4: Unpack back to padded format\n",
        "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ATTENTION MECHANISM\n",
        "# ============================================================\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Bahdanau Attention Mechanism\n",
        "\n",
        "    Allows decoder to focus on relevant parts of encoder output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, encoder_output_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.attention = nn.Linear(hidden_size + encoder_output_size, 1)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs, src_lengths):\n",
        "        \"\"\"\n",
        "        Calculate attention weights\n",
        "\n",
        "        Args:\n",
        "            decoder_hidden: shape (batch_size, hidden_size)\n",
        "            encoder_outputs: shape (batch_size, src_seq_len, encoder_output_size)\n",
        "            src_lengths: shape (batch_size,) - for masking padding\n",
        "\n",
        "        Returns:\n",
        "            context: shape (batch_size, encoder_output_size) - weighted encoder output\n",
        "            attention_weights: shape (batch_size, src_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, src_seq_len, encoder_output_size = encoder_outputs.size()\n",
        "\n",
        "        # Expand decoder hidden to match encoder sequence length\n",
        "        # decoder_hidden: (batch_size, hidden_size) -> (batch_size, src_seq_len, hidden_size)\n",
        "        decoder_hidden_expanded = decoder_hidden.unsqueeze(1).expand(\n",
        "            batch_size, src_seq_len, -1\n",
        "        )\n",
        "\n",
        "        # Concatenate decoder hidden with encoder outputs\n",
        "        combined = torch.cat(\n",
        "            [decoder_hidden_expanded, encoder_outputs],\n",
        "            dim=2\n",
        "        )  # (batch_size, src_seq_len, hidden_size + encoder_output_size)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        energy = self.attention(combined).squeeze(2)\n",
        "        # energy: (batch_size, src_seq_len)\n",
        "\n",
        "        # Create mask for padding - FIXED: Add device parameter\n",
        "        mask = torch.arange(src_seq_len, device=encoder_outputs.device).unsqueeze(0) < src_lengths.unsqueeze(1)\n",
        "        mask = mask.to(encoder_outputs.device)\n",
        "\n",
        "        # Apply mask (set padding positions to very negative value)\n",
        "        energy = energy.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = torch.softmax(energy, dim=1)\n",
        "\n",
        "        # Handle any NaN values from -inf\n",
        "        attention_weights = attention_weights.masked_fill(~mask, 0)\n",
        "\n",
        "        # Calculate context vector as weighted sum of encoder outputs\n",
        "        context = torch.bmm(\n",
        "            attention_weights.unsqueeze(1),\n",
        "            encoder_outputs\n",
        "        ).squeeze(1)\n",
        "        # context: (batch_size, encoder_output_size)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DECODER - LSTM with 4 layers and Attention\n",
        "# ============================================================\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM Decoder with 4 layers and Attention\n",
        "\n",
        "    Generates Roman Urdu text using encoder outputs and attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size,\n",
        "                 encoder_hidden_size, num_layers=4, dropout=0.3):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        # Encoder outputs have size encoder_hidden_size * 2 (bidirectional)\n",
        "        self.attention = Attention(hidden_size, encoder_hidden_size * 2)\n",
        "\n",
        "        # LSTM takes embedding + context from attention\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim + encoder_hidden_size * 2,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_id, hidden, cell, encoder_outputs, src_lengths):\n",
        "        \"\"\"\n",
        "        Forward pass of decoder (one step at a time)\n",
        "\n",
        "        Args:\n",
        "            input_id: shape (batch_size, 1) - single token\n",
        "            hidden: shape (num_layers, batch_size, hidden_size)\n",
        "            cell: shape (num_layers, batch_size, hidden_size)\n",
        "            encoder_outputs: shape (batch_size, src_seq_len, encoder_hidden_size*2)\n",
        "            src_lengths: shape (batch_size,)\n",
        "\n",
        "        Returns:\n",
        "            logits: shape (batch_size, vocab_size)\n",
        "            hidden: updated hidden state\n",
        "            cell: updated cell state\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 1: Embed input token\n",
        "        embedded = self.embedding(input_id)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # embedded: (batch_size, 1, embedding_dim)\n",
        "\n",
        "        # Step 2: Calculate attention\n",
        "        # Use the top layer hidden state for attention\n",
        "        decoder_hidden_for_attention = hidden[-1]  # (batch_size, hidden_size)\n",
        "        context, attention_weights = self.attention(\n",
        "            decoder_hidden_for_attention,\n",
        "            encoder_outputs,\n",
        "            src_lengths\n",
        "        )\n",
        "        # context: (batch_size, encoder_hidden_size*2)\n",
        "\n",
        "        # Step 3: Concatenate embedding with attention context\n",
        "        lstm_input = torch.cat([embedded.squeeze(1), context], dim=1)\n",
        "        # lstm_input: (batch_size, embedding_dim + encoder_hidden_size*2)\n",
        "        lstm_input = lstm_input.unsqueeze(1)  # Add sequence dimension back\n",
        "        # lstm_input: (batch_size, 1, embedding_dim + encoder_hidden_size*2)\n",
        "\n",
        "        # Step 4: LSTM forward pass\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "        # output: (batch_size, 1, hidden_size)\n",
        "\n",
        "        # Step 5: Project to vocabulary\n",
        "        logits = self.fc_out(output.squeeze(1))\n",
        "        # logits: (batch_size, vocab_size)\n",
        "\n",
        "        return logits, hidden, cell\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# COMPLETE SEQ2SEQ MODEL WITH ATTENTION\n",
        "# ============================================================\n",
        "\n",
        "class Seq2SeqNMT(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Sequence-to-Sequence Model with Attention\n",
        "\n",
        "    BiLSTM Encoder (2 layers) + LSTM Decoder (4 layers) + Attention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2SeqNMT, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src_ids, src_lengths, tgt_ids, teacher_forcing_ratio=0.5):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            src_ids: shape (batch_size, src_seq_len) - Urdu tokens\n",
        "            src_lengths: shape (batch_size,) - actual Urdu lengths\n",
        "            tgt_ids: shape (batch_size, tgt_seq_len) - Roman tokens (ground truth)\n",
        "            teacher_forcing_ratio: probability to use teacher forcing\n",
        "\n",
        "        Returns:\n",
        "            outputs: shape (batch_size, tgt_seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = src_ids.size(0)\n",
        "        tgt_seq_length = tgt_ids.size(1)\n",
        "        tgt_vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        # Step 1: Encode source\n",
        "        encoder_outputs, encoder_hidden, encoder_cell = self.encoder(src_ids, src_lengths)\n",
        "        # encoder_outputs: (batch_size, src_seq_len, hidden_size*2)\n",
        "        # encoder_hidden: (num_layers*2, batch_size, hidden_size)\n",
        "\n",
        "        # Step 2: Initialize decoder states from encoder\n",
        "        decoder_hidden, decoder_cell = self._init_decoder_states(\n",
        "            encoder_hidden, encoder_cell, batch_size\n",
        "        )\n",
        "\n",
        "        # Step 3: Prepare output storage\n",
        "        outputs = torch.zeros(batch_size, tgt_seq_length, tgt_vocab_size).to(self.device)\n",
        "\n",
        "        # Step 4: Start token (usually 2)\n",
        "        decoder_input = torch.full(\n",
        "            (batch_size, 1),\n",
        "            2,\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Step 5: Decode step by step\n",
        "        for t in range(tgt_seq_length):\n",
        "            logits, decoder_hidden, decoder_cell = self.decoder(\n",
        "                decoder_input,\n",
        "                decoder_hidden,\n",
        "                decoder_cell,\n",
        "                encoder_outputs,\n",
        "                src_lengths\n",
        "            )\n",
        "            # logits: (batch_size, vocab_size)\n",
        "\n",
        "            outputs[:, t, :] = logits\n",
        "\n",
        "            # Teacher forcing decision\n",
        "            if torch.rand(1).item() < teacher_forcing_ratio:\n",
        "                decoder_input = tgt_ids[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                decoder_input = logits.argmax(1).unsqueeze(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _init_decoder_states(self, encoder_hidden, encoder_cell, batch_size):\n",
        "        \"\"\"\n",
        "        Initialize decoder states from encoder states using learned projection\n",
        "\n",
        "        This is better than just repeating states\n",
        "        \"\"\"\n",
        "\n",
        "        # encoder_hidden: (num_layers*2, batch_size, hidden_size)\n",
        "        # We need: (decoder_layers, batch_size, hidden_size)\n",
        "\n",
        "        # Average bidirectional states for each encoder layer\n",
        "        encoder_hidden = encoder_hidden.view(\n",
        "            self.encoder.num_layers,\n",
        "            2,\n",
        "            batch_size,\n",
        "            self.encoder.hidden_size\n",
        "        )\n",
        "        encoder_hidden = encoder_hidden.mean(dim=1)\n",
        "        # Now: (num_encoder_layers, batch_size, hidden_size)\n",
        "\n",
        "        encoder_cell = encoder_cell.view(\n",
        "            self.encoder.num_layers,\n",
        "            2,\n",
        "            batch_size,\n",
        "            self.encoder.hidden_size\n",
        "        )\n",
        "        encoder_cell = encoder_cell.mean(dim=1)\n",
        "\n",
        "        # Expand to decoder layers\n",
        "        decoder_hidden_list = []\n",
        "        decoder_cell_list = []\n",
        "\n",
        "        num_encoder_layers = encoder_hidden.size(0)\n",
        "\n",
        "        for layer_idx in range(self.decoder.num_layers):\n",
        "            # Cycle through encoder layers instead of repeating last\n",
        "            encoder_layer_idx = layer_idx % num_encoder_layers\n",
        "\n",
        "            h = encoder_hidden[encoder_layer_idx]  # (batch_size, hidden_size)\n",
        "            c = encoder_cell[encoder_layer_idx]\n",
        "\n",
        "            decoder_hidden_list.append(h.unsqueeze(0))\n",
        "            decoder_cell_list.append(c.unsqueeze(0))\n",
        "\n",
        "        decoder_hidden = torch.cat(decoder_hidden_list, dim=0)\n",
        "        decoder_cell = torch.cat(decoder_cell_list, dim=0)\n",
        "\n",
        "        return decoder_hidden, decoder_cell\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TEST\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing Fixed Model Architecture...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Hyperparameters\n",
        "    URDU_VOCAB_SIZE = 560\n",
        "    ROMAN_VOCAB_SIZE = 534\n",
        "    EMBEDDING_DIM = 256\n",
        "    HIDDEN_SIZE = 256\n",
        "    DROPOUT = 0.3\n",
        "\n",
        "    print(f\"\\nHyperparameters:\")\n",
        "    print(f\"Urdu Vocab Size: {URDU_VOCAB_SIZE}\")\n",
        "    print(f\"Roman Vocab Size: {ROMAN_VOCAB_SIZE}\")\n",
        "    print(f\"Embedding Dimension: {EMBEDDING_DIM}\")\n",
        "    print(f\"Hidden Size: {HIDDEN_SIZE}\")\n",
        "    print(f\"Dropout: {DROPOUT}\")\n",
        "\n",
        "    # Create encoder\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Encoder (BiLSTM with 2 layers)...\")\n",
        "    encoder = Encoder(\n",
        "        vocab_size=URDU_VOCAB_SIZE,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=2,\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "    print(f\"Encoder parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
        "\n",
        "    # Create decoder with attention\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Decoder (LSTM with 4 layers + Attention)...\")\n",
        "    decoder = Decoder(\n",
        "        vocab_size=ROMAN_VOCAB_SIZE,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        encoder_hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=4,\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "    print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters()):,}\")\n",
        "\n",
        "    # Create full model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Creating Complete Seq2Seq Model with Attention...\")\n",
        "    model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total model parameters: {total_params:,}\")\n",
        "\n",
        "    # Test forward pass\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Testing forward pass...\")\n",
        "\n",
        "    batch_size = 4\n",
        "    src_seq_len = 15\n",
        "    tgt_seq_len = 18\n",
        "\n",
        "    src_ids = torch.randint(0, URDU_VOCAB_SIZE, (batch_size, src_seq_len)).to(device)\n",
        "    src_lengths = torch.tensor([15, 12, 14, 13]).to(device)\n",
        "    tgt_ids = torch.randint(0, ROMAN_VOCAB_SIZE, (batch_size, tgt_seq_len)).to(device)\n",
        "\n",
        "    print(f\"Input shape - src_ids: {src_ids.shape}, src_lengths: {src_lengths.shape}\")\n",
        "    print(f\"Target shape - tgt_ids: {tgt_ids.shape}\")\n",
        "\n",
        "    outputs = model(src_ids, src_lengths, tgt_ids, teacher_forcing_ratio=0.5)\n",
        "\n",
        "    print(f\"\\nOutput shape: {outputs.shape}\")\n",
        "    print(f\"Expected shape: ({batch_size}, {tgt_seq_len}, {ROMAN_VOCAB_SIZE})\")\n",
        "\n",
        "    if outputs.shape == (batch_size, tgt_seq_len, ROMAN_VOCAB_SIZE):\n",
        "        print(\"✓ Forward pass successful!\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Model Testing Complete!\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "y1AVcIkl_LSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "KgVHXX7Xb7kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1: SETUP - Basic stuff we need\n",
        "# ============================================================\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using: {device}\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: DEFINE 3 EXPERIMENTS\n",
        "# We're gonna try 3 different settings to see which works best\n",
        "# ============================================================\n",
        "\n",
        "experiments = {\n",
        "    'Exp1_Baseline': {\n",
        "        'embedding_dim': 256,\n",
        "        'hidden_size': 256,\n",
        "        'learning_rate': 1e-3,\n",
        "        'dropout': 0.3,\n",
        "        'batch_size': 32\n",
        "    },\n",
        "    'Exp2_BiggerModel': {\n",
        "        'embedding_dim': 512,      # Bigger embeddings\n",
        "        'hidden_size': 256,\n",
        "        'learning_rate': 5e-4,     # Slower learning\n",
        "        'dropout': 0.5,            # More dropout (to avoid overfitting)\n",
        "        'batch_size': 32\n",
        "    },\n",
        "    'Exp3_SmallAndFast': {\n",
        "        'embedding_dim': 128,      # Smaller embeddings\n",
        "        'hidden_size': 512,        # But bigger hidden layer\n",
        "        'learning_rate': 1e-4,     # Very slow learning\n",
        "        'dropout': 0.1,            # Less dropout\n",
        "        'batch_size': 64           # Bigger batches\n",
        "    }\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3: TRAINING FUNCTION - This trains for ONE epoch\n",
        "# ============================================================\n",
        "\n",
        "def train_one_epoch(model, train_loader, optimizer, loss_fn, device):\n",
        "    \"\"\"\n",
        "    Train the model for one pass through the training data.\n",
        "\n",
        "    What happens:\n",
        "    1. Go through each batch\n",
        "    2. Pass through model\n",
        "    3. Calculate how wrong we are (loss)\n",
        "    4. Update the model\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()  # Tell model we're training (affects dropout, etc)\n",
        "\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    # Go through all training batches\n",
        "    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "\n",
        "        # Get the data\n",
        "        source_text = batch['urdu'].to(device)\n",
        "        source_lengths = batch['urdu_lengths'].to(device)\n",
        "        target_text = batch['roman'].to(device)\n",
        "\n",
        "        # Step 1: Feed through model\n",
        "        predictions = model(source_text, source_lengths, target_text,\n",
        "                          teacher_forcing_ratio=0.5)\n",
        "\n",
        "        # Step 2: Reshape for loss calculation\n",
        "        # Loss wants: (total_words, vocab_size) and (total_words,)\n",
        "        predictions_flat = predictions.reshape(-1, predictions.size(-1))\n",
        "        target_flat = target_text.reshape(-1)\n",
        "\n",
        "        # Step 3: Calculate loss (how wrong are we?)\n",
        "        loss = loss_fn(predictions_flat, target_flat)\n",
        "\n",
        "        # Step 4: Backpropagation (find where to improve)\n",
        "        optimizer.zero_grad()  # Clear old gradients\n",
        "        loss.backward()  # Calculate gradients\n",
        "\n",
        "        # Prevent gradients from exploding\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Step 5: Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Keep track of total loss\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    # Return average loss and perplexity\n",
        "    avg_loss = total_loss / num_batches\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return avg_loss, perplexity\n",
        "\n",
        "# ============================================================\n",
        "# STEP 4: VALIDATION FUNCTION - Check how we're doing\n",
        "# ============================================================\n",
        "\n",
        "def validate_model(model, val_loader, loss_fn, device):\n",
        "    \"\"\"\n",
        "    Check how good the model is on validation data.\n",
        "    We DON'T update weights here, just check performance.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()  # Tell model we're evaluating\n",
        "\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():  # Don't calculate gradients (saves memory)\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "\n",
        "            # Get data\n",
        "            source_text = batch['urdu'].to(device)\n",
        "            source_lengths = batch['urdu_lengths'].to(device)\n",
        "            target_text = batch['roman'].to(device)\n",
        "\n",
        "            # Feed through model (no teacher forcing)\n",
        "            predictions = model(source_text, source_lengths, target_text,\n",
        "                              teacher_forcing_ratio=0.0)\n",
        "\n",
        "            # Reshape\n",
        "            predictions_flat = predictions.reshape(-1, predictions.size(-1))\n",
        "            target_flat = target_text.reshape(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(predictions_flat, target_flat)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    perplexity = math.exp(avg_loss)\n",
        "\n",
        "    return avg_loss, perplexity\n",
        "\n",
        "# ============================================================\n",
        "# STEP 5: RUN ONE EXPERIMENT - Main training loop\n",
        "# ============================================================\n",
        "\n",
        "def run_one_experiment(exp_name, exp_config, model, train_loader,\n",
        "                       val_loader, test_loader, device):\n",
        "    \"\"\"\n",
        "    Train a model with specific settings and save results.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXPERIMENT: {exp_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Settings: {exp_config}\")\n",
        "\n",
        "    # Setup\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
        "    optimizer = optim.Adam(model.parameters(), lr=exp_config['learning_rate'])\n",
        "\n",
        "    # Track results\n",
        "    results = {\n",
        "        'train_loss': [],\n",
        "        'train_perplexity': [],\n",
        "        'val_loss': [],\n",
        "        'val_perplexity': [],\n",
        "        'epochs': []\n",
        "    }\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    num_epochs = 20\n",
        "\n",
        "    # Training loop - do 20 epochs\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_perp = train_one_epoch(model, train_loader,\n",
        "                                                optimizer, loss_fn, device)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_perp = validate_model(model, val_loader, loss_fn, device)\n",
        "\n",
        "        # Save results\n",
        "        results['epochs'].append(epoch)\n",
        "        results['train_loss'].append(train_loss)\n",
        "        results['train_perplexity'].append(train_perp)\n",
        "        results['val_loss'].append(val_loss)\n",
        "        results['val_perplexity'].append(val_perp)\n",
        "\n",
        "        # Print what happened this epoch\n",
        "        print(f\"Train - Loss: {train_loss:.4f}, Perplexity: {train_perp:.4f}\")\n",
        "        print(f\"Val   - Loss: {val_loss:.4f}, Perplexity: {val_perp:.4f}\")\n",
        "\n",
        "        # If this is our best so far, save the model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), f'/content/drive/MyDrive/ANLP/project_1/best_model_{exp_name}.pth')\n",
        "            print(f\"  -> Saved best model!\")\n",
        "\n",
        "        # Every 5 epochs, also save a checkpoint\n",
        "        if epoch % 5 == 0:\n",
        "            torch.save(model.state_dict(), f'/content/drive/MyDrive/ANLP/project_1/checkpoint_{exp_name}_epoch{epoch}.pth')\n",
        "\n",
        "    # After training, check test set\n",
        "    print(f\"\\nChecking on test data...\")\n",
        "    test_loss, test_perp = validate_model(model, test_loader, loss_fn, device)\n",
        "    print(f\"Test - Loss: {test_loss:.4f}, Perplexity: {test_perp:.4f}\")\n",
        "\n",
        "    # Save results to file\n",
        "    results['best_val_loss'] = best_val_loss\n",
        "    results['best_epoch'] = best_epoch\n",
        "    results['test_loss'] = test_loss\n",
        "    results['test_perplexity'] = test_perp\n",
        "\n",
        "    with open(f'results_{exp_name}.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"\\nResults saved to: results_{exp_name}.json\")\n",
        "    print(f\"Best model saved to: best_model_{exp_name}.pth\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================\n",
        "# STEP 6: COMPARE RESULTS - See which experiment was best\n",
        "# ============================================================\n",
        "\n",
        "def compare_all_experiments(all_results, experiment_names):\n",
        "    \"\"\"\n",
        "    Show a table comparing all experiments.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"COMPARISON OF ALL EXPERIMENTS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    print(f\"{'Experiment':<20} {'Best Val Loss':<15} {'Test Loss':<15} {'Test Perplexity':<15}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for exp_name, results in zip(experiment_names, all_results):\n",
        "        best_val = min(results['val_loss'])\n",
        "        test_loss = results['test_loss']\n",
        "        test_perp = results['test_perplexity']\n",
        "\n",
        "        print(f\"{exp_name:<20} {best_val:<15.4f} {test_loss:<15.4f} {test_perp:<15.4f}\")\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "# ============================================================\n",
        "# STEP 7: PLOT RESULTS - Draw graphs\n",
        "# ============================================================\n",
        "\n",
        "def plot_results(all_results, experiment_names):\n",
        "    \"\"\"\n",
        "    Draw nice graphs of training.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create figure with 2 graphs\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss graph\n",
        "    for results, name in zip(all_results, experiment_names):\n",
        "        ax1.plot(results['epochs'], results['train_loss'],\n",
        "                label=f\"{name} Train\", marker='o')\n",
        "        ax1.plot(results['epochs'], results['val_loss'],\n",
        "                label=f\"{name} Val\", marker='s', linestyle='--')\n",
        "\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Loss Over Time')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Perplexity graph\n",
        "    for results, name in zip(all_results, experiment_names):\n",
        "        ax2.plot(results['epochs'], results['train_perplexity'],\n",
        "                label=f\"{name} Train\", marker='o')\n",
        "        ax2.plot(results['epochs'], results['val_perplexity'],\n",
        "                label=f\"{name} Val\", marker='s', linestyle='--')\n",
        "\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Perplexity')\n",
        "    ax2.set_title('Perplexity Over Time')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/ANLP/project_1/experiment_comparison.png', dpi=200)\n",
        "    print(\"\\nGraph saved as: experiment_comparison.png\")\n",
        "    plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# STEP 8: MAIN - Run everything\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"Starting training...\\n\")\n",
        "\n",
        "    # NOTE: You need to have these from previous steps:\n",
        "    # - train_loader\n",
        "    # - val_loader\n",
        "    # - test_loader\n",
        "    # - model (with encoder + decoder)\n",
        "    # - device\n",
        "\n",
        "    print(\"Make sure you have these ready:\")\n",
        "    print(\"  1. train_loader (from data prep)\")\n",
        "    print(\"  2. val_loader (from data prep)\")\n",
        "    print(\"  3. test_loader (from data prep)\")\n",
        "    print(\"  4. model (from model architecture)\")\n",
        "    print(\"  5. device defined\")\n",
        "\n",
        "    print(\"\\nWhen ready, uncomment and run this code:\\n\")\n",
        "\n",
        "    code_to_run = \"\"\"\n",
        "# Run all 3 experiments\n",
        "all_results = []\n",
        "\n",
        "for exp_name, exp_config in experiments.items():\n",
        "    # Create a new model for this experiment\n",
        "    from model_architecture import Encoder, Decoder, Seq2SeqNMT\n",
        "\n",
        "    encoder = Encoder(\n",
        "        vocab_size=560,\n",
        "        embedding_dim=exp_config['embedding_dim'],\n",
        "        hidden_size=exp_config['hidden_size'],\n",
        "        num_layers=2,\n",
        "        dropout=exp_config['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    decoder = Decoder(\n",
        "        vocab_size=534,\n",
        "        embedding_dim=exp_config['embedding_dim'],\n",
        "        hidden_size=exp_config['hidden_size'],\n",
        "        encoder_hidden_size=exp_config['hidden_size'],\n",
        "        num_layers=4,\n",
        "        dropout=exp_config['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "\n",
        "    # Train this model\n",
        "    results = run_one_experiment(\n",
        "        exp_name, exp_config, model,\n",
        "        train_loader, val_loader, test_loader, device\n",
        "    )\n",
        "\n",
        "    all_results.append(results)\n",
        "\n",
        "# Show comparison\n",
        "compare_all_experiments(all_results, list(experiments.keys()))\n",
        "\n",
        "# Draw graphs\n",
        "plot_results(all_results, list(experiments.keys()))\n",
        "\n",
        "print(\"\\\\nDone! All results saved.\")\n",
        "\"\"\"\n",
        "\n",
        "    print(code_to_run)"
      ],
      "metadata": {
        "id": "icTQRcTOkig9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if these exist in your notebook\n",
        "print(\"Checking prerequisites...\")\n",
        "print(f\"train_loader exists: {'train_loader' in dir()}\")\n",
        "print(f\"val_loader exists: {'val_loader' in dir()}\")\n",
        "print(f\"test_loader exists: {'test_loader' in dir()}\")\n",
        "print(f\"device: {device}\")"
      ],
      "metadata": {
        "id": "lrxso7JKyzFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you have imported the model classes\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Now run all 3 experiments\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING ALL 3 EXPERIMENTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for exp_name, exp_config in experiments.items():\n",
        "    print(f\"\\n\\nCreating model for {exp_name}...\")\n",
        "\n",
        "    # Import model classes (already defined in your notebook)\n",
        "    # If they give error, run the model_architecture cell again\n",
        "\n",
        "    encoder = Encoder(\n",
        "        vocab_size=560,\n",
        "        embedding_dim=exp_config['embedding_dim'],\n",
        "        hidden_size=exp_config['hidden_size'],\n",
        "        num_layers=2,\n",
        "        dropout=exp_config['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    decoder = Decoder(\n",
        "        vocab_size=534,\n",
        "        embedding_dim=exp_config['embedding_dim'],\n",
        "        hidden_size=exp_config['hidden_size'],\n",
        "        encoder_hidden_size=exp_config['hidden_size'],\n",
        "        num_layers=4,\n",
        "        dropout=exp_config['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "\n",
        "    print(f\"Model created. Starting training...\\n\")\n",
        "\n",
        "    # Train this model\n",
        "    results = run_one_experiment(\n",
        "        exp_name, exp_config, model,\n",
        "        train_loader, val_loader, test_loader, device\n",
        "    )\n",
        "\n",
        "    all_results.append(results)\n",
        "    print(f\"\\n{exp_name} FINISHED!\")\n",
        "\n",
        "# Show comparison\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "compare_all_experiments(all_results, list(experiments.keys()))\n",
        "\n",
        "# Draw graphs\n",
        "print(\"\\n\\nCreating graphs...\")\n",
        "plot_results(all_results, list(experiments.keys()))\n",
        "\n",
        "print(\"\\nDone! All results saved.\")"
      ],
      "metadata": {
        "id": "jSaloKPLzbkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "blu"
      ],
      "metadata": {
        "id": "TrSxGlKGK81W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# My Evaluation Code - Testing BLEU and CER\n",
        "# ============================================================\n",
        "\n",
        "# okay so first i need to install this package for character errors\n",
        "print(\"installing editdistance...\")\n",
        "try:\n",
        "    import editdistance\n",
        "    print(\"already installed, cool!\")\n",
        "except:\n",
        "    !pip install editdistance\n",
        "    import editdistance\n",
        "\n",
        "import math\n",
        "import json\n",
        "\n",
        "print(\"\\nStarting evaluation...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# BLEU Score Function\n",
        "# this checks how good our translations are\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def calculate_bleu(correct, predicted):\n",
        "    \"\"\"\n",
        "    BLEU score - basically checks how many words match\n",
        "    1.0 = perfect, 0.0 = completely wrong\n",
        "    \"\"\"\n",
        "\n",
        "    # make everything lowercase and split into words\n",
        "    correct_words = correct.lower().split()\n",
        "    predicted_words = predicted.lower().split()\n",
        "\n",
        "    # if we didn't predict anything, score is zero\n",
        "    if len(predicted_words) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # check 1-word, 2-word, 3-word, 4-word matches\n",
        "    scores = []\n",
        "\n",
        "    for n in [1,2,3,4]:  # n-gram sizes\n",
        "        # get all n-grams from correct answer\n",
        "        correct_ngrams = []\n",
        "        for i in range(len(correct_words) - n + 1):\n",
        "            ngram = tuple(correct_words[i:i+n])\n",
        "            correct_ngrams.append(ngram)\n",
        "\n",
        "        # get all n-grams from our prediction\n",
        "        pred_ngrams = []\n",
        "        for i in range(len(predicted_words) - n + 1):\n",
        "            ngram = tuple(predicted_words[i:i+n])\n",
        "            pred_ngrams.append(ngram)\n",
        "\n",
        "        # count matches\n",
        "        matches = 0\n",
        "        correct_copy = correct_ngrams.copy()\n",
        "        for ngram in pred_ngrams:\n",
        "            if ngram in correct_copy:\n",
        "                matches += 1\n",
        "                correct_copy.remove(ngram)  # dont count twice\n",
        "\n",
        "        # calculate precision for this n\n",
        "        if len(pred_ngrams) > 0:\n",
        "            precision = matches / len(pred_ngrams)\n",
        "        else:\n",
        "            precision = 0\n",
        "\n",
        "        scores.append(precision)\n",
        "\n",
        "    # if any score is 0, bleu is 0\n",
        "    if min(scores) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # calculate geometric mean (dont worry about the math)\n",
        "    bleu = 1.0\n",
        "    for s in scores:\n",
        "        bleu = bleu * s\n",
        "    bleu = bleu ** (1/4)  # fourth root\n",
        "\n",
        "    return bleu\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CER Function - counts character mistakes\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def calculate_cer(correct, predicted):\n",
        "    \"\"\"\n",
        "    Character Error Rate\n",
        "    counts how many characters are different\n",
        "    lower is better\n",
        "    \"\"\"\n",
        "\n",
        "    distance = editdistance.eval(correct, predicted)\n",
        "\n",
        "    if len(correct) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cer = distance / len(correct)\n",
        "    return cer\n",
        "\n",
        "print(\"functions created!\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Translation Function\n",
        "# takes urdu tokens and gives back roman urdu text\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def translate(model, urdu_tokens, roman_bpe, device):\n",
        "    \"\"\"translate one sentence\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # prepare input\n",
        "        src = torch.tensor([urdu_tokens]).to(device)\n",
        "        src_len = torch.tensor([len(urdu_tokens)]).to(device)\n",
        "\n",
        "        # encode urdu\n",
        "        enc_outputs, enc_hidden, enc_cell = model.encoder(src, src_len)\n",
        "\n",
        "        # initialize decoder\n",
        "        dec_h, dec_c = model._init_decoder_states(enc_hidden, enc_cell, 1)\n",
        "\n",
        "        # start token\n",
        "        current = torch.tensor([[2]]).to(device)  # 2 = <SOS>\n",
        "\n",
        "        # generate tokens\n",
        "        output_tokens = []\n",
        "        for _ in range(50):  # max 50 tokens\n",
        "            logits, dec_h, dec_c = model.decoder(current, dec_h, dec_c)\n",
        "            next_token = logits.argmax(-1).item()\n",
        "\n",
        "            # stop if end token or padding\n",
        "            if next_token == 3 or next_token == 0:\n",
        "                break\n",
        "\n",
        "            output_tokens.append(next_token)\n",
        "            current = torch.tensor([[next_token]]).to(device)\n",
        "\n",
        "        # decode to text\n",
        "        try:\n",
        "            text = roman_bpe.decode(output_tokens)\n",
        "        except:\n",
        "            text = \"\"\n",
        "\n",
        "        return text\n",
        "\n",
        "print(\"translation function ready!\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Load Model\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nloading best model...\")\n",
        "\n",
        "# create encoder\n",
        "encoder = Encoder(\n",
        "    vocab_size=560,\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "# create decoder\n",
        "decoder = Decoder(\n",
        "    vocab_size=534,\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=4,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "# combine them\n",
        "model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "\n",
        "# load trained weights\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.eval()\n",
        "\n",
        "print(\"model loaded!\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Test on examples\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\ntesting model on examples...\")\n",
        "\n",
        "# im testing on 20 examples (can change this)\n",
        "num_test = 20\n",
        "\n",
        "bleu_scores = []\n",
        "cer_scores = []\n",
        "examples = []  # save some examples to show\n",
        "\n",
        "print(f\"testing {num_test} examples...\\n\")\n",
        "\n",
        "for i in range(num_test):\n",
        "\n",
        "    # get test example\n",
        "    sample = test_dataset[i]\n",
        "    urdu_toks = sample['urdu'].tolist()\n",
        "    roman_toks = sample['roman'].tolist()\n",
        "\n",
        "    # clean tokens (remove padding etc)\n",
        "    urdu_clean = [t for t in urdu_toks if t not in [0,2,3]]\n",
        "    roman_clean = [t for t in roman_toks if t not in [0,2,3]]\n",
        "\n",
        "    # decode to text\n",
        "    try:\n",
        "        urdu_text = urdu_bpe.decode(urdu_clean)\n",
        "        correct_roman = roman_bpe.decode(roman_clean)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    if not correct_roman.strip():\n",
        "        continue\n",
        "\n",
        "    # translate using model\n",
        "    predicted_roman = translate(model, urdu_toks, roman_bpe, device)\n",
        "\n",
        "    # calculate scores\n",
        "    bleu = calculate_bleu(correct_roman, predicted_roman)\n",
        "    cer = calculate_cer(correct_roman, predicted_roman)\n",
        "\n",
        "    bleu_scores.append(bleu)\n",
        "    cer_scores.append(cer)\n",
        "\n",
        "    # save first 10 for showing later\n",
        "    if len(examples) < 10:\n",
        "        examples.append({\n",
        "            'urdu': urdu_text,\n",
        "            'correct': correct_roman,\n",
        "            'predicted': predicted_roman,\n",
        "            'bleu': bleu,\n",
        "            'cer': cer\n",
        "        })\n",
        "\n",
        "    # show progress every 5 examples\n",
        "    if (i+1) % 5 == 0:\n",
        "        print(f\"done {i+1} examples...\")\n",
        "\n",
        "print(f\"\\ntesting complete!\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Calculate averages\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "avg_cer = sum(cer_scores) / len(cer_scores)\n",
        "\n",
        "print(f\"\\nAverage BLEU: {avg_bleu:.4f}\")\n",
        "print(f\"Average CER:  {avg_cer:.4f}\")\n",
        "print(f\"Perplexity:   17.53\")\n",
        "\n",
        "print(f\"\\nwhat these mean:\")\n",
        "print(f\"BLEU = {avg_bleu:.4f} (closer to 1.0 is better)\")\n",
        "print(f\"CER = {avg_cer:.4f} (closer to 0.0 is better)\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Show examples\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXAMPLE TRANSLATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for idx, ex in enumerate(examples, 1):\n",
        "    print(f\"\\nExample {idx}:\")\n",
        "    print(f\"Urdu:      {ex['urdu']}\")\n",
        "    print(f\"Expected:  {ex['correct']}\")\n",
        "    print(f\"Got:       {ex['predicted']}\")\n",
        "    print(f\"BLEU: {ex['bleu']:.3f}, CER: {ex['cer']:.3f}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Save results\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nsaving results...\")\n",
        "\n",
        "results = {\n",
        "    'model': 'Exp1_Baseline',\n",
        "    'bleu': avg_bleu,\n",
        "    'cer': avg_cer,\n",
        "    'perplexity': 17.53,\n",
        "    'num_examples': len(bleu_scores),\n",
        "    'examples': examples\n",
        "}\n",
        "\n",
        "# save to file\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/eval_results.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"saved to eval_results.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DONE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nYour scores:\")\n",
        "print(f\"BLEU: {avg_bleu:.4f}\")\n",
        "print(f\"CER:  {avg_cer:.4f}\")\n",
        "print(f\"\\nNow we can write the blog post!\")"
      ],
      "metadata": {
        "id": "8qKkq0BY4DDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import Levenshtein\n",
        "\n",
        "# calculate bleu score\n",
        "def calculate_bleu(model, test_data, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    bleu_scores = []\n",
        "\n",
        "    for i in range(len(test_data)):\n",
        "        src = test_data[i][0]  # urdu\n",
        "        tgt = test_data[i][1]  # roman\n",
        "\n",
        "        # translate\n",
        "        prediction = translate_sentence(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        # get reference\n",
        "        reference = [list(tgt)]\n",
        "        hypothesis = list(prediction)\n",
        "\n",
        "        # calculate bleu\n",
        "        score = sentence_bleu(reference, hypothesis)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    return avg_bleu * 100\n",
        "\n",
        "# calculate cer\n",
        "def calculate_cer(model, test_data, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    total_distance = 0\n",
        "    total_length = 0\n",
        "\n",
        "    for i in range(len(test_data)):\n",
        "        src = test_data[i][0]\n",
        "        tgt = test_data[i][1]\n",
        "\n",
        "        # translate\n",
        "        prediction = translate_sentence(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        # calculate distance\n",
        "        distance = Levenshtein.distance(prediction, tgt)\n",
        "        total_distance += distance\n",
        "        total_length += len(tgt)\n",
        "\n",
        "    cer = (total_distance / total_length) * 100\n",
        "    return cer\n",
        "\n",
        "# translate one sentence\n",
        "def translate_sentence(model, src_text, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    # convert to indices\n",
        "    src_indices = [src_vocab['<sos>']]\n",
        "    for char in src_text:\n",
        "        if char in src_vocab:\n",
        "            src_indices.append(src_vocab[char])\n",
        "        else:\n",
        "            src_indices.append(src_vocab['<unk>'])\n",
        "    src_indices.append(src_vocab['<eos>'])\n",
        "\n",
        "    # to tensor\n",
        "    src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # encode\n",
        "        enc_output, hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "        # fix hidden for decoder\n",
        "        hidden = model.init_decoder_state(hidden, cell, 1)[0]\n",
        "        cell = model.init_decoder_state(hidden, cell, 1)[1]\n",
        "\n",
        "        # start decoding\n",
        "        input_idx = tgt_vocab['<sos>']\n",
        "        output_chars = []\n",
        "\n",
        "        for _ in range(100):  # max length\n",
        "            input_tensor = torch.tensor([input_idx]).unsqueeze(0).to(device)\n",
        "\n",
        "            output, hidden, cell = model.decoder(input_tensor, hidden, cell)\n",
        "\n",
        "            predicted_idx = output.argmax(1).item()\n",
        "\n",
        "            if predicted_idx == tgt_vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "            # get character\n",
        "            for char, idx in tgt_vocab.items():\n",
        "                if idx == predicted_idx:\n",
        "                    if char not in ['<pad>', '<sos>', '<eos>', '<unk>']:\n",
        "                        output_chars.append(char)\n",
        "                    break\n",
        "\n",
        "            input_idx = predicted_idx\n",
        "\n",
        "    return ''.join(output_chars)\n",
        "\n",
        "# load best model\n",
        "print(\"loading best model...\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "\n",
        "print(\"calculating bleu score...\")\n",
        "bleu = calculate_bleu(model, test_data[:100], urdu_vocab, roman_vocab)  # test on 100 samples\n",
        "print(f\"BLEU Score: {bleu:.2f}\")\n",
        "\n",
        "print(\"calculating cer...\")\n",
        "cer = calculate_cer(model, test_data[:100], urdu_vocab, roman_vocab)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show some translations\n",
        "print(\"\\nSample Translations:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(5):\n",
        "    src = test_urdu[i]\n",
        "    actual = test_roman[i]\n",
        "    predicted = translate_sentence(model, src, urdu_vocab, roman_vocab)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"Urdu:      {src}\")\n",
        "    print(f\"Actual:    {actual}\")\n",
        "    print(f\"Predicted: {predicted}\")\n",
        "    print(\"-\"*60)"
      ],
      "metadata": {
        "id": "lGEAeGYJK-YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk python-Levenshtein"
      ],
      "metadata": {
        "id": "pKUh0yXgNCkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install required libraries\n",
        "!pip install python-Levenshtein -q\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import Levenshtein\n",
        "\n",
        "# calculate bleu score\n",
        "def calculate_bleu(model, test_data, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    bleu_scores = []\n",
        "\n",
        "    for i in range(len(test_data)):\n",
        "        src = test_data[i][0]  # urdu\n",
        "        tgt = test_data[i][1]  # roman\n",
        "\n",
        "        # translate\n",
        "        prediction = translate_sentence(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        # get reference\n",
        "        reference = [list(tgt)]\n",
        "        hypothesis = list(prediction)\n",
        "\n",
        "        # calculate bleu\n",
        "        score = sentence_bleu(reference, hypothesis)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    return avg_bleu * 100\n",
        "\n",
        "# calculate cer\n",
        "def calculate_cer(model, test_data, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    total_distance = 0\n",
        "    total_length = 0\n",
        "\n",
        "    for i in range(len(test_data)):\n",
        "        src = test_data[i][0]\n",
        "        tgt = test_data[i][1]\n",
        "\n",
        "        # translate\n",
        "        prediction = translate_sentence(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        # calculate distance\n",
        "        distance = Levenshtein.distance(prediction, tgt)\n",
        "        total_distance += distance\n",
        "        total_length += len(tgt)\n",
        "\n",
        "    cer = (total_distance / total_length) * 100\n",
        "    return cer\n",
        "\n",
        "# translate one sentence\n",
        "def translate_sentence(model, src_text, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    # convert to indices\n",
        "    src_indices = [src_vocab['<sos>']]\n",
        "    for char in src_text:\n",
        "        if char in src_vocab:\n",
        "            src_indices.append(src_vocab[char])\n",
        "        else:\n",
        "            src_indices.append(src_vocab['<unk>'])\n",
        "    src_indices.append(src_vocab['<eos>'])\n",
        "\n",
        "    # to tensor\n",
        "    src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # encode\n",
        "        enc_output, hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "        # fix hidden for decoder\n",
        "        hidden = model.init_decoder_state(hidden, cell, 1)[0]\n",
        "        cell = model.init_decoder_state(hidden, cell, 1)[1]\n",
        "\n",
        "        # start decoding\n",
        "        input_idx = tgt_vocab['<sos>']\n",
        "        output_chars = []\n",
        "\n",
        "        for _ in range(100):  # max length\n",
        "            input_tensor = torch.tensor([input_idx]).unsqueeze(0).to(device)\n",
        "\n",
        "            output, hidden, cell = model.decoder(input_tensor, hidden, cell)\n",
        "\n",
        "            predicted_idx = output.argmax(1).item()\n",
        "\n",
        "            if predicted_idx == tgt_vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "            # get character\n",
        "            for char, idx in tgt_vocab.items():\n",
        "                if idx == predicted_idx:\n",
        "                    if char not in ['<pad>', '<sos>', '<eos>', '<unk>']:\n",
        "                        output_chars.append(char)\n",
        "                    break\n",
        "\n",
        "            input_idx = predicted_idx\n",
        "\n",
        "    return ''.join(output_chars)\n",
        "\n",
        "# load best model\n",
        "print(\"loading best model...\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "\n",
        "print(\"calculating bleu score...\")\n",
        "bleu = calculate_bleu(model, test_data[:100], urdu_vocab, roman_vocab)  # test on 100 samples\n",
        "print(f\"BLEU Score: {bleu:.2f}\")\n",
        "\n",
        "print(\"calculating cer...\")\n",
        "cer = calculate_cer(model, test_data[:100], urdu_vocab, roman_vocab)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show some translations\n",
        "print(\"\\nSample Translations:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(5):\n",
        "    src = test_urdu[i]\n",
        "    actual = test_roman[i]\n",
        "    predicted = translate_sentence(model, src, urdu_vocab, roman_vocab)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"Urdu:      {src}\")\n",
        "    print(f\"Actual:    {actual}\")\n",
        "    print(f\"Predicted: {predicted}\")\n",
        "    print(\"-\"*60)"
      ],
      "metadata": {
        "id": "jce4j5oDNx8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install required libraries\n",
        "!pip install python-Levenshtein -q\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import Levenshtein\n",
        "import torch\n",
        "\n",
        "# translate one sentence\n",
        "def translate_sentence(model, src_text, src_vocab, tgt_vocab, device):\n",
        "    model.eval()\n",
        "\n",
        "    # convert to indices\n",
        "    src_indices = [src_vocab['<sos>']]\n",
        "    for char in src_text:\n",
        "        if char in src_vocab:\n",
        "            src_indices.append(src_vocab[char])\n",
        "        else:\n",
        "            src_indices.append(src_vocab['<unk>'])\n",
        "    src_indices.append(src_vocab['<eos>'])\n",
        "\n",
        "    # to tensor\n",
        "    src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # encode\n",
        "        enc_output, hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "        # fix hidden for decoder\n",
        "        hidden, cell = model.init_decoder_state(hidden, cell, 1)\n",
        "\n",
        "        # start decoding\n",
        "        input_idx = tgt_vocab['<sos>']\n",
        "        output_chars = []\n",
        "\n",
        "        for _ in range(100):\n",
        "            input_tensor = torch.tensor([input_idx]).unsqueeze(0).to(device)\n",
        "\n",
        "            output, hidden, cell = model.decoder(input_tensor, hidden, cell)\n",
        "\n",
        "            predicted_idx = output.argmax(1).item()\n",
        "\n",
        "            if predicted_idx == tgt_vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "            # get character\n",
        "            for char, idx in tgt_vocab.items():\n",
        "                if idx == predicted_idx:\n",
        "                    if char not in ['<pad>', '<sos>', '<eos>', '<unk>']:\n",
        "                        output_chars.append(char)\n",
        "                    break\n",
        "\n",
        "            input_idx = predicted_idx\n",
        "\n",
        "    return ''.join(output_chars)\n",
        "\n",
        "# calculate bleu score\n",
        "def calculate_bleu(model, test_urdu, test_roman, src_vocab, tgt_vocab, device):\n",
        "    model.eval()\n",
        "\n",
        "    bleu_scores = []\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        # translate\n",
        "        prediction = translate_sentence(model, src, src_vocab, tgt_vocab, device)\n",
        "\n",
        "        # get reference\n",
        "        reference = [list(tgt)]\n",
        "        hypothesis = list(prediction)\n",
        "\n",
        "        # calculate bleu\n",
        "        score = sentence_bleu(reference, hypothesis)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    return avg_bleu * 100\n",
        "\n",
        "# calculate cer\n",
        "def calculate_cer(model, test_urdu, test_roman, src_vocab, tgt_vocab, device):\n",
        "    model.eval()\n",
        "\n",
        "    total_distance = 0\n",
        "    total_length = 0\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        # translate\n",
        "        prediction = translate_sentence(model, src, src_vocab, tgt_vocab, device)\n",
        "\n",
        "        # calculate distance\n",
        "        distance = Levenshtein.distance(prediction, tgt)\n",
        "        total_distance += distance\n",
        "        total_length += len(tgt)\n",
        "\n",
        "    cer = (total_distance / total_length) * 100\n",
        "    return cer\n",
        "\n",
        "# create model with EXACT same settings as Exp1_Baseline\n",
        "print(\"creating model with baseline settings...\")\n",
        "embedding_dim = 256\n",
        "hidden_size = 256\n",
        "dropout = 0.3\n",
        "\n",
        "encoder = Encoder(len(urdu_vocab), embedding_dim, hidden_size, n_layers=2, dropout=dropout)\n",
        "decoder = Decoder(len(roman_vocab), embedding_dim, hidden_size, n_layers=4, dropout=dropout)\n",
        "model = Seq2SeqNMT(encoder, decoder, device)\n",
        "\n",
        "# load saved weights\n",
        "print(\"loading best model...\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"model loaded successfully!\")\n",
        "\n",
        "# calculate metrics\n",
        "print(\"\\ncalculating bleu score...\")\n",
        "bleu = calculate_bleu(model, test_urdu, test_roman, urdu_vocab, roman_vocab, device)\n",
        "print(f\"BLEU Score: {bleu:.2f}\")\n",
        "\n",
        "print(\"\\ncalculating cer...\")\n",
        "cer = calculate_cer(model, test_urdu, test_roman, urdu_vocab, roman_vocab, device)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show some translations\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Sample Translations:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i in range(5):\n",
        "    src = test_urdu[i]\n",
        "    actual = test_roman[i]\n",
        "    predicted = translate_sentence(model, src, urdu_vocab, roman_vocab, device)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"Urdu:      {src}\")\n",
        "    print(f\"Actual:    {actual}\")\n",
        "    print(f\"Predicted: {predicted}\")\n",
        "    print(\"-\"*70)"
      ],
      "metadata": {
        "id": "KjY58t5COfYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install required libraries\n",
        "!pip install python-Levenshtein -q\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import Levenshtein\n",
        "import torch\n",
        "\n",
        "# translate one sentence\n",
        "def translate_sentence(model, src_text, src_vocab, tgt_vocab, device):\n",
        "    model.eval()\n",
        "\n",
        "    # convert to indices\n",
        "    src_indices = [src_vocab['<sos>']]\n",
        "    for char in src_text:\n",
        "        if char in src_vocab:\n",
        "            src_indices.append(src_vocab[char])\n",
        "        else:\n",
        "            src_indices.append(src_vocab['<unk>'])\n",
        "    src_indices.append(src_vocab['<eos>'])\n",
        "\n",
        "    # to tensor\n",
        "    src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # encode\n",
        "        enc_output, hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "        # fix hidden for decoder\n",
        "        hidden, cell = model.init_decoder_state(hidden, cell, 1)\n",
        "\n",
        "        # start decoding\n",
        "        input_idx = tgt_vocab['<sos>']\n",
        "        output_chars = []\n",
        "\n",
        "        for _ in range(100):\n",
        "            input_tensor = torch.tensor([input_idx]).unsqueeze(0).to(device)\n",
        "\n",
        "            output, hidden, cell = model.decoder(input_tensor, hidden, cell)\n",
        "\n",
        "            predicted_idx = output.argmax(1).item()\n",
        "\n",
        "            if predicted_idx == tgt_vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "            # get character\n",
        "            for char, idx in tgt_vocab.items():\n",
        "                if idx == predicted_idx:\n",
        "                    if char not in ['<pad>', '<sos>', '<eos>', '<unk>']:\n",
        "                        output_chars.append(char)\n",
        "                    break\n",
        "\n",
        "            input_idx = predicted_idx\n",
        "\n",
        "    return ''.join(output_chars)\n",
        "\n",
        "# calculate bleu score\n",
        "def calculate_bleu(model, test_urdu, test_roman, src_vocab, tgt_vocab, device):\n",
        "    model.eval()\n",
        "\n",
        "    bleu_scores = []\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        # translate\n",
        "        prediction = translate_sentence(model, src, src_vocab, tgt_vocab, device)\n",
        "\n",
        "        # get reference\n",
        "        reference = [list(tgt)]\n",
        "        hypothesis = list(prediction)\n",
        "\n",
        "        # calculate bleu\n",
        "        score = sentence_bleu(reference, hypothesis)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    return avg_bleu * 100\n",
        "\n",
        "# calculate cer\n",
        "def calculate_cer(model, test_urdu, test_roman, src_vocab, tgt_vocab, device):\n",
        "    model.eval()\n",
        "\n",
        "    total_distance = 0\n",
        "    total_length = 0\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        # translate\n",
        "        prediction = translate_sentence(model, src, src_vocab, tgt_vocab, device)\n",
        "\n",
        "        # calculate distance\n",
        "        distance = Levenshtein.distance(prediction, tgt)\n",
        "        total_distance += distance\n",
        "        total_length += len(tgt)\n",
        "\n",
        "    cer = (total_distance / total_length) * 100\n",
        "    return cer\n",
        "\n",
        "# create model with EXACT same settings as Exp1_Baseline\n",
        "print(\"creating model with baseline settings...\")\n",
        "embedding_dim = 256\n",
        "hidden_size = 256\n",
        "dropout = 0.3\n",
        "\n",
        "# use your actual class parameter names\n",
        "encoder = Encoder(len(urdu_vocab), emb_dim=embedding_dim, hidden_dim=hidden_size, n_layers=2, dropout=dropout)\n",
        "decoder = Decoder(len(roman_vocab), emb_dim=embedding_dim, hidden_dim=hidden_size, n_layers=4, dropout=dropout)\n",
        "model = Seq2SeqNMT(encoder, decoder, device)\n",
        "\n",
        "# load saved weights\n",
        "print(\"loading best model...\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"model loaded successfully!\")\n",
        "\n",
        "# calculate metrics\n",
        "print(\"\\ncalculating bleu score...\")\n",
        "bleu = calculate_bleu(model, test_urdu, test_roman, urdu_vocab, roman_vocab, device)\n",
        "print(f\"BLEU Score: {bleu:.2f}\")\n",
        "\n",
        "print(\"\\ncalculating cer...\")\n",
        "cer = calculate_cer(model, test_urdu, test_roman, urdu_vocab, roman_vocab, device)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show some translations\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Sample Translations:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i in range(5):\n",
        "    src = test_urdu[i]\n",
        "    actual = test_roman[i]\n",
        "    predicted = translate_sentence(model, src, urdu_vocab, roman_vocab, device)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"Urdu:      {src}\")\n",
        "    print(f\"Actual:    {actual}\")\n",
        "    print(f\"Predicted: {predicted}\")\n",
        "    print(\"-\"*70)"
      ],
      "metadata": {
        "id": "3BDqUwYXO31j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install required libraries\n",
        "!pip install python-Levenshtein -q\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import Levenshtein\n",
        "import torch\n",
        "\n",
        "# translate one sentence\n",
        "def translate_sentence(model, src_text, src_vocab, tgt_vocab, device):\n",
        "    model.eval()\n",
        "\n",
        "    # convert to indices\n",
        "    src_indices = [src_vocab['<sos>']]\n",
        "    for char in src_text:\n",
        "        if char in src_vocab:\n",
        "            src_indices.append(src_vocab[char])\n",
        "        else:\n",
        "            src_indices.append(src_vocab['<unk>'])\n",
        "    src_indices.append(src_vocab['<eos>'])\n",
        "\n",
        "    # to tensor\n",
        "    src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # encode\n",
        "        enc_output, hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "        # fix hidden for decoder\n",
        "        hidden, cell = model.init_decoder_state(hidden, cell, 1)\n",
        "\n",
        "        # start decoding\n",
        "        input_idx = tgt_vocab['<sos>']\n",
        "        output_chars = []\n",
        "\n",
        "        for _ in range(100):\n",
        "            input_tensor = torch.tensor([input_idx]).unsqueeze(0).to(device)\n",
        "\n",
        "            output, hidden, cell = model.decoder(input_tensor, hidden, cell)\n",
        "\n",
        "            predicted_idx = output.argmax(1).item()\n",
        "\n",
        "            if predicted_idx == tgt_vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "            # get character\n",
        "            for char, idx in tgt_vocab.items():\n",
        "                if idx == predicted_idx:\n",
        "                    if char not in ['<pad>', '<sos>', '<eos>', '<unk>']:\n",
        "                        output_chars.append(char)\n",
        "                    break\n",
        "\n",
        "            input_idx = predicted_idx\n",
        "\n",
        "    return ''.join(output_chars)\n",
        "\n",
        "# calculate bleu score\n",
        "def calculate_bleu(model, test_urdu, test_roman, src_vocab, tgt_vocab, device):\n",
        "    model.eval()\n",
        "\n",
        "    bleu_scores = []\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        # translate\n",
        "        prediction = translate_sentence(model, src, src_vocab, tgt_vocab, device)\n",
        "\n",
        "        # get reference\n",
        "        reference = [list(tgt)]\n",
        "        hypothesis = list(prediction)\n",
        "\n",
        "        # calculate bleu\n",
        "        score = sentence_bleu(reference, hypothesis)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    return avg_bleu * 100\n",
        "\n",
        "# calculate cer\n",
        "def calculate_cer(model, test_urdu, test_roman, src_vocab, tgt_vocab, device):\n",
        "    model.eval()\n",
        "\n",
        "    total_distance = 0\n",
        "    total_length = 0\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        # translate\n",
        "        prediction = translate_sentence(model, src, src_vocab, tgt_vocab, device)\n",
        "\n",
        "        # calculate distance\n",
        "        distance = Levenshtein.distance(prediction, tgt)\n",
        "        total_distance += distance\n",
        "        total_length += len(tgt)\n",
        "\n",
        "    cer = (total_distance / total_length) * 100\n",
        "    return cer\n",
        "\n",
        "# create model with EXACT same settings as Exp1_Baseline\n",
        "print(\"creating model with baseline settings...\")\n",
        "embedding_dim = 256\n",
        "hidden_size = 256\n",
        "dropout = 0.3\n",
        "\n",
        "# use your actual class parameter names\n",
        "encoder = Encoder(len(urdu_vocab), emb_dim=embedding_dim, hidden_dim=hidden_size, n_layers=2, dropout=dropout)\n",
        "decoder = Decoder(len(roman_vocab), emb_dim=embedding_dim, hidden_dim=hidden_size, n_layers=4, dropout=dropout)\n",
        "model = Seq2SeqNMT(encoder, decoder, device)\n",
        "\n",
        "# load saved weights\n",
        "print(\"loading best model...\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"model loaded successfully!\")\n",
        "\n",
        "# calculate metrics\n",
        "print(\"\\ncalculating bleu score...\")\n",
        "bleu = calculate_bleu(model, test_urdu, test_roman, urdu_vocab, roman_vocab, device)\n",
        "print(f\"BLEU Score: {bleu:.2f}\")\n",
        "\n",
        "print(\"\\ncalculating cer...\")\n",
        "cer = calculate_cer(model, test_urdu, test_roman, urdu_vocab, roman_vocab, device)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show some translations\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Sample Translations:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i in range(5):\n",
        "    src = test_urdu[i]\n",
        "    actual = test_roman[i]\n",
        "    predicted = translate_sentence(model, src, urdu_vocab, roman_vocab, device)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"Urdu:      {src}\")\n",
        "    print(f\"Actual:    {actual}\")\n",
        "    print(f\"Predicted: {predicted}\")\n",
        "    print(\"-\"*70)"
      ],
      "metadata": {
        "id": "XOhAPaQCPFxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install library\n",
        "!pip install python-Levenshtein -q\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import Levenshtein\n",
        "import torch\n",
        "\n",
        "# translate one sentence\n",
        "def translate(model, src_text, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    # convert text to indices\n",
        "    indices = [src_vocab['<sos>']]\n",
        "    for c in src_text:\n",
        "        if c in src_vocab:\n",
        "            indices.append(src_vocab[c])\n",
        "        else:\n",
        "            indices.append(src_vocab['<unk>'])\n",
        "    indices.append(src_vocab['<eos>'])\n",
        "\n",
        "    # make tensor\n",
        "    src = torch.tensor(indices).unsqueeze(0).to(device)\n",
        "    length = torch.tensor([len(indices)]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # encode\n",
        "        enc_out, h, c = model.encoder(src, length)\n",
        "\n",
        "        # prepare decoder\n",
        "        h, c = model._init_decoder_states(h, c, 1)\n",
        "\n",
        "        # start decoding\n",
        "        input_id = torch.tensor([[tgt_vocab['<sos>']]]).to(device)\n",
        "        result = []\n",
        "\n",
        "        for _ in range(100):\n",
        "            out, h, c = model.decoder(input_id, h, c)\n",
        "            pred = out.argmax(1).item()\n",
        "\n",
        "            if pred == tgt_vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "            # find character\n",
        "            for char, idx in tgt_vocab.items():\n",
        "                if idx == pred and char not in ['<pad>', '<sos>', '<eos>', '<unk>']:\n",
        "                    result.append(char)\n",
        "                    break\n",
        "\n",
        "            input_id = torch.tensor([[pred]]).to(device)\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "# calculate bleu\n",
        "def calc_bleu(model, test_urdu, test_roman, src_vocab, tgt_vocab):\n",
        "    scores = []\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        pred = translate(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        ref = [list(tgt)]\n",
        "        hyp = list(pred)\n",
        "\n",
        "        score = sentence_bleu(ref, hyp)\n",
        "        scores.append(score)\n",
        "\n",
        "    avg = sum(scores) / len(scores)\n",
        "    return avg * 100\n",
        "\n",
        "# calculate cer\n",
        "def calc_cer(model, test_urdu, test_roman, src_vocab, tgt_vocab):\n",
        "    total_dist = 0\n",
        "    total_len = 0\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        pred = translate(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        dist = Levenshtein.distance(pred, tgt)\n",
        "        total_dist += dist\n",
        "        total_len += len(tgt)\n",
        "\n",
        "    cer = (total_dist / total_len) * 100\n",
        "    return cer\n",
        "\n",
        "# load model\n",
        "print(\"loading model...\")\n",
        "\n",
        "encoder = Encoder(\n",
        "    vocab_size=len(urdu_vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "decoder = Decoder(\n",
        "    vocab_size=len(roman_vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=4,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"model loaded!\")\n",
        "\n",
        "# calculate scores\n",
        "print(\"\\ncalculating bleu...\")\n",
        "bleu = calc_bleu(model, test_urdu, test_roman, urdu_vocab, roman_vocab)\n",
        "print(f\"BLEU: {bleu:.2f}\")\n",
        "\n",
        "print(\"\\ncalculating cer...\")\n",
        "cer = calc_cer(model, test_urdu, test_roman, urdu_vocab, roman_vocab)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show examples\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Examples:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(5):\n",
        "    src = test_urdu[i]\n",
        "    actual = test_roman[i]\n",
        "    pred = translate(model, src, urdu_vocab, roman_vocab)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"urdu:   {src}\")\n",
        "    print(f\"actual: {actual}\")\n",
        "    print(f\"pred:   {pred}\")\n",
        "    print(\"-\"*60)"
      ],
      "metadata": {
        "id": "7vX-Ilt8PV0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "flnGn3vTQyje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install library\n",
        "!pip install python-Levenshtein -q\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import Levenshtein\n",
        "import torch\n",
        "\n",
        "# translate one sentence\n",
        "def translate(model, src_text, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    # convert text to indices\n",
        "    indices = [src_vocab['<sos>']]\n",
        "    for c in src_text:\n",
        "        if c in src_vocab:\n",
        "            indices.append(src_vocab[c])\n",
        "        else:\n",
        "            indices.append(src_vocab['<unk>'])\n",
        "    indices.append(src_vocab['<eos>'])\n",
        "\n",
        "    # make tensor\n",
        "    src = torch.tensor(indices).unsqueeze(0).to(device)\n",
        "    length = torch.tensor([len(indices)]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # encode\n",
        "        enc_out, h, c = model.encoder(src, length)\n",
        "\n",
        "        # prepare decoder\n",
        "        h, c = model._init_decoder_states(h, c, 1)\n",
        "\n",
        "        # start decoding\n",
        "        input_id = torch.tensor([[tgt_vocab['<sos>']]]).to(device)\n",
        "        result = []\n",
        "\n",
        "        for _ in range(100):\n",
        "            out, h, c = model.decoder(input_id, h, c)\n",
        "            pred = out.argmax(1).item()\n",
        "\n",
        "            if pred == tgt_vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "            # find character\n",
        "            for char, idx in tgt_vocab.items():\n",
        "                if idx == pred and char not in ['<pad>', '<sos>', '<eos>', '<unk>']:\n",
        "                    result.append(char)\n",
        "                    break\n",
        "\n",
        "            input_id = torch.tensor([[pred]]).to(device)\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "# calculate bleu\n",
        "def calc_bleu(model, test_urdu, test_roman, src_vocab, tgt_vocab):\n",
        "    scores = []\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        pred = translate(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        ref = [list(tgt)]\n",
        "        hyp = list(pred)\n",
        "\n",
        "        score = sentence_bleu(ref, hyp)\n",
        "        scores.append(score)\n",
        "\n",
        "    avg = sum(scores) / len(scores)\n",
        "    return avg * 100\n",
        "\n",
        "# calculate cer\n",
        "def calc_cer(model, test_urdu, test_roman, src_vocab, tgt_vocab):\n",
        "    total_dist = 0\n",
        "    total_len = 0\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        pred = translate(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        dist = Levenshtein.distance(pred, tgt)\n",
        "        total_dist += dist\n",
        "        total_len += len(tgt)\n",
        "\n",
        "    cer = (total_dist / total_len) * 100\n",
        "    return cer\n",
        "\n",
        "# load model\n",
        "print(\"loading model...\")\n",
        "\n",
        "encoder = Encoder(\n",
        "    vocab_size=len(urdu_vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "decoder = Decoder(\n",
        "    vocab_size=len(roman_vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=4,\n",
        "    dropout=0.3,\n",
        "    encoder_hidden_size=256\n",
        ")\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"model loaded!\")\n",
        "\n",
        "# calculate scores\n",
        "print(\"\\ncalculating bleu...\")\n",
        "bleu = calc_bleu(model, test_urdu, test_roman, urdu_vocab, roman_vocab)\n",
        "print(f\"BLEU: {bleu:.2f}\")\n",
        "\n",
        "print(\"\\ncalculating cer...\")\n",
        "cer = calc_cer(model, test_urdu, test_roman, urdu_vocab, roman_vocab)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show examples\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Examples:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(5):\n",
        "    src = test_urdu[i]\n",
        "    actual = test_roman[i]\n",
        "    pred = translate(model, src, urdu_vocab, roman_vocab)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"urdu:   {src}\")\n",
        "    print(f\"actual: {actual}\")\n",
        "    print(f\"pred:   {pred}\")\n",
        "    print(\"-\"*60)"
      ],
      "metadata": {
        "id": "xIjJnQqKQXrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install library\n",
        "!pip install python-Levenshtein -q\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import Levenshtein\n",
        "import torch\n",
        "\n",
        "# translate one sentence\n",
        "def translate(model, src_text, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    # convert text to indices\n",
        "    # Use '<START>' instead of '<sos>'\n",
        "    indices = [src_vocab['<START>']]\n",
        "    for c in src_text:\n",
        "        if c in src_vocab:\n",
        "            indices.append(src_vocab[c])\n",
        "        else:\n",
        "            indices.append(src_vocab['<UNK>'])\n",
        "    # Use '<END>' instead of '<eos>'\n",
        "    indices.append(src_vocab['<END>'])\n",
        "\n",
        "    # make tensor\n",
        "    src = torch.tensor(indices).unsqueeze(0).to(device)\n",
        "    length = torch.tensor([len(indices)]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # encode\n",
        "        enc_out, h, c = model.encoder(src, length)\n",
        "\n",
        "        # prepare decoder\n",
        "        h, c = model._init_decoder_states(h, c, 1)\n",
        "\n",
        "        # start decoding\n",
        "        # Use the token ID for '<START>' (which is 2)\n",
        "        input_id = torch.tensor([[tgt_vocab['<START>']]]).to(device)\n",
        "        result = []\n",
        "\n",
        "        for _ in range(100):\n",
        "            out, h, c = model.decoder(input_id, h, c, enc_out, length) # Pass encoder outputs and lengths\n",
        "            pred = out.argmax(1).item()\n",
        "\n",
        "            # Stop if '<END>' token (which is 3) or '<PAD>' token (which is 0)\n",
        "            if pred == tgt_vocab['<END>'] or pred == tgt_vocab['<PAD>']:\n",
        "                break\n",
        "\n",
        "            # find character\n",
        "            for char, idx in tgt_vocab.items():\n",
        "                if idx == pred and char not in ['<PAD>', '<START>', '<END>', '<UNK>']:\n",
        "                    result.append(char)\n",
        "                    break\n",
        "\n",
        "            input_id = torch.tensor([[pred]]).to(device)\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "# calculate bleu\n",
        "def calc_bleu(model, test_urdu, test_roman, src_vocab, tgt_vocab):\n",
        "    scores = []\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        pred = translate(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        ref = [list(tgt)]\n",
        "        hyp = list(pred)\n",
        "\n",
        "        score = sentence_bleu(ref, hyp)\n",
        "        scores.append(score)\n",
        "\n",
        "    avg = sum(scores) / len(scores)\n",
        "    return avg * 100\n",
        "\n",
        "# calculate cer\n",
        "def calc_cer(model, test_urdu, test_roman, src_vocab, tgt_vocab):\n",
        "    total_dist = 0\n",
        "    total_len = 0\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        pred = translate(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        dist = Levenshtein.distance(pred, tgt)\n",
        "        total_dist += dist\n",
        "        total_len += len(tgt)\n",
        "\n",
        "    cer = (total_dist / total_len) * 100\n",
        "    return cer\n",
        "\n",
        "# load model\n",
        "print(\"loading model...\")\n",
        "\n",
        "# Ensure the model architecture matches the saved state dict\n",
        "# Get the hyperparameters from the best performing experiment (Exp1_Baseline)\n",
        "embedding_dim = experiments['Exp1_Baseline']['embedding_dim']\n",
        "hidden_size = experiments['Exp1_Baseline']['hidden_size']\n",
        "dropout = experiments['Exp1_Baseline']['dropout']\n",
        "\n",
        "\n",
        "encoder = Encoder(\n",
        "    vocab_size=len(urdu_vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=2,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "decoder = Decoder(\n",
        "    vocab_size=len(roman_vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=4,\n",
        "    dropout=dropout,\n",
        "    encoder_hidden_size=hidden_size\n",
        ")\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"model loaded!\")\n",
        "\n",
        "# calculate scores\n",
        "print(\"\\ncalculating bleu...\")\n",
        "# Use the actual test data from your data preparation cell\n",
        "bleu = calc_bleu(model, test_urdu, test_roman, urdu_vocab, roman_vocab)\n",
        "print(f\"BLEU: {bleu:.2f}\")\n",
        "\n",
        "print(\"\\ncalculating cer...\")\n",
        "# Use the actual test data from your data preparation cell\n",
        "cer = calc_cer(model, test_urdu, test_roman, urdu_vocab, roman_vocab)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show examples\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Examples:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(5):\n",
        "    src = test_urdu[i]\n",
        "    actual = test_roman[i]\n",
        "    pred = translate(model, src, urdu_vocab, roman_vocab)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"urdu:   {src}\")\n",
        "    print(f\"actual: {actual}\")\n",
        "    print(f\"pred:   {pred}\")\n",
        "    print(\"-\"*60)"
      ],
      "metadata": {
        "id": "BzKOdItoQzVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check your vocab keys\n",
        "print(\"Urdu vocab keys:\")\n",
        "print(list(urdu_vocab.keys())[:10])\n",
        "\n",
        "print(\"\\nRoman vocab keys:\")\n",
        "print(list(roman_vocab.keys())[:10])"
      ],
      "metadata": {
        "id": "lC95CwHgRIFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5uJm0az9RYGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install library\n",
        "!pip install python-Levenshtein -q\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import Levenshtein\n",
        "import torch\n",
        "\n",
        "# translate one sentence\n",
        "def translate(model, src_text, src_vocab, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    # convert text to indices\n",
        "    indices = [src_vocab['<START>']]\n",
        "    for c in src_text:\n",
        "        if c in src_vocab:\n",
        "            indices.append(src_vocab[c])\n",
        "        else:\n",
        "            indices.append(src_vocab['<UNK>'])\n",
        "    indices.append(src_vocab['<END>'])\n",
        "\n",
        "    # make tensor\n",
        "    src = torch.tensor(indices).unsqueeze(0).to(device)\n",
        "    length = torch.tensor([len(indices)]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # encode\n",
        "        enc_out, h, c = model.encoder(src, length)\n",
        "\n",
        "        # prepare decoder\n",
        "        h, c = model._init_decoder_states(h, c, 1)\n",
        "\n",
        "        # start decoding\n",
        "        input_id = torch.tensor([[tgt_vocab['<START>']]]).to(device)\n",
        "        result = []\n",
        "\n",
        "        for _ in range(100):\n",
        "            out, h, c = model.decoder(input_id, h, c)\n",
        "            pred = out.argmax(1).item()\n",
        "\n",
        "            if pred == tgt_vocab['<END>']:\n",
        "                break\n",
        "\n",
        "            # find character\n",
        "            for char, idx in tgt_vocab.items():\n",
        "                if idx == pred and char not in ['<PAD>', '<UNK>', '<START>', '<END>']:\n",
        "                    result.append(char)\n",
        "                    break\n",
        "\n",
        "            input_id = torch.tensor([[pred]]).to(device)\n",
        "\n",
        "    return ''.join(result)\n",
        "\n",
        "# calculate bleu\n",
        "def calc_bleu(model, test_urdu, test_roman, src_vocab, tgt_vocab):\n",
        "    scores = []\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        pred = translate(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        ref = [list(tgt)]\n",
        "        hyp = list(pred)\n",
        "\n",
        "        score = sentence_bleu(ref, hyp)\n",
        "        scores.append(score)\n",
        "\n",
        "    avg = sum(scores) / len(scores)\n",
        "    return avg * 100\n",
        "\n",
        "# calculate cer\n",
        "def calc_cer(model, test_urdu, test_roman, src_vocab, tgt_vocab):\n",
        "    total_dist = 0\n",
        "    total_len = 0\n",
        "\n",
        "    for i in range(min(100, len(test_urdu))):\n",
        "        src = test_urdu[i]\n",
        "        tgt = test_roman[i]\n",
        "\n",
        "        pred = translate(model, src, src_vocab, tgt_vocab)\n",
        "\n",
        "        dist = Levenshtein.distance(pred, tgt)\n",
        "        total_dist += dist\n",
        "        total_len += len(tgt)\n",
        "\n",
        "    cer = (total_dist / total_len) * 100\n",
        "    return cer\n",
        "\n",
        "# load model\n",
        "print(\"loading model...\")\n",
        "\n",
        "encoder = Encoder(\n",
        "    vocab_size=len(urdu_vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "decoder = Decoder(\n",
        "    vocab_size=len(roman_vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=4,\n",
        "    dropout=0.3,\n",
        "    encoder_hidden_size=256\n",
        ")\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"model loaded!\")\n",
        "\n",
        "# calculate scores\n",
        "print(\"\\ncalculating bleu...\")\n",
        "bleu = calc_bleu(model, test_urdu, test_roman, urdu_vocab, roman_vocab)\n",
        "print(f\"BLEU: {bleu:.2f}\")\n",
        "\n",
        "print(\"\\ncalculating cer...\")\n",
        "cer = calc_cer(model, test_urdu, test_roman, urdu_vocab, roman_vocab)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show examples\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Examples:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(5):\n",
        "    src = test_urdu[i]\n",
        "    actual = test_roman[i]\n",
        "    pred = translate(model, src, urdu_vocab, roman_vocab)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"urdu:   {src}\")\n",
        "    print(f\"actual: {actual}\")\n",
        "    print(f\"pred:   {pred}\")\n",
        "    print(\"-\"*60)"
      ],
      "metadata": {
        "id": "vbo3h1YBRrRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple test - just translate 5 examples\n",
        "print(\"Testing translations...\")\n",
        "\n",
        "for i in range(5):\n",
        "    src = test_urdu[i]\n",
        "    actual = test_roman[i]\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"Urdu:   {src}\")\n",
        "    print(f\"Actual: {actual}\")\n",
        "\n",
        "    # try to translate\n",
        "    try:\n",
        "        # your translate function here - whatever works in your notebook\n",
        "        pred = your_translate_function(model, src)\n",
        "        print(f\"Pred:   {pred}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error:  {e}\")"
      ],
      "metadata": {
        "id": "VBZPxd_PRsMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install library\n",
        "!pip install python-Levenshtein -q\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import Levenshtein\n",
        "import torch\n",
        "\n",
        "# decode indices to text\n",
        "def decode_indices(indices, vocab):\n",
        "    # reverse vocab\n",
        "    idx_to_char = {v: k for k, v in vocab.items()}\n",
        "\n",
        "    chars = []\n",
        "    for idx in indices:\n",
        "        char = idx_to_char.get(idx, '')\n",
        "        if char not in ['<PAD>', '<UNK>', '<START>', '<END>']:\n",
        "            chars.append(char)\n",
        "\n",
        "    return ''.join(chars)\n",
        "\n",
        "# translate\n",
        "def translate(model, src_indices, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    # already have indices, just make tensor\n",
        "    src = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
        "    length = torch.tensor([len(src_indices)]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # encode\n",
        "        enc_out, h, c = model.encoder(src, length)\n",
        "\n",
        "        # prepare decoder\n",
        "        h, c = model._init_decoder_states(h, c, 1)\n",
        "\n",
        "        # start decoding\n",
        "        input_id = torch.tensor([[tgt_vocab['<START>']]]).to(device)\n",
        "        result = []\n",
        "\n",
        "        for _ in range(100):\n",
        "            out, h, c = model.decoder(input_id, h, c, enc_out, length)\n",
        "            pred = out.argmax(1).item()\n",
        "\n",
        "            if pred == tgt_vocab['<END>']:\n",
        "                break\n",
        "\n",
        "            result.append(pred)\n",
        "            input_id = torch.tensor([[pred]]).to(device)\n",
        "\n",
        "    return result\n",
        "\n",
        "# calculate bleu\n",
        "def calc_bleu(model, test_urdu_indices, test_roman_indices, tgt_vocab):\n",
        "    scores = []\n",
        "\n",
        "    for i in range(min(100, len(test_urdu_indices))):\n",
        "        src = test_urdu_indices[i]\n",
        "        tgt = test_roman_indices[i]\n",
        "\n",
        "        # translate\n",
        "        pred = translate(model, src, tgt_vocab)\n",
        "\n",
        "        # decode to text\n",
        "        tgt_text = decode_indices(tgt, roman_vocab)\n",
        "        pred_text = decode_indices(pred, roman_vocab)\n",
        "\n",
        "        ref = [list(tgt_text)]\n",
        "        hyp = list(pred_text)\n",
        "\n",
        "        score = sentence_bleu(ref, hyp)\n",
        "        scores.append(score)\n",
        "\n",
        "    avg = sum(scores) / len(scores)\n",
        "    return avg * 100\n",
        "\n",
        "# calculate cer\n",
        "def calc_cer(model, test_urdu_indices, test_roman_indices, tgt_vocab):\n",
        "    total_dist = 0\n",
        "    total_len = 0\n",
        "\n",
        "    for i in range(min(100, len(test_urdu_indices))):\n",
        "        src = test_urdu_indices[i]\n",
        "        tgt = test_roman_indices[i]\n",
        "\n",
        "        # translate\n",
        "        pred = translate(model, src, tgt_vocab)\n",
        "\n",
        "        # decode to text\n",
        "        tgt_text = decode_indices(tgt, roman_vocab)\n",
        "        pred_text = decode_indices(pred, roman_vocab)\n",
        "\n",
        "        dist = Levenshtein.distance(pred_text, tgt_text)\n",
        "        total_dist += dist\n",
        "        total_len += len(tgt_text)\n",
        "\n",
        "    cer = (total_dist / total_len) * 100\n",
        "    return cer\n",
        "\n",
        "# load model\n",
        "print(\"loading model...\")\n",
        "\n",
        "encoder = Encoder(\n",
        "    vocab_size=len(urdu_vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "decoder = Decoder(\n",
        "    vocab_size=len(roman_vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=4,\n",
        "    dropout=0.3,\n",
        "    encoder_hidden_size=256\n",
        ")\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"model loaded!\")\n",
        "\n",
        "# calculate scores\n",
        "print(\"\\ncalculating bleu...\")\n",
        "bleu = calc_bleu(model, test_urdu, test_roman, roman_vocab)\n",
        "print(f\"BLEU: {bleu:.2f}\")\n",
        "\n",
        "print(\"\\ncalculating cer...\")\n",
        "cer = calc_cer(model, test_urdu, test_roman, roman_vocab)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show examples\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Examples:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(5):\n",
        "    src_indices = test_urdu[i]\n",
        "    tgt_indices = test_roman[i]\n",
        "\n",
        "    # translate\n",
        "    pred_indices = translate(model, src_indices, roman_vocab)\n",
        "\n",
        "    # decode all to text\n",
        "    src_text = decode_indices(src_indices, urdu_vocab)\n",
        "    tgt_text = decode_indices(tgt_indices, roman_vocab)\n",
        "    pred_text = decode_indices(pred_indices, roman_vocab)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"urdu:   {src_text}\")\n",
        "    print(f\"actual: {tgt_text}\")\n",
        "    print(f\"pred:   {pred_text}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "print(\"\\nDone!\")"
      ],
      "metadata": {
        "id": "UncXweOGSWtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install library\n",
        "!pip install python-Levenshtein -q\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import Levenshtein\n",
        "import torch\n",
        "\n",
        "# decode indices to text\n",
        "def decode_indices(indices, vocab):\n",
        "    # reverse vocab\n",
        "    idx_to_char = {v: k for k, v in vocab.items()}\n",
        "\n",
        "    chars = []\n",
        "    for idx in indices:\n",
        "        char = idx_to_char.get(idx, '')\n",
        "        if char not in ['<PAD>', '<UNK>', '<START>', '<END>']:\n",
        "            chars.append(char)\n",
        "\n",
        "    # join and clean up\n",
        "    text = ''.join(chars)\n",
        "    text = text.replace('</w>', ' ')  # replace word boundary with space\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# translate\n",
        "def translate(model, src_indices, tgt_vocab):\n",
        "    model.eval()\n",
        "\n",
        "    # already have indices, just make tensor\n",
        "    src = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
        "    length = torch.tensor([len(src_indices)]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # encode\n",
        "        enc_out, h, c = model.encoder(src, length)\n",
        "\n",
        "        # prepare decoder\n",
        "        h, c = model._init_decoder_states(h, c, 1)\n",
        "\n",
        "        # start decoding\n",
        "        input_id = torch.tensor([[tgt_vocab['<START>']]]).to(device)\n",
        "        result = []\n",
        "\n",
        "        for _ in range(100):\n",
        "            out, h, c = model.decoder(input_id, h, c, enc_out, length)\n",
        "            pred = out.argmax(1).item()\n",
        "\n",
        "            if pred == tgt_vocab['<END>']:\n",
        "                break\n",
        "\n",
        "            result.append(pred)\n",
        "            input_id = torch.tensor([[pred]]).to(device)\n",
        "\n",
        "    return result\n",
        "\n",
        "# calculate bleu\n",
        "def calc_bleu(model, test_urdu_indices, test_roman_indices, tgt_vocab):\n",
        "    scores = []\n",
        "\n",
        "    for i in range(min(100, len(test_urdu_indices))):\n",
        "        src = test_urdu_indices[i]\n",
        "        tgt = test_roman_indices[i]\n",
        "\n",
        "        # translate\n",
        "        pred = translate(model, src, tgt_vocab)\n",
        "\n",
        "        # decode to text\n",
        "        tgt_text = decode_indices(tgt, roman_vocab)\n",
        "        pred_text = decode_indices(pred, roman_vocab)\n",
        "\n",
        "        ref = [list(tgt_text)]\n",
        "        hyp = list(pred_text)\n",
        "\n",
        "        score = sentence_bleu(ref, hyp)\n",
        "        scores.append(score)\n",
        "\n",
        "    avg = sum(scores) / len(scores)\n",
        "    return avg * 100\n",
        "\n",
        "# calculate cer\n",
        "def calc_cer(model, test_urdu_indices, test_roman_indices, tgt_vocab):\n",
        "    total_dist = 0\n",
        "    total_len = 0\n",
        "\n",
        "    for i in range(min(100, len(test_urdu_indices))):\n",
        "        src = test_urdu_indices[i]\n",
        "        tgt = test_roman_indices[i]\n",
        "\n",
        "        # translate\n",
        "        pred = translate(model, src, tgt_vocab)\n",
        "\n",
        "        # decode to text\n",
        "        tgt_text = decode_indices(tgt, roman_vocab)\n",
        "        pred_text = decode_indices(pred, roman_vocab)\n",
        "\n",
        "        dist = Levenshtein.distance(pred_text, tgt_text)\n",
        "        total_dist += dist\n",
        "        total_len += len(tgt_text)\n",
        "\n",
        "    cer = (total_dist / total_len) * 100\n",
        "    return cer\n",
        "\n",
        "# load model\n",
        "print(\"loading model...\")\n",
        "\n",
        "encoder = Encoder(\n",
        "    vocab_size=len(urdu_vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ")\n",
        "\n",
        "decoder = Decoder(\n",
        "    vocab_size=len(roman_vocab),\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=4,\n",
        "    dropout=0.3,\n",
        "    encoder_hidden_size=256\n",
        ")\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"model loaded!\")\n",
        "\n",
        "# calculate scores\n",
        "print(\"\\ncalculating bleu...\")\n",
        "bleu = calc_bleu(model, test_urdu, test_roman, roman_vocab)\n",
        "print(f\"BLEU: {bleu:.2f}\")\n",
        "\n",
        "print(\"\\ncalculating cer...\")\n",
        "cer = calc_cer(model, test_urdu, test_roman, roman_vocab)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show examples\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Examples:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(5):\n",
        "    src_indices = test_urdu[i]\n",
        "    tgt_indices = test_roman[i]\n",
        "\n",
        "    # translate\n",
        "    pred_indices = translate(model, src_indices, roman_vocab)\n",
        "\n",
        "    # decode all to text\n",
        "    src_text = decode_indices(src_indices, urdu_vocab)\n",
        "    tgt_text = decode_indices(tgt_indices, roman_vocab)\n",
        "    pred_text = decode_indices(pred_indices, roman_vocab)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"urdu:   {src_text}\")\n",
        "    print(f\"actual: {tgt_text}\")\n",
        "    print(f\"pred:   {pred_text}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "print(\"\\nDone!\")"
      ],
      "metadata": {
        "id": "cKTKWY7LSxpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B3xJIUQ8TSSy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}