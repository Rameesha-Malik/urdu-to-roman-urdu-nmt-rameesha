{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVD11YwbiHvE"
      },
      "source": [
        "#Project1: Neural Machine Translation – Urdu to Roman Urdu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLNmZBcuimdC"
      },
      "source": [
        "**Objective**\n",
        "\n",
        "---\n",
        "Build\ta\tsequence-to-sequence\tmodel\tusing\ta\tbidirectional\tLSTM\t(BiLSTM)\tencoder-decoder\tto\ttranslate\tUrdu\ttext\tinto\tits\tRoman\tUrdu transliteration.\tThe\tgoal\tis\tto\texperiment\twith data\tfrom\turdu_ghazals_rekhta\tand\tpush\tthe\tlimits\tof\twhat\tBiLSTM-based\tNMT\tcan achieve for\tlow-resource,\tpoetic\ttext."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrmITYMejP_M"
      },
      "source": [
        "**Dataset**\n",
        "\n",
        "---\n",
        "You\twill\tuse\tthe\t*urdu_ghazals_rekhta*\tdataset:\n",
        "https://github.com/amir9ume/urdu_ghazals_rekhta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vLiMzebM1iK"
      },
      "source": [
        "**Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6Ivy3AhzEWD",
        "outputId": "a8ad23ef-ac34-48d9-d0db-afd757fb10de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6SLpI5pN0_6"
      },
      "source": [
        "**view dataset folders**\n",
        "\n",
        "mainfolders -> Subfolders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzrlOX-1OSvj"
      },
      "source": [
        "**poets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZN4wQJVbgXS",
        "outputId": "5c01f568-a6e2-4345-fc40-f05a6fa38bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 135\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[0m\u001b[01;34mahmad-faraz\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34makbar-allahabadi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mallama-iqbal\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34maltaf-hussain-hali\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mameer-khusrau\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mbahadur-shah-zafar\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mdagh-dehlvi\u001b[0m/\n",
            "-rw------- 1 root root 14340 Oct 11 10:15 .DS_Store\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mfahmida-riaz\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mfaiz-ahmad-faiz\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mfiraq-gorakhpuri\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mgulzar\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mhabib-jalib\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mjaan-nisar-akhtar\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mjaun-eliya\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mjaved-akhtar\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mjigar-moradabadi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mkaifi-azmi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mmeer-anees\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mmeer-taqi-meer\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mmirza-ghalib\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mmohsin-naqvi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mnaji-shakir\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mnaseer-turabi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mnazm-tabatabai\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mnida-fazli\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mnoon-meem-rashid\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mparveen-shakir\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34msahir-ludhianvi\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mwali-mohammad-wali\u001b[0m/\n",
            "drwx------ 2 root root  4096 Oct 11 10:16 \u001b[01;34mwaseem-barelvi\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls -la /content/drive/MyDrive/ANLP/project_1/dataset | head -n 40\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H44j-aVTOWa1"
      },
      "source": [
        "**langauge**\n",
        "according to our project requiremnt we only need **en** and **ur**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*   en->roman urdu\n",
        "*   ur->actual urdu\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ngRCAY_4KTk",
        "outputId": "2f661565-23f7-4078-b806-c45e0cecd1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 19\n",
            "-rw------- 1 root root 6148 Oct 11 10:15 .DS_Store\n",
            "drwx------ 2 root root 4096 Oct 11 10:16 \u001b[0m\u001b[01;34men\u001b[0m/\n",
            "drwx------ 2 root root 4096 Oct 11 10:16 \u001b[01;34mhi\u001b[0m/\n",
            "drwx------ 2 root root 4096 Oct 11 10:16 \u001b[01;34mur\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls -la /content/drive/MyDrive/ANLP/project_1/dataset/ahmad-faraz | head -n 40\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfQEnz0KO-8e"
      },
      "source": [
        "**poets -> en-ghazal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cTJjlke4YcN",
        "outputId": "eecba054-fa57-463e-b679-05042859f3b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 49\n",
            "-rw------- 1 root root  476 Oct 11 10:15 aankh-se-duur-na-ho-dil-se-utar-jaaegaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  709 Oct 11 10:15 aashiqii-men-miir-jaise-khvaab-mat-dekhaa-karo-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  473 Oct 11 10:15 ab-aur-kyaa-kisii-se-maraasim-badhaaen-ham-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  977 Oct 11 10:15 abhii-kuchh-aur-karishme-gazal-ke-dekhte-hain-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  650 Oct 11 10:15 ab-ke-ham-bichhde-to-shaayad-kabhii-khvaabon-men-milen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1408 Oct 11 10:15 ab-ke-tajdiid-e-vafaa-kaa-nahiin-imkaan-jaanaan-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  640 Oct 11 10:15 ab-kyaa-sochen-kyaa-haalaat-the-kis-kaaran-ye-zahr-piyaa-hai-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  574 Oct 11 10:15 ab-shauq-se-ki-jaan-se-guzar-jaanaa-chaahiye-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  780 Oct 11 10:15 agarche-zor-havaaon-ne-daal-rakkhaa-hai-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  690 Oct 11 10:15 aisaa-hai-ki-sab-khvaab-musalsal-nahiin-hote-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  598 Oct 11 10:15 aise-chup-hain-ki-ye-manzil-bhii-kadii-ho-jaise-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  677 Oct 11 10:15 ajab-junuun-e-masaafat-men-ghar-se-niklaa-thaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  784 Oct 11 10:15 avval-avval-kii-dostii-hai-abhii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  866 Oct 11 10:15 dost-ban-kar-bhii-nahiin-saath-nibhaane-vaalaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  545 Oct 11 10:15 dukh-fasaana-nahiin-ki-tujh-se-kahen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  843 Oct 11 10:15 guftuguu-achchhii-lagii-zauq-e-nazar-achchhaa-lagaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  680 Oct 11 10:15 har-koii-dil-kii-hathelii-pe-hai-sahraa-rakkhe-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  788 Oct 11 10:15 havaa-ke-zor-se-pindaar-e-baam-o-dar-bhii-gayaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  605 Oct 11 10:15 huii-hai-shaam-to-aankhon-men-bas-gayaa-phir-tuu-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  792 Oct 11 10:15 is-qadar-musalsal-thiin-shiddaten-judaaii-kii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  684 Oct 11 10:15 is-se-pahle-ki-be-vafaa-ho-jaaen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  417 Oct 11 10:15 jab-bhii-dil-khol-ke-roe-honge-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  993 Oct 11 10:15 jis-samt-bhii-dekhuun-nazar-aataa-hai-ki-tum-ho-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  555 Oct 11 10:15 jo-gair-the-vo-isii-baat-par-hamaare-hue-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1035 Oct 11 10:15 juz-tire-koii-bhii-din-raat-na-jaane-mere-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  598 Oct 11 10:15 karuun-na-yaad-magar-kis-tarah-bhulaauun-use-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  594 Oct 11 10:15 kathin-hai-raahguzar-thodii-duur-saath-chalo-ahmad-faraz-ghazals-1\n",
            "-rw------- 1 root root  598 Oct 11 10:15 khaamosh-ho-kyuun-daad-e-jafaa-kyuun-nahiin-dete-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  541 Oct 11 10:15 kyaa-aise-kam-sukhan-se-koii-guftuguu-kare-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  615 Oct 11 10:15 main-to-maqtal-men-bhii-qismat-kaa-sikandar-niklaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  426 Oct 11 10:15 phir-usii-rahguzaar-par-shaayad-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  620 Oct 11 10:15 qurbat-bhii-nahiin-dil-se-utar-bhii-nahiin-jaataa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  562 Oct 11 10:15 qurbaton-men-bhii-judaaii-ke-zamaane-maange-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  576 Oct 11 10:15 ranjish-hii-sahii-dil-hii-dukhaane-ke-liye-aa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  602 Oct 11 10:15 rog-aise-bhii-gam-e-yaar-se-lag-jaate-hain-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  953 Oct 11 10:15 saamne-us-ke-kabhii-us-kii-sataaish-nahiin-kii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  560 Oct 11 10:15 saaqiyaa-ek-nazar-jaam-se-pahle-pahle-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  490 Oct 11 10:15 shoala-thaa-jal-bujhaa-huun-havaaen-mujhe-na-do-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  467 Oct 11 10:15 silsile-tod-gayaa-vo-sabhii-jaate-jaate-ahmad-faraz-ghazals\n"
          ]
        }
      ],
      "source": [
        "ls -la /content/drive/MyDrive/ANLP/project_1/dataset/ahmad-faraz/en | head -n 40\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrd_laNxPLUi"
      },
      "source": [
        "**poets -> ur-ghazal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qviNoXD4fIq",
        "outputId": "dc766be6-20f7-4532-941b-e36b9d9d7089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 59\n",
            "-rw------- 1 root root  648 Oct 11 10:15 aankh-se-duur-na-ho-dil-se-utar-jaaegaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  935 Oct 11 10:15 aashiqii-men-miir-jaise-khvaab-mat-dekhaa-karo-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  641 Oct 11 10:15 ab-aur-kyaa-kisii-se-maraasim-badhaaen-ham-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1348 Oct 11 10:15 abhii-kuchh-aur-karishme-gazal-ke-dekhte-hain-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  837 Oct 11 10:15 ab-ke-ham-bichhde-to-shaayad-kabhii-khvaabon-men-milen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1832 Oct 11 10:15 ab-ke-tajdiid-e-vafaa-kaa-nahiin-imkaan-jaanaan-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  868 Oct 11 10:15 ab-kyaa-sochen-kyaa-haalaat-the-kis-kaaran-ye-zahr-piyaa-hai-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  741 Oct 11 10:15 ab-shauq-se-ki-jaan-se-guzar-jaanaa-chaahiye-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  987 Oct 11 10:15 agarche-zor-havaaon-ne-daal-rakkhaa-hai-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  897 Oct 11 10:15 aisaa-hai-ki-sab-khvaab-musalsal-nahiin-hote-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  805 Oct 11 10:15 aise-chup-hain-ki-ye-manzil-bhii-kadii-ho-jaise-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  894 Oct 11 10:15 ajab-junuun-e-masaafat-men-ghar-se-niklaa-thaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1009 Oct 11 10:15 avval-avval-kii-dostii-hai-abhii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1139 Oct 11 10:15 dost-ban-kar-bhii-nahiin-saath-nibhaane-vaalaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  714 Oct 11 10:15 dukh-fasaana-nahiin-ki-tujh-se-kahen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1076 Oct 11 10:15 guftuguu-achchhii-lagii-zauq-e-nazar-achchhaa-lagaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  911 Oct 11 10:15 har-koii-dil-kii-hathelii-pe-hai-sahraa-rakkhe-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1044 Oct 11 10:15 havaa-ke-zor-se-pindaar-e-baam-o-dar-bhii-gayaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  732 Oct 11 10:15 huii-hai-shaam-to-aankhon-men-bas-gayaa-phir-tuu-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1025 Oct 11 10:15 is-qadar-musalsal-thiin-shiddaten-judaaii-kii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  927 Oct 11 10:15 is-se-pahle-ki-be-vafaa-ho-jaaen-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  599 Oct 11 10:15 jab-bhii-dil-khol-ke-roe-honge-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1323 Oct 11 10:15 jis-samt-bhii-dekhuun-nazar-aataa-hai-ki-tum-ho-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  760 Oct 11 10:15 jo-gair-the-vo-isii-baat-par-hamaare-hue-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1388 Oct 11 10:15 juz-tire-koii-bhii-din-raat-na-jaane-mere-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  746 Oct 11 10:15 karuun-na-yaad-magar-kis-tarah-bhulaauun-use-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  757 Oct 11 10:15 kathin-hai-raahguzar-thodii-duur-saath-chalo-ahmad-faraz-ghazals-1\n",
            "-rw------- 1 root root  780 Oct 11 10:15 khaamosh-ho-kyuun-daad-e-jafaa-kyuun-nahiin-dete-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  708 Oct 11 10:15 kyaa-aise-kam-sukhan-se-koii-guftuguu-kare-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  777 Oct 11 10:15 main-to-maqtal-men-bhii-qismat-kaa-sikandar-niklaa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  542 Oct 11 10:15 phir-usii-rahguzaar-par-shaayad-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  797 Oct 11 10:15 qurbat-bhii-nahiin-dil-se-utar-bhii-nahiin-jaataa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  750 Oct 11 10:15 qurbaton-men-bhii-judaaii-ke-zamaane-maange-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  748 Oct 11 10:15 ranjish-hii-sahii-dil-hii-dukhaane-ke-liye-aa-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  800 Oct 11 10:15 rog-aise-bhii-gam-e-yaar-se-lag-jaate-hain-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root 1221 Oct 11 10:15 saamne-us-ke-kabhii-us-kii-sataaish-nahiin-kii-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  748 Oct 11 10:15 saaqiyaa-ek-nazar-jaam-se-pahle-pahle-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  644 Oct 11 10:15 shoala-thaa-jal-bujhaa-huun-havaaen-mujhe-na-do-ahmad-faraz-ghazals\n",
            "-rw------- 1 root root  628 Oct 11 10:15 silsile-tod-gayaa-vo-sabhii-jaate-jaate-ahmad-faraz-ghazals\n"
          ]
        }
      ],
      "source": [
        "ls -la /content/drive/MyDrive/ANLP/project_1/dataset/ahmad-faraz/ur | head -n 40\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90V7kyujcbFl"
      },
      "source": [
        "**1. Preprocessing**\n",
        " ---\n",
        "* Clean\tthe\tUrdu\ttext:normalize\tcharacters,\tremove\textraneous\tpunctuation\tas\tneeded.\n",
        "* Define\tor\tcollect\trules\tfor\tconverting\tUrdu\tinto\tRoman\tUrdu\t(if\tRoman\tUrdu\tis\tnot\tdirectly\n",
        "given\tin\tthe\tdataset).\n",
        "* Tokenization:\tchoose\tproper\ttokenization\tstrategy\tfor\tboth\tsource\t(Urdu)\tand\ttarget\n",
        "(Roman\tUrdu).Consider\tsubword\tmethods\t(e.g.\tByte-Pair\tEncoding,\tWordPiece)\tif\thelpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxfdx52pPQt6"
      },
      "source": [
        "**count dataset**\n",
        "* total poets in dataset\n",
        "* total pairs of urdu & roman urdu lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBlwbWC84pKk",
        "outputId": "0b20b96f-541e-40c3-ab60-0fb79605c27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total no of poets in dataset: 30\n",
            "total pairs: 21003\n",
            "\n",
            "1.\n",
            "urdu: دل سے تری نگاہ جگر تک اتر گئی\n",
            "roman: dil se tirī nigāh jigar tak utar ga.ī\n",
            "\n",
            "2.\n",
            "urdu: دونوں کو اک ادا میں رضامند کر گئی\n",
            "roman: donoñ ko ik adā meñ razā-mand kar ga.ī\n",
            "\n",
            "3.\n",
            "urdu: شق ہو گیا ہے سینہ خوشا لذت فراغ\n",
            "roman: shaq ho gayā hai siina ḳhushā lazzat-e-farāġh\n",
            "\n",
            "4.\n",
            "urdu: تکلیف پردہ داری زخم جگر گئی\n",
            "roman: taklīf-e-parda-dāri-e-zaḳhm-e-jigar ga.ī\n",
            "\n",
            "5.\n",
            "urdu: وہ بادۂ شبانہ کی سرمستیاں کہاں\n",
            "roman: vo bāda-e-shabāna kī sarmastiyāñ kahāñ\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "datapath = '/content/drive/MyDrive/ANLP/project_1/dataset'\n",
        "\n",
        "# get all poets\n",
        "poets = os.listdir(datapath)\n",
        "poets = [p for p in poets if os.path.isdir(os.path.join(datapath, p))]\n",
        "print(f\"total no of poets in dataset: {len(poets)}\")\n",
        "\n",
        "# load data from each poets\n",
        "actual_urdu = []\n",
        "roman_urdu = []\n",
        "\n",
        "for poet in poets:\n",
        "    urdu_data_path = os.path.join(datapath, poet, 'ur')\n",
        "    roman_data_path = os.path.join(datapath, poet, 'en')\n",
        "\n",
        "    if not os.path.exists(urdu_data_path) or not os.path.exists(roman_data_path):\n",
        "        continue\n",
        "\n",
        "    files = os.listdir(urdu_data_path)\n",
        "\n",
        "    for file in files:\n",
        "        f = open(os.path.join(urdu_data_path, file), 'r', encoding='utf-8')\n",
        "        urdudata_lines = f.read().split('\\n')\n",
        "        f.close()\n",
        "\n",
        "        f = open(os.path.join(roman_data_path, file), 'r', encoding='utf-8')\n",
        "        romandata_lines = f.read().split('\\n')\n",
        "        f.close()\n",
        "\n",
        "        for u, r in zip(urdudata_lines, romandata_lines):\n",
        "            if u.strip() and r.strip():\n",
        "                actual_urdu.append(u.strip())\n",
        "                roman_urdu.append(r.strip())\n",
        "\n",
        "print(f\"total pairs: {len(actual_urdu)}\")\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"urdu: {actual_urdu[i]}\")\n",
        "    print(f\"roman: {roman_urdu[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SVdTCtgZKdS"
      },
      "source": [
        "# data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3NhMGy1ZPgv"
      },
      "source": [
        "**urdu data cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UQmXVMj5OiM",
        "outputId": "472cc141-6685-4a1b-ad95-c48cdd90d3f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total urdu data(cleaned): 21003\n",
            "\n",
            "2. دونوں کو اک ادا میں رضامند کر گیی\n",
            "\n",
            "4. تکلیف پردہ داری زخم جگر گیی\n",
            "\n",
            "6. اٹہیے بس اب کہ لذت خواب سحر گیی\n",
            "\n",
            "8. بارے اب اے ہوا ہوس بال و پر گیی\n",
            "\n",
            "10. موج خرام یار بہی کیا گل کتر گیی\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# urdu data cleaniing\n",
        "clean_urdudata = []\n",
        "\n",
        "# set different types of urdu haraf(alphabet)\n",
        "for txt in actual_urdu:\n",
        "    #alif\n",
        "    txt = txt.replace('ﺍ', 'ا')\n",
        "    txt = txt.replace('ﺎ', 'ا')\n",
        "    txt = txt.replace('ﺁ', 'آ')\n",
        "    txt = txt.replace('ﺃ', 'أ')\n",
        "\n",
        "    #ye\n",
        "    txt = txt.replace('ﯼ', 'ی')\n",
        "    txt = txt.replace('ﯽ', 'ی')\n",
        "    txt = txt.replace('ﯾ', 'ی')\n",
        "    txt = txt.replace('ﯿ', 'ی')\n",
        "    txt = txt.replace('ﮮ', 'ی')\n",
        "    txt = txt.replace('ئ', 'ی')\n",
        "    txt = txt.replace('ي', 'ی')\n",
        "\n",
        "    #hey\n",
        "    txt = txt.replace('ﮨ', 'ہ')\n",
        "    txt = txt.replace('ﮩ', 'ہ')\n",
        "    txt = txt.replace('ﮪ', 'ہ')\n",
        "    txt = txt.replace('ﮫ', 'ہ')\n",
        "    txt = txt.replace('ھ', 'ہ')\n",
        "    txt = txt.replace('ه', 'ہ')\n",
        "\n",
        "    #kaaf\n",
        "    txt = txt.replace('ﮐ', 'ک')\n",
        "    txt = txt.replace('ﮑ', 'ک')\n",
        "    txt = txt.replace('ك', 'ک')\n",
        "\n",
        "    #gaaf\n",
        "    txt = txt.replace('ﮒ', 'گ')\n",
        "    txt = txt.replace('ﮓ', 'گ')\n",
        "\n",
        "    #wao\n",
        "    txt = txt.replace('ؤ', 'و')\n",
        "\n",
        "    # remove urdu punctuation\n",
        "    txt = txt.replace('۔', '')\n",
        "    txt = txt.replace('؍', '')\n",
        "    txt = txt.replace('،', '')\n",
        "\n",
        "    # remove zabar zer pesh\n",
        "    txt = re.sub(r'[\\u064B-\\u065F]', '', txt)\n",
        "\n",
        "    # remove extra spaces\n",
        "    txt = re.sub(r'\\s+', ' ', txt)\n",
        "    txt = txt.strip()\n",
        "\n",
        "    if txt:\n",
        "        clean_urdudata.append(txt)\n",
        "\n",
        "print(f\"total urdu data(cleaned): {len(clean_urdudata)}\")\n",
        "\n",
        "for i in range(1,10,2):\n",
        "    print(f\"\\n{i+1}. {clean_urdudata[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAXqW6EnZYP5"
      },
      "source": [
        "**roman urdu data cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zx9aWmJAoJy",
        "outputId": "933a3b53-1669-4daf-d6cc-8e3b609a1990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total roman data(cleaned): 21003\n",
            "\n",
            "2. donon ko ik ada men razamand kar gai\n",
            "\n",
            "4. taklifepardadariezahmejigar gai\n",
            "\n",
            "6. uthiye bas ab ki lazzatehvabesahar gai\n",
            "\n",
            "8. baare ab ai hava havasebalopar gai\n",
            "\n",
            "10. maujehirameyar bhi kya gul katar gai\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# roman data cleaning\n",
        "clean_roman_data = []\n",
        "\n",
        "for txt in roman_urdu:\n",
        "    # convert to lowercase\n",
        "    txt = txt.lower()\n",
        "\n",
        "    # remove special urdu letters\n",
        "    txt = txt.replace('ā', 'a')\n",
        "    txt = txt.replace('ī', 'i')\n",
        "    txt = txt.replace('ū', 'u')\n",
        "    txt = txt.replace('ñ', 'n')\n",
        "    txt = txt.replace('ṭ', 't')\n",
        "    txt = txt.replace('ḍ', 'd')\n",
        "    txt = txt.replace('ṛ', 'r')\n",
        "    txt = txt.replace('ṇ', 'n')\n",
        "\n",
        "    # keep only english letters and spaces\n",
        "    txt = re.sub(r'[^a-z\\s]', '', txt)\n",
        "\n",
        "    # remove extra spaces\n",
        "    txt = re.sub(r'\\s+', ' ', txt)\n",
        "    txt = txt.strip()\n",
        "\n",
        "    if txt:\n",
        "        clean_roman_data.append(txt)\n",
        "\n",
        "print(f\"total roman data(cleaned): {len(clean_roman_data)}\")\n",
        "\n",
        "for i in range(1,10,2):\n",
        "    print(f\"\\n{i+1}. {clean_roman_data[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgVD3tg5Zf_1"
      },
      "source": [
        "**combine view of data cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw0ZVGTPBZsG",
        "outputId": "e30f3e09-61c7-4bad-d517-0aee8ed67254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total urdu data(cleaned): 21003\n",
            "total roman data(cleaned): 21003\n",
            "\n",
            "random clean data samples pairs:\n",
            "\n",
            "2.\n",
            "actual urdu : دونوں کو اک ادا میں رضامند کر گیی\n",
            "roman urdu: donon ko ik ada men razamand kar gai\n",
            "\n",
            "6.\n",
            "actual urdu : اٹہیے بس اب کہ لذت خواب سحر گیی\n",
            "roman urdu: uthiye bas ab ki lazzatehvabesahar gai\n",
            "\n",
            "10.\n",
            "actual urdu : موج خرام یار بہی کیا گل کتر گیی\n",
            "roman urdu: maujehirameyar bhi kya gul katar gai\n",
            "\n",
            "14.\n",
            "actual urdu : مستی سے ہر نگہ ترے رخ پر بکہر گیی\n",
            "roman urdu: masti se har nigah tire ruh par bikhar gai\n",
            "\n",
            "18.\n",
            "actual urdu : وہ ولولے کہاں وہ جوانی کدہر گیی\n",
            "roman urdu: vo valvale kahan vo javani kidhar gai\n"
          ]
        }
      ],
      "source": [
        "print(f\"total urdu data(cleaned): {len(clean_urdudata)}\")\n",
        "print(f\"total roman data(cleaned): {len(clean_roman_data)}\")\n",
        "\n",
        "print(\"\\nrandom clean data samples pairs:\")\n",
        "for i in range(1,20,4):\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"actual urdu : {clean_urdudata[i]}\")\n",
        "    print(f\"roman urdu: {clean_roman_data[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xInTXx_WKI7Z"
      },
      "source": [
        "**merge all data in one file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CWFZz-MKG6g",
        "outputId": "4d9d4756-c68b-4346-db81-88310dcca268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start data merging\n",
            "dataset path: /content/drive/MyDrive/ANLP/project_1/dataset\n",
            "\n",
            "no of total POETS folders: 30 \n",
            "\n",
            "ahmad-faraz: 748 pairs added\n",
            "akbar-allahabadi: 776 pairs added\n",
            "allama-iqbal: 738 pairs added\n",
            "altaf-hussain-hali: 582 pairs added\n",
            "ameer-khusrau: 10 pairs added\n",
            "bahadur-shah-zafar: 878 pairs added\n",
            "dagh-dehlvi: 1414 pairs added\n",
            "fahmida-riaz: 84 pairs added\n",
            "faiz-ahmad-faiz: 564 pairs added\n",
            "firaq-gorakhpuri: 1502 pairs added\n",
            "gulzar: 432 pairs added\n",
            "habib-jalib: 608 pairs added\n",
            "jaan-nisar-akhtar: 550 pairs added\n",
            "jaun-eliya: 946 pairs added\n",
            "javed-akhtar: 564 pairs added\n",
            "jigar-moradabadi: 1088 pairs added\n",
            "kaifi-azmi: 174 pairs added\n",
            "meer-anees: 168 pairs added\n",
            "meer-taqi-meer: 842 pairs added\n",
            "mirza-ghalib: 3717 pairs added\n",
            "mohsin-naqvi: 632 pairs added\n",
            "naji-shakir: 116 pairs added\n",
            "naseer-turabi: 234 pairs added\n",
            "nazm-tabatabai: 650 pairs added\n",
            "nida-fazli: 532 pairs added\n",
            "noon-meem-rashid: 62 pairs added\n",
            "parveen-shakir: 754 pairs added\n",
            "sahir-ludhianvi: 498 pairs added\n",
            "wali-mohammad-wali: 552 pairs added\n",
            "waseem-barelvi: 588 pairs added\n",
            "\n",
            "o of total urdu & roman pairs: 21003\n",
            "no of total Urdu lines: 21003\n",
            "no of total Roman lines: 21003\n",
            "\n",
            "Save merge files\n",
            "Saved 21003 Urdu lines-> /content/drive/MyDrive/ANLP/project_1/urdu.txt\n",
            "Saved 21003 Roman lines-> /content/drive/MyDrive/ANLP/project_1/roman.txt\n",
            "sample data pair\n",
            "Pair no 1:\n",
            "  Urdu:  آنکھ سے دور نہ ہو دل سے اتر جائے گا\n",
            "  Roman: aankh se duur na ho dil se utar jaega\n",
            "\n",
            "Pair no 2:\n",
            "  Urdu:  وقت کا کیا ہے گزرتا ہے گزر جائے گا\n",
            "  Roman: vaqt ka kya hai guzarta hai guzar jaega\n",
            "\n",
            "Pair no 3:\n",
            "  Urdu:  اتنا مانوس نہ ہو خلوت غم سے اپنی\n",
            "  Roman: itna manus na ho khalvat-e-gham se apni\n",
            "\n",
            "Pair no 4:\n",
            "  Urdu:  تو کبھی خود کو بھی دیکھے گا تو ڈر جائے گا\n",
            "  Roman: tu kabhi khud ko bhi dekhega to dar jaega\n",
            "\n",
            "Pair no 5:\n",
            "  Urdu:  ڈوبتے ڈوبتے کشتی کو اچھالا دے دوں\n",
            "  Roman: dubte dubte kashti ko uchhala de duun\n",
            "\n",
            "Dataset merging done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "class DatasetAggregator:\n",
        "    #get all data from each poet and create combine file\n",
        "\n",
        "    def __init__(self, dataset_path):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.urdu_lines = []\n",
        "        self.roman_lines = []\n",
        "\n",
        "    def is_letter(self, ch):\n",
        "        #checck if char is a letter\n",
        "        return ch.isalpha()\n",
        "\n",
        "    def remove_inner_dots(self, text):\n",
        "        #remove dots that show between letters\n",
        "        if not text:\n",
        "            return text\n",
        "\n",
        "        result = []\n",
        "        for i, ch in enumerate(text):\n",
        "            if ch == '.':\n",
        "                # Check if previous and next are letters\n",
        "                prev_is_letter = (i > 0 and self.is_letter(text[i-1]))\n",
        "                next_is_letter = (i < len(text) - 1 and self.is_letter(text[i+1]))\n",
        "\n",
        "                # only remove if surrounded by letters\n",
        "                if prev_is_letter and next_is_letter:\n",
        "                    continue\n",
        "                else:\n",
        "                    result.append(ch)\n",
        "            else:\n",
        "                result.append(ch)\n",
        "\n",
        "        return ''.join(result)\n",
        "\n",
        "    def normalize_roman(self, text):\n",
        "        #Normalize Roman txt\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Normalize unicode\n",
        "        text = unicodedata.normalize(\"NFKD\", text)\n",
        "\n",
        "        # Remove non-ASCII char\n",
        "        text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "\n",
        "        # Remove inner dots\n",
        "        text = self.remove_inner_dots(text)\n",
        "\n",
        "        # Remove quotes\n",
        "        text = re.sub(r\"['''\\\"`]\", \"\", text)\n",
        "\n",
        "        # Clean whitespace\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def normalize_urdu(self, text):\n",
        "        #Normalize Urdu txt\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Normalize unicode\n",
        "        text = unicodedata.normalize(\"NFC\", text)\n",
        "\n",
        "        # Remove diacritical marks (Arabic Presentation Forms)\n",
        "        cleaned = []\n",
        "        for ch in text:\n",
        "            cp = ord(ch)\n",
        "            # Skip Arabic Presentation Forms\n",
        "            if (0xFB50 <= cp <= 0xFDFF) or (0xFE70 <= cp <= 0xFEFF):\n",
        "                continue\n",
        "            cleaned.append(ch)\n",
        "\n",
        "        text = ''.join(cleaned)\n",
        "\n",
        "        # Remove special char\n",
        "        text = text.replace(\"ؔ\", \"\")\n",
        "\n",
        "        # Remove inner dots\n",
        "        text = self.remove_inner_dots(text)\n",
        "\n",
        "        # Clean whitespace\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def process_poet_folder(self, poet_name):\n",
        "        #Process single poets folder\n",
        "        poet_path = os.path.join(self.dataset_path, poet_name)\n",
        "\n",
        "        en_dir = os.path.join(poet_path, \"en\")\n",
        "        ur_dir = os.path.join(poet_path, \"ur\")\n",
        "\n",
        "        # Check if directories exist\n",
        "        if not os.path.isdir(en_dir) or not os.path.isdir(ur_dir):\n",
        "            return 0\n",
        "\n",
        "        # Get file lists\n",
        "        en_files = sorted([f for f in os.listdir(en_dir) if os.path.isfile(os.path.join(en_dir, f))])\n",
        "        ur_files = sorted([f for f in os.listdir(ur_dir) if os.path.isfile(os.path.join(ur_dir, f))])\n",
        "\n",
        "        pairs_added = 0\n",
        "\n",
        "        # Process each file pair\n",
        "        for en_file, ur_file in zip(en_files, ur_files):\n",
        "            en_path = os.path.join(en_dir, en_file)\n",
        "            ur_path = os.path.join(ur_dir, ur_file)\n",
        "\n",
        "            try:\n",
        "                # Read Roman file\n",
        "                with open(en_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "                    en_lines = f.readlines()\n",
        "\n",
        "                # Read Urdu file\n",
        "                with open(ur_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "                    ur_lines = f.readlines()\n",
        "\n",
        "                # Process each line pair\n",
        "                for en_line, ur_line in zip(en_lines, ur_lines):\n",
        "                    en_line = en_line.strip()\n",
        "                    ur_line = ur_line.strip()\n",
        "\n",
        "                    if en_line and ur_line:\n",
        "                        # Normalize both\n",
        "                        en_norm = self.normalize_roman(en_line)\n",
        "                        ur_norm = self.normalize_urdu(ur_line)\n",
        "\n",
        "                        # Only add if both are non-empty after normalization\n",
        "                        if en_norm and ur_norm:\n",
        "                            self.roman_lines.append(en_norm)\n",
        "                            self.urdu_lines.append(ur_norm)\n",
        "                            pairs_added += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {poet_name}/{en_file}: {e}\")\n",
        "\n",
        "        return pairs_added\n",
        "\n",
        "    def aggregate_all(self):\n",
        "        #Process all poet folders\n",
        "        print(\"start data merging\")\n",
        "        print(f\"dataset path: {self.dataset_path}\\n\")\n",
        "\n",
        "        # Get all poet directories\n",
        "        poets = sorted([d for d in os.listdir(self.dataset_path)\n",
        "                       if os.path.isdir(os.path.join(self.dataset_path, d))\n",
        "                       and not d.startswith('.')])\n",
        "\n",
        "        print(f\"no of total POETS folders: {len(poets)} \\n\")\n",
        "\n",
        "        total_pairs = 0\n",
        "\n",
        "        for poet_name in poets:\n",
        "            pairs = self.process_poet_folder(poet_name)\n",
        "            if pairs > 0:\n",
        "                print(f\"{poet_name}: {pairs} pairs added\")\n",
        "                total_pairs += pairs\n",
        "\n",
        "        print(f\"\\no of total urdu & roman pairs: {total_pairs}\")\n",
        "        print(f\"no of total Urdu lines: {len(self.urdu_lines)}\")\n",
        "        print(f\"no of total Roman lines: {len(self.roman_lines)}\")\n",
        "\n",
        "        return total_pairs\n",
        "\n",
        "    def save_files(self, urdu_output, roman_output):\n",
        "        #Save merged data to files\n",
        "        print(f\"\\nSave merge files\")\n",
        "\n",
        "        # Save Urdu\n",
        "        with open(urdu_output, 'w', encoding='utf-8') as f:\n",
        "            for line in self.urdu_lines:\n",
        "                f.write(line + '\\n')\n",
        "        print(f\"Saved {len(self.urdu_lines)} Urdu lines-> {urdu_output}\")\n",
        "\n",
        "        # Save Roman\n",
        "        with open(roman_output, 'w', encoding='utf-8') as f:\n",
        "            for line in self.roman_lines:\n",
        "                f.write(line + '\\n')\n",
        "        print(f\"Saved {len(self.roman_lines)} Roman lines-> {roman_output}\")\n",
        "\n",
        "    def show_samples(self, num_samples=5):\n",
        "        #show sample pairs\n",
        "\n",
        "        print(\"sample data pair\")\n",
        "\n",
        "        for i in range(min(num_samples, len(self.urdu_lines))):\n",
        "            print(f\"Pair no {i+1}:\")\n",
        "            print(f\"  Urdu:  {self.urdu_lines[i]}\")\n",
        "            print(f\"  Roman: {self.roman_lines[i]}\")\n",
        "            print()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # UPDATE THIS PATH to your actual dataset location\n",
        "    DATASET_PATH = \"/content/drive/MyDrive/ANLP/project_1/dataset\"\n",
        "\n",
        "    # Create merger\n",
        "    aggregator = DatasetAggregator(DATASET_PATH)\n",
        "\n",
        "    # Process all poets\n",
        "    total = aggregator.aggregate_all()\n",
        "\n",
        "    # Save files\n",
        "    aggregator.save_files(\"/content/drive/MyDrive/ANLP/project_1/urdu.txt\", \"/content/drive/MyDrive/ANLP/project_1/roman.txt\")\n",
        "\n",
        "    # Show samples\n",
        "    aggregator.show_samples(num_samples=5)\n",
        "\n",
        "\n",
        "    print(\"Dataset merging done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU9SGQwAdQHp"
      },
      "source": [
        "**tokenization on sample data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeDZEIHlE4s-",
        "outputId": "6b24249c-81c8-4894-846f-078c54772055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training urdu bpe tokenizer\n",
            "Start training bpe \n",
            "Initial words: 27\n",
            "Merge 50: merged ('ن', 'ظ')\n",
            "Total no of merges : 50\n",
            "final vocablry size: 44\n",
            "\n",
            "Original urdu txt:  وہ جو خوابوں میں آیا کبھی حقیقت بن جائے\n",
            "Encoded urdu txt:   [26, 38, 4, 1, 27, 1, 26, 37, 24, 1, 40, 7, 32, 1, 1, 41, 14, 1, 40, 1, 1, 9, 1, 1, 1, 43]\n",
            "Decoded urdu txt:   وہ <unk>و <unk>وں میں <unk>یا ک<unk><unk>ی ح<unk>ی<unk><unk>بن <unk><unk><unk>ے\n",
            "cross check:     False\n",
            "training roman bpe tokenizer\n",
            "Start training bpe \n",
            "Initial words: 26\n",
            "Merge 50: merged ('b', 'an</w>')\n",
            "Total no of merges : 50\n",
            "final vocablry size: 45\n",
            "\n",
            "Original roman txt:  woh jo khwabon mein aaya kabhi haqeeqat ban jaye\n",
            "Encoded roman txt:   [1, 32, 1, 4, 1, 32, 4, 21, 1, 1, 5, 1, 32, 31, 29, 5, 5, 43, 6, 21, 5, 1, 1, 19, 1, 1, 1, 1, 1, 10, 4, 11, 1, 43, 14]\n",
            "Decoded roman txt:   <unk>o<unk> <unk>o k<unk><unk>a<unk>on mein aaya ka<unk><unk>i <unk><unk><unk><unk><unk>at ban <unk>ye\n",
            "cross check:     False\n",
            "\n",
            "comparison\n",
            "Urdu vocablry size:   44 :::Roman vocablry size:  45\n",
            "Urdu seq length:   26 :::Roman seq length:  35\n",
            "word level comparison\n",
            "Word level vocablry size: 31\n",
            "Word level vocablry size: 30\n",
            "Urdu word vocablry:   31 :::Roman word vocablry:  30\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "#bpe\n",
        "class BPETokenizer:\n",
        "   #why vocab size 1000 nd num merge 500\n",
        "    def __init__(self, vocab_size=1000, num_merges=500):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_merges = num_merges\n",
        "        self.merges = []  # List of merge operations: [('r','a'), ('ra','m'), nd so on]\n",
        "        self.vocab = {}\n",
        "        self.special_tokens = ['<pad>', '<unk>', '<strt>', '<end>']\n",
        "\n",
        "    def train(self, texts):\n",
        "        print(\"Start training bpe \")\n",
        "\n",
        " #build initial word freq dict nd store each word as space-separated char with </w> marker\n",
        "        word_freq = defaultdict(int)\n",
        "\n",
        "        for text in texts:\n",
        "            text = text.strip().lower()\n",
        "            words = text.split()\n",
        "            for word in words:\n",
        "                if word:\n",
        "                    # changee \"meesha\" to \"m e e s h a </w>\"\n",
        "                    char_word = ' '.join(list(word)) + ' </w>'\n",
        "                    word_freq[char_word] += 1\n",
        "\n",
        "        print(f\"Initial words: {len(word_freq)}\")\n",
        "\n",
        "        # bpe merge\n",
        "        for merge_step in range(self.num_merges):\n",
        "            #count all adjacent pairs\n",
        "            pairs = defaultdict(int)\n",
        "\n",
        "            for word_str, freq in word_freq.items():\n",
        "                symbols = word_str.split()\n",
        "                for i in range(len(symbols) - 1):\n",
        "                    pair = (symbols[i], symbols[i + 1])\n",
        "                    pairs[pair] += freq\n",
        "\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            #find most common pair\n",
        "            best = max(pairs, key=pairs.get)\n",
        "\n",
        "            # Merge pair in all words\n",
        "            new_word_freq = defaultdict(int)\n",
        "            for word_str, freq in word_freq.items():\n",
        "                symbols = word_str.split()\n",
        "                new_symbols = []\n",
        "                i = 0\n",
        "                while i < len(symbols):\n",
        "                    if i < len(symbols) - 1 and symbols[i] == best[0] and symbols[i+1] == best[1]:\n",
        "                        new_symbols.append(best[0] + best[1])\n",
        "                        i += 2\n",
        "                    else:\n",
        "                        new_symbols.append(symbols[i])\n",
        "                        i += 1\n",
        "                new_word_freq[' '.join(new_symbols)] += freq\n",
        "\n",
        "            word_freq = new_word_freq\n",
        "            self.merges.append(best)\n",
        "\n",
        "            if (merge_step + 1) % 50 == 0:\n",
        "                print(f\"Merge {merge_step + 1}: merged {best}\")\n",
        "\n",
        "        print(f\"Total no of merges : {len(self.merges)}\")\n",
        "\n",
        "        # build vocabulary from final words\n",
        "        self.vocab = {}\n",
        "        idx = 0\n",
        "\n",
        "        #add special tokens\n",
        "        for token in self.special_tokens:\n",
        "            self.vocab[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "        #collect all tokens that show in final words\n",
        "        all_tokens = set()\n",
        "        for word_str in word_freq.keys():\n",
        "            for token in word_str.split():\n",
        "                all_tokens.add(token)\n",
        "\n",
        "        #add tokens to vocab\n",
        "        for token in sorted(all_tokens):\n",
        "            self.vocab[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "        print(f\"final vocablry size: {len(self.vocab)}\")\n",
        "        return self.vocab\n",
        "\n",
        "    def encode(self, text):\n",
        "        #convert text -> token IDs\n",
        "        text = text.strip().lower()\n",
        "        words = text.split()\n",
        "\n",
        "        token_ids = []\n",
        "\n",
        "        for word in words:\n",
        "            #convert word -> tokens\n",
        "            tokens = self._encode_word(word)\n",
        "            for token in tokens:\n",
        "                if token in self.vocab:\n",
        "                    token_ids.append(self.vocab[token])\n",
        "                else:\n",
        "                    token_ids.append(self.vocab['<unk>'])\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def _encode_word(self, word):\n",
        "        #encode a single word using trained BPE\n",
        "        word = word.lower()\n",
        "\n",
        "        #start with char: \"meesha\" -> ['m', 'e', 'e', 's', 'h', 'a' ,'</w>']\n",
        "        tokens = list(word) + ['</w>']\n",
        "\n",
        "        #apply each merge operation\n",
        "        for pair in self.merges:\n",
        "            new_tokens = []\n",
        "            i = 0\n",
        "            while i < len(tokens):\n",
        "                if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
        "                    #found pair nd merge it\n",
        "                    new_tokens.append(pair[0] + pair[1])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1\n",
        "            tokens = new_tokens\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        #convert token IDs -> text\n",
        "        id_to_token = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "        tokens = []\n",
        "        for tid in token_ids:\n",
        "            if tid in id_to_token:\n",
        "                tokens.append(id_to_token[tid])\n",
        "            else:\n",
        "                tokens.append('<unk>')\n",
        "\n",
        "        # Join nd recover spaces\n",
        "        text = ''.join(tokens)\n",
        "        text = text.replace('</w>', ' ')\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def save(self, filename):\n",
        "        #Save tokenizer\n",
        "        data = {\n",
        "            'vocablry': self.vocab,\n",
        "            'merges': self.merges,\n",
        "            'special tokens': self.special_tokens\n",
        "        }\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"saved to {filename}\")\n",
        "\n",
        "    def load(self, filename):\n",
        "      #Load tokenizer\n",
        "        with open(filename, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.vocab = data['vocablry']\n",
        "        self.merges = data['merges']\n",
        "        self.special_tokens = data['special tokens']\n",
        "        print(f\"loaded from {filename}\")\n",
        "\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    #word-level tokenizer\n",
        "    #splits text -> words - maps each word -> ID\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word_to_id = {}\n",
        "        self.id_to_word = {}\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        #build vocabulary\n",
        "        words = set()\n",
        "        for text in texts:\n",
        "            for word in text.lower().split():\n",
        "                words.add(word)\n",
        "\n",
        "        idx = 0\n",
        "        for token in ['<pad>', '<unk>', '<strt>', '<end>']:\n",
        "            self.word_to_id[token] = idx\n",
        "            self.id_to_word[idx] = token\n",
        "            idx += 1\n",
        "\n",
        "        for word in sorted(words):\n",
        "            self.word_to_id[word] = idx\n",
        "            self.id_to_word[idx] = word\n",
        "            idx += 1\n",
        "\n",
        "        print(f\"Word level vocablry size: {len(self.word_to_id)}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        #Text -> IDs\n",
        "        text = text.strip().lower()\n",
        "        words = text.split()\n",
        "        return [self.word_to_id.get(w, self.word_to_id['<unk>']) for w in words]\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        #IDs -> text\n",
        "        words = [self.id_to_word.get(tid, '<unk>') for tid in token_ids]\n",
        "        return ' '.join(words)\n",
        "\n",
        "#main\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # We need seperate tokenizers for Urdu nd Roman Urdu\n",
        "    # Bcz both use different char sets\n",
        "\n",
        "    urdu_texts = [\n",
        "\"چاندنی رات میں تیرا خیال آتا ہے\",\n",
        "\"دل کے ویران کو پھر سے جمال آتا ہے\",\n",
        "\"خواب بن کر تری صورت نظر آتی ہے\",\n",
        "\"یوں لگتا ہے کہ تُو میرے حال آتا ہے\" ]\n",
        "\n",
        "    roman_texts = [\n",
        "\"chaandni raat mein tera khayal aata hai\",\n",
        "\"dil ke veeran ko phir se jamal aata hai\",\n",
        "\"khaab ban kar teri soorat nazar aati hai\",\n",
        "\"yun lagta hai ke tu mere haal aata hai\"\n",
        "\n",
        "   ]\n",
        "\n",
        "    print(\"training urdu bpe tokenizer\")\n",
        "\n",
        "    urdu_bpe = BPETokenizer(vocab_size=500, num_merges=50)\n",
        "    urdu_bpe.train(urdu_texts)\n",
        "\n",
        "    test_urdu = \"وہ جو خوابوں میں آیا کبھی حقیقت بن جائے\"\n",
        "    enc_urdu = urdu_bpe.encode(test_urdu)\n",
        "    dec_urdu = urdu_bpe.decode(enc_urdu)\n",
        "\n",
        "    print(f\"\\nOriginal urdu txt:  {test_urdu}\")\n",
        "    print(f\"Encoded urdu txt:   {enc_urdu}\")\n",
        "    print(f\"Decoded urdu txt:   {dec_urdu}\")\n",
        "    print(f\"cross check:     {test_urdu == dec_urdu}\")\n",
        "\n",
        "    print(\"training roman bpe tokenizer\")\n",
        "\n",
        "    roman_bpe = BPETokenizer(vocab_size=500, num_merges=50)\n",
        "    roman_bpe.train(roman_texts)\n",
        "\n",
        "    test_roman = \"woh jo khwabon mein aaya kabhi haqeeqat ban jaye\"\n",
        "    enc_roman = roman_bpe.encode(test_roman)\n",
        "    dec_roman = roman_bpe.decode(enc_roman)\n",
        "\n",
        "    print(f\"\\nOriginal roman txt:  {test_roman}\")\n",
        "    print(f\"Encoded roman txt:   {enc_roman}\")\n",
        "    print(f\"Decoded roman txt:   {dec_roman}\")\n",
        "    print(f\"cross check:     {test_roman == dec_roman}\")\n",
        "\n",
        "    print(\"\\ncomparison\")\n",
        "\n",
        "    print(f\"Urdu vocablry size:   {len(urdu_bpe.vocab)} :::\" + f\"Roman vocablry size:  {len(roman_bpe.vocab)}\" )\n",
        "    print(f\"Urdu seq length:   {len(enc_urdu)} :::\" + f\"Roman seq length:  {len(enc_roman)}\")\n",
        "\n",
        "    print(\"word level comparison\")\n",
        "\n",
        "    urdu_word = SimpleTokenizer()\n",
        "    urdu_word.build_vocab(urdu_texts)\n",
        "\n",
        "    roman_word = SimpleTokenizer()\n",
        "    roman_word.build_vocab(roman_texts)\n",
        "\n",
        "    print(f\"Urdu word vocablry:   {len(urdu_word.word_to_id)} :::\" + f\"Roman word vocablry:  {len(roman_word.word_to_id)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmmvgtKXERlF"
      },
      "source": [
        "**tokenization on actual data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-fj2lT_IdhK",
        "outputId": "4a3299ce-013d-45c7-c3b7-40b63d05afbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load full dataset\n",
            "Loaded 21003 Urdu sentences\n",
            "Loaded 21003 Roman sentences\n",
            "training urdu bpe tokenizer on actual data\n",
            "Start training bpe \n",
            "Initial words: 10340\n",
            "Merge 50: merged ('ا', 'س</w>')\n",
            "Merge 100: merged ('ت', 'ی')\n",
            "Merge 150: merged ('ن', 'ظ')\n",
            "Merge 200: merged ('ا', 'ل</w>')\n",
            "Merge 250: merged ('پ', 'و')\n",
            "Merge 300: merged ('م', 'ی</w>')\n",
            "Merge 350: merged ('س', 'تی</w>')\n",
            "Merge 400: merged ('ت', 'و')\n",
            "Merge 450: merged ('گ', 'ا')\n",
            "Merge 500: merged ('کھ', 'ی')\n",
            "Total no of merges : 500\n",
            "final vocablry size: 560\n",
            "saved to /content/drive/MyDrive/ANLP/project_1/urdu_tokenizer.pkl\n",
            "\n",
            "Test example:\n",
            "actual:  آنکھ سے دور نہ ہو دل سے اتر جائے گا\n",
            "decoded:   آنکھ سے دور نہ ہو دل سے اتر جائے گا\n",
            "cross check:     True\n",
            "training roman bpe tokenizer on actual data\n",
            "Start training bpe \n",
            "Initial words: 16917\n",
            "Merge 50: merged ('s', 'h')\n",
            "Merge 100: merged ('g', 'u')\n",
            "Merge 150: merged ('a', 'n')\n",
            "Merge 200: merged ('p', 'u')\n",
            "Merge 250: merged ('aa', 'm</w>')\n",
            "Merge 300: merged ('yaa', 'd</w>')\n",
            "Merge 350: merged ('ha', 's')\n",
            "Merge 400: merged ('ya', 't</w>')\n",
            "Merge 450: merged ('sh', '-e-')\n",
            "Merge 500: merged ('jaa', 'na</w>')\n",
            "Total no of merges : 500\n",
            "final vocablry size: 534\n",
            "saved to /content/drive/MyDrive/ANLP/project_1/roman_tokenizer.pkl\n",
            "\n",
            "Test example:\n",
            "actual:  aankh se duur na ho dil se utar jaega\n",
            "decoded:   aankh se duur na ho dil se utar jaega\n",
            "cross check:     True\n",
            "Fianl comparison\n",
            "\n",
            "Urdu vocab size:         560 ::: Roman vocab size:        534\n",
            "Urdu total merges:       500 ::: Roman total merges:      500\n",
            "\n",
            "Tokenizers saved:\n",
            "- urdu_tokenizer.pkl- roman_tokenizer.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class BPETokenizer:\n",
        "   #why vocab size 1000 nd num merge 500\n",
        "    def __init__(self, vocab_size=1000, num_merges=500):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_merges = num_merges\n",
        "        self.merges = []  # List of merge operations: [('r','a'), ('ra','m'), nd so on]\n",
        "        self.vocab = {}\n",
        "        self.special_tokens = ['<pad>', '<unk>', '<strt>', '<end>']\n",
        "\n",
        "    def train(self, texts):\n",
        "        print(\"Start training bpe \")\n",
        "\n",
        " #build initial word freq dict nd store each word as space-separated char with </w> marker\n",
        "        word_freq = defaultdict(int)\n",
        "\n",
        "        for text in texts:\n",
        "            text = text.strip().lower()\n",
        "            words = text.split()\n",
        "            for word in words:\n",
        "                if word:\n",
        "                    # changee \"meesha\" to \"m e e s h a </w>\"\n",
        "                    char_word = ' '.join(list(word)) + ' </w>'\n",
        "                    word_freq[char_word] += 1\n",
        "\n",
        "        print(f\"Initial words: {len(word_freq)}\")\n",
        "\n",
        "\n",
        "        # bpe merge\n",
        "        for merge_step in range(self.num_merges):\n",
        "            #count all adjacent pairs\n",
        "            pairs = defaultdict(int)\n",
        "\n",
        "            for word_str, freq in word_freq.items():\n",
        "                symbols = word_str.split()\n",
        "                for i in range(len(symbols) - 1):\n",
        "                    pair = (symbols[i], symbols[i + 1])\n",
        "                    pairs[pair] += freq\n",
        "\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            #find most common pair\n",
        "            best = max(pairs, key=pairs.get)\n",
        "\n",
        "            # Merge pair in all words\n",
        "            new_word_freq = defaultdict(int)\n",
        "            for word_str, freq in word_freq.items():\n",
        "                symbols = word_str.split()\n",
        "                new_symbols = []\n",
        "                i = 0\n",
        "                while i < len(symbols):\n",
        "                    if i < len(symbols) - 1 and symbols[i] == best[0] and symbols[i+1] == best[1]:\n",
        "                        new_symbols.append(best[0] + best[1])\n",
        "                        i += 2\n",
        "                    else:\n",
        "                        new_symbols.append(symbols[i])\n",
        "                        i += 1\n",
        "                new_word_freq[' '.join(new_symbols)] += freq\n",
        "\n",
        "            word_freq = new_word_freq\n",
        "            self.merges.append(best)\n",
        "\n",
        "            if (merge_step + 1) % 50 == 0:\n",
        "                print(f\"Merge {merge_step + 1}: merged {best}\")\n",
        "\n",
        "        print(f\"Total no of merges : {len(self.merges)}\")\n",
        "\n",
        "        # build vocabulary from final words\n",
        "        self.vocab = {}\n",
        "        idx = 0\n",
        "\n",
        "        #add special tokens\n",
        "        for token in self.special_tokens:\n",
        "            self.vocab[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "        #collect all tokens that show in final words\n",
        "        all_tokens = set()\n",
        "        for word_str in word_freq.keys():\n",
        "            for token in word_str.split():\n",
        "                all_tokens.add(token)\n",
        "\n",
        "        #add tokens to vocab\n",
        "        for token in sorted(all_tokens):\n",
        "            self.vocab[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "        print(f\"final vocablry size: {len(self.vocab)}\")\n",
        "        return self.vocab\n",
        "\n",
        "    def encode(self, text):\n",
        "        #convert text -> token IDs\n",
        "        text = text.strip().lower()\n",
        "        words = text.split()\n",
        "\n",
        "        token_ids = []\n",
        "\n",
        "        for word in words:\n",
        "            #convert word -> tokens\n",
        "            tokens = self._encode_word(word)\n",
        "            for token in tokens:\n",
        "                if token in self.vocab:\n",
        "                    token_ids.append(self.vocab[token])\n",
        "                else:\n",
        "                    token_ids.append(self.vocab['<unk>'])\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def _encode_word(self, word):\n",
        "        #encode a single word using trained BPE\n",
        "        word = word.lower()\n",
        "\n",
        "        #start with char: \"meesha\" -> ['m', 'e', 'e', 's', 'h', 'a' ,'</w>']\n",
        "        tokens = list(word) + ['</w>']\n",
        "\n",
        "        #apply each merge operation\n",
        "        for pair in self.merges:\n",
        "            new_tokens = []\n",
        "            i = 0\n",
        "            while i < len(tokens):\n",
        "                if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
        "                    #found pair nd merge it\n",
        "                    new_tokens.append(pair[0] + pair[1])\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1\n",
        "            tokens = new_tokens\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        #convert token IDs -> text\n",
        "        id_to_token = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "        tokens = []\n",
        "        for tid in token_ids:\n",
        "            if tid in id_to_token:\n",
        "                tokens.append(id_to_token[tid])\n",
        "            else:\n",
        "                tokens.append('<unk>')\n",
        "\n",
        "        # Join nd recover spaces\n",
        "        text = ''.join(tokens)\n",
        "        text = text.replace('</w>', ' ')\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def save(self, filename):\n",
        "        #Save tokenizer\n",
        "        data = {\n",
        "            'vocablry': self.vocab,\n",
        "            'merges': self.merges,\n",
        "            'special tokens': self.special_tokens\n",
        "        }\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"saved to {filename}\")\n",
        "\n",
        "    def load(self, filename):\n",
        "      #Load tokenizer\n",
        "        with open(filename, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.vocab = data['vocablry']\n",
        "        self.merges = data['merges']\n",
        "        self.special_tokens = data['special tokens']\n",
        "        print(f\"loaded from {filename}\")\n",
        "\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    #word-level tokenizer\n",
        "    #splits text -> words - maps each word -> ID\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word_to_id = {}\n",
        "        self.id_to_word = {}\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        #build vocabulary\n",
        "        words = set()\n",
        "        for text in texts:\n",
        "            for word in text.lower().split():\n",
        "                words.add(word)\n",
        "\n",
        "        idx = 0\n",
        "        for token in ['<pad>', '<unk>', '<strt>', '<end>']:\n",
        "            self.word_to_id[token] = idx\n",
        "            self.id_to_word[idx] = token\n",
        "            idx += 1\n",
        "\n",
        "        for word in sorted(words):\n",
        "            self.word_to_id[word] = idx\n",
        "            self.id_to_word[idx] = word\n",
        "            idx += 1\n",
        "\n",
        "        print(f\"Word level vocablry size: {len(self.word_to_id)}\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        #Text -> IDs\n",
        "        text = text.strip().lower()\n",
        "        words = text.split()\n",
        "        return [self.word_to_id.get(w, self.word_to_id['<unk>']) for w in words]\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        #IDs -> text\n",
        "        words = [self.id_to_word.get(tid, '<unk>') for tid in token_ids]\n",
        "        return ' '.join(words)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load from your ACTUAL cleaned dataset files\n",
        "\n",
        "    print(\"Load full dataset\")\n",
        "\n",
        "    # Load all Urdu texts (21,003 sentences)\n",
        "    with open('/content/drive/MyDrive/ANLP/project_1/urdu.txt', 'r', encoding='utf-8') as f:\n",
        "        urdu_texts = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "    # Load all Roman texts (21,003 sentences)\n",
        "    with open('/content/drive/MyDrive/ANLP/project_1/roman.txt', 'r', encoding='utf-8') as f:\n",
        "        roman_texts = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "    print(f\"Loaded {len(urdu_texts)} Urdu sentences\")\n",
        "    print(f\"Loaded {len(roman_texts)} Roman sentences\")\n",
        "\n",
        "    print(\"training urdu bpe tokenizer on actual data\")\n",
        "\n",
        "    urdu_bpe = BPETokenizer(vocab_size=1000, num_merges=500)\n",
        "    urdu_bpe.train(urdu_texts)\n",
        "    urdu_bpe.save('/content/drive/MyDrive/ANLP/project_1/urdu_tokenizer.pkl')\n",
        "\n",
        "    # Test with one example\n",
        "    test_urdu = urdu_texts[0]\n",
        "    enc_urdu = urdu_bpe.encode(test_urdu)\n",
        "    dec_urdu = urdu_bpe.decode(enc_urdu)\n",
        "\n",
        "    print(f\"\\nTest example:\")\n",
        "    print(f\"actual:  {test_urdu}\")\n",
        "    print(f\"decoded:   {dec_urdu}\")\n",
        "    print(f\"cross check:     {test_urdu == dec_urdu}\")\n",
        "\n",
        "    print(\"training roman bpe tokenizer on actual data\")\n",
        "\n",
        "    roman_bpe = BPETokenizer(vocab_size=1000, num_merges=500)\n",
        "    roman_bpe.train(roman_texts)\n",
        "    roman_bpe.save('/content/drive/MyDrive/ANLP/project_1/roman_tokenizer.pkl')\n",
        "\n",
        "    # Test with one example\n",
        "    test_roman = roman_texts[0]\n",
        "    enc_roman = roman_bpe.encode(test_roman)\n",
        "    dec_roman = roman_bpe.decode(enc_roman)\n",
        "\n",
        "    print(f\"\\nTest example:\")\n",
        "    print(f\"actual:  {test_roman}\")\n",
        "    print(f\"decoded:   {dec_roman}\")\n",
        "    print(f\"cross check:     {test_roman == dec_roman}\")\n",
        "\n",
        "    print(\"Fianl comparison\\n\")\n",
        "    print(f\"Urdu vocab size:         {len(urdu_bpe.vocab)} ::: \" + f\"Roman vocab size:        {len(roman_bpe.vocab)}\")\n",
        "    print(f\"Urdu total merges:       {len(urdu_bpe.merges)} ::: \" +f\"Roman total merges:      {len(roman_bpe.merges)}\")\n",
        "    print(f\"\\nTokenizers saved:\")\n",
        "    print(f\"- urdu_tokenizer.pkl\"  + f\"- roman_tokenizer.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnax2j2fN1GU"
      },
      "source": [
        "**Loading Tokenizers | Encoding Text | Creating PyTorch Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNPUVl3JN2w1",
        "outputId": "ab5faedf-1f0d-45b4-e090-e0fef31ac189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizers...\n",
            "Tokenizers loaded successfully!\n",
            "Urdu vocab size: 560, Merges: 500\n",
            "Roman vocab size: 534, Merges: 500\n",
            "\n",
            "Load dataset\n",
            "Loaded 21003 Urdu sentences\n",
            "Loaded 21003 Roman sentences\n",
            "\n",
            "Tokenizing sentences using BPE\n",
            "Tokenized 0 sentences\n",
            "Tokenized 5000 sentences\n",
            "Tokenized 10000 sentences\n",
            "Tokenized 15000 sentences\n",
            "Tokenized 20000 sentences\n",
            "Total sequences tokenized: 21003\n",
            "Creating train/val/test splits\n",
            "Train set size: 10501\n",
            "Validation set size: 5251\n",
            "Test set size: 5251\n",
            "\n",
            "Creating Dataset obj\n",
            "Train dataset: 10501 samples\n",
            "Val dataset: 5251 samples\n",
            "Test dataset: 5251 samples\n",
            "creating DataLoaders\n",
            "Train loader batches: 329\n",
            "Val loader batches: 165\n",
            "Test loader batches: 165\n",
            "Testing one batch from train loader\n",
            "Urdu batch shape: torch.Size([32, 19]) ::: Roman batch shape: torch.Size([32, 21])\n",
            "Urdu lengths: tensor([13, 13, 16, 19,  8]) ::: Roman lengths: tensor([16, 14, 16, 16,  9])\n",
            "\n",
            "Data preparation done next step -> model training\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the trained tokenizers\n",
        "print(\"Loading tokenizers...\")\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/urdu_tokenizer.pkl', 'rb') as f:\n",
        "    urdu_data = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/roman_tokenizer.pkl', 'rb') as f:\n",
        "    roman_data = pickle.load(f)\n",
        "\n",
        "print(\"Tokenizers loaded successfully!\")\n",
        "\n",
        "# Extract vocabulary nd merges\n",
        "urdu_vocab = urdu_data['vocablry']\n",
        "urdu_merges = urdu_data['merges']\n",
        "\n",
        "roman_vocab = roman_data['vocablry']\n",
        "roman_merges = roman_data['merges']\n",
        "\n",
        "print(f\"Urdu vocab size: {len(urdu_vocab)}, Merges: {len(urdu_merges)}\")\n",
        "print(f\"Roman vocab size: {len(roman_vocab)}, Merges: {len(roman_merges)}\")\n",
        "\n",
        "# Load the data\n",
        "print(\"\\nLoad dataset\")\n",
        "urdu_sentences = []\n",
        "roman_sentences = []\n",
        "\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/urdu.txt', 'r', encoding='utf-8') as f:\n",
        "    urdu_sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/roman.txt', 'r', encoding='utf-8') as f:\n",
        "    roman_sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "print(f\"Loaded {len(urdu_sentences)} Urdu sentences\")\n",
        "print(f\"Loaded {len(roman_sentences)} Roman sentences\")\n",
        "\n",
        "# Function to encode words using BPE merges\n",
        "def encode_with_bpe(word, vocab, merges):\n",
        "    #Encode a single word using BPE vocablty nd merges\n",
        "    word = word.lower()\n",
        "\n",
        "    tokens = list(word) + ['</w>']\n",
        "\n",
        "    # Apply each merge operation\n",
        "    for pair in merges:\n",
        "        new_tokens = []\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
        "                # Found pair, merge it\n",
        "                new_tokens.append(pair[0] + pair[1])\n",
        "                i += 2\n",
        "            else:\n",
        "                new_tokens.append(tokens[i])\n",
        "                i += 1\n",
        "        tokens = new_tokens\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function to encode full text\n",
        "def encode_text(text, vocab, merges):\n",
        "    #convert text -> token IDs using BPE\n",
        "    text = text.strip().lower()\n",
        "    words = text.split()\n",
        "\n",
        "    token_ids = []\n",
        "    for word in words:\n",
        "        tokens = encode_with_bpe(word, vocab, merges)\n",
        "        for token in tokens:\n",
        "            if token in vocab:\n",
        "                token_ids.append(vocab[token])\n",
        "            else:\n",
        "                token_ids.append(vocab.get('<unk>', 1))  # Use unk token ID\n",
        "\n",
        "    return token_ids\n",
        "\n",
        "# Tokenize all sentences using BPE\n",
        "print(\"\\nTokenizing sentences using BPE\")\n",
        "urdu_tokens = []\n",
        "roman_tokens = []\n",
        "\n",
        "for i, (urdu_sent, roman_sent) in enumerate(zip(urdu_sentences, roman_sentences)):\n",
        "    if i % 5000 == 0:\n",
        "        print(f\"Tokenized {i} sentences\")\n",
        "\n",
        "    # Encode using BPE -> returns list of token IDs\n",
        "    urdu_seq = encode_text(urdu_sent, urdu_vocab, urdu_merges)\n",
        "    roman_seq = encode_text(roman_sent, roman_vocab, roman_merges)\n",
        "\n",
        "    urdu_tokens.append(urdu_seq)\n",
        "    roman_tokens.append(roman_seq)\n",
        "\n",
        "print(f\"Total sequences tokenized: {len(urdu_tokens)}\")\n",
        "\n",
        "# Create train/val/test splits 50%, 25%, 25%\n",
        "print(\"Creating train/val/test splits\")\n",
        "\n",
        "# split train 50% nd temp 50%\n",
        "train_urdu, temp_urdu, train_roman, temp_roman = train_test_split(\n",
        "    urdu_tokens, roman_tokens, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# split temp into val 25% nd test 25%\n",
        "val_urdu, test_urdu, val_roman, test_roman = train_test_split(\n",
        "    temp_urdu, temp_roman, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train set size: {len(train_urdu)}\")\n",
        "print(f\"Validation set size: {len(val_urdu)}\")\n",
        "print(f\"Test set size: {len(test_urdu)}\")\n",
        "\n",
        "# Custom PyTorch Dataset class\n",
        "class UrduRomanDataset(Dataset):\n",
        "    def __init__(self, urdu_sequences, roman_sequences):\n",
        "        self.urdu_sequences = urdu_sequences\n",
        "        self.roman_sequences = roman_sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.urdu_sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        urdu_seq = self.urdu_sequences[idx]\n",
        "        roman_seq = self.roman_sequences[idx]\n",
        "\n",
        "        return {\n",
        "            'urdu': torch.tensor(urdu_seq, dtype=torch.long),\n",
        "            'roman': torch.tensor(roman_seq, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create Dataset obj\n",
        "print(\"\\nCreating Dataset obj\")\n",
        "train_dataset = UrduRomanDataset(train_urdu, train_roman)\n",
        "val_dataset = UrduRomanDataset(val_urdu, val_roman)\n",
        "test_dataset = UrduRomanDataset(test_urdu, test_roman)\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Val dataset: {len(val_dataset)} samples\")\n",
        "print(f\"Test dataset: {len(test_dataset)} samples\")\n",
        "\n",
        "# Padding function for DataLoader\n",
        "def pad_batch(batch):\n",
        "    urdu_seqs = [item['urdu'] for item in batch]\n",
        "    roman_seqs = [item['roman'] for item in batch]\n",
        "\n",
        "    # find max length in batch\n",
        "    urdu_max_len = max(len(seq) for seq in urdu_seqs)\n",
        "    roman_max_len = max(len(seq) for seq in roman_seqs)\n",
        "\n",
        "    # Pad sequences\n",
        "    urdu_padded = []\n",
        "    roman_padded = []\n",
        "\n",
        "    for seq in urdu_seqs:\n",
        "        padded = torch.cat([seq, torch.zeros(urdu_max_len - len(seq), dtype=torch.long)])\n",
        "        urdu_padded.append(padded)\n",
        "\n",
        "    for seq in roman_seqs:\n",
        "        padded = torch.cat([seq, torch.zeros(roman_max_len - len(seq), dtype=torch.long)])\n",
        "        roman_padded.append(padded)\n",
        "\n",
        "    urdu_batch = torch.stack(urdu_padded)\n",
        "    roman_batch = torch.stack(roman_padded)\n",
        "\n",
        "    return {\n",
        "        'urdu': urdu_batch,\n",
        "        'roman': roman_batch,\n",
        "        'urdu_lengths': torch.tensor([len(seq) for seq in urdu_seqs], dtype=torch.long),\n",
        "        'roman_lengths': torch.tensor([len(seq) for seq in roman_seqs], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "# Create DataLoaders\n",
        "print(\"creating DataLoaders\")\n",
        "\n",
        "BATCH_SIZE = 32  # You can change this to 64 or 128\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=pad_batch\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=pad_batch\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=pad_batch\n",
        ")\n",
        "\n",
        "print(f\"Train loader batches: {len(train_loader)}\")\n",
        "print(f\"Val loader batches: {len(val_loader)}\")\n",
        "print(f\"Test loader batches: {len(test_loader)}\")\n",
        "\n",
        "# Test one batch\n",
        "print(\"Testing one batch from train loader\")\n",
        "\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"Urdu batch shape: {sample_batch['urdu'].shape} ::: \" + f\"Roman batch shape: {sample_batch['roman'].shape}\" )\n",
        "print(f\"Urdu lengths: {sample_batch['urdu_lengths'][:5]} ::: \" + f\"Roman lengths: {sample_batch['roman_lengths'][:5]}\")\n",
        "\n",
        "print(\"\\nData preparation done next step -> model training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NNHqjlcL3YW"
      },
      "source": [
        "**2. Model Architecture**\n",
        "\n",
        "* Build\ta\tseq2seq\tmodel\twith\ta\tBiLSTM\n",
        "encoder\tand\tan\tLSTM\tdecoder.\n",
        "* Use\t2\tlayers\tin\tthe\tencoder\tand\t4\tlayers\tin\tthe\tdecod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqFIIUCEN38V",
        "outputId": "5860f7e7-372b-4183-e750-93b62fe88a7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Model Architecture\n",
            "Device: cuda\n",
            "\n",
            "Hyperparameters:\n",
            "Urdu Vocab Size: 560 ::: Roman Vocab Size: 534\n",
            "Embedding Dimension: 256\n",
            "Hidden Size: 256\n",
            "Dropout: 0.3\n",
            "Creating Encoder BiLSTM with 2 layers\n",
            "Encoder created\n",
            "Encoder parameters: 2,772,992\n",
            "Creating Decoder LSTM with 4 layers\n",
            "Decoder created\n",
            "Decoder parameters: 2,379,286\n",
            "Creating Complete Seq2Seq Model\n",
            "Seq2Seq model created\n",
            "\n",
            "Total model parameters: 5,152,278\n",
            "Testing forward pass\n",
            "Input shape - src_ids: torch.Size([4, 15]), src_lengths: torch.Size([4])\n",
            "Target shape - tgt_ids: torch.Size([4, 18])\n",
            "Output shape: torch.Size([4, 18, 534])\n",
            "Expected shape: (4, 18, 534)\n",
            "\n",
            " Forward pass done\n",
            "Model Architecture Testing donee\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# ENCODER BiLSTM with 2 layers\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "\n",
        "    #Takes Urdu text (token IDs) and encodes it -> context vector\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=2, dropout=0.3):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer converts token IDs -> vectors\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0  # Treat padding as special token\n",
        "        )\n",
        "\n",
        "        # BiLSTM processes sequence in both directions\n",
        "        # bidirectional=True means output size will be hidden_size * 2\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, lengths):\n",
        "\n",
        "        #Forward pass of encoder\n",
        "\n",
        "        # Convert token IDs-> embedding vectors\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Pack padded sequence to handle variable lengths\n",
        "        packed_embedded = pack_padded_sequence(\n",
        "            embedded,\n",
        "            lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Pass through BiLSTM\n",
        "        packed_outputs, (hidden, cell) = self.bilstm(packed_embedded)\n",
        "\n",
        "        # Unpack sequence back to padded format\n",
        "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "\n",
        "# DECODER - LSTM with 4 layers\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "\n",
        "   # Takes the encoder output and generates Roman Urdu text one token at a time\n",
        "\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=4, dropout=0.3):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Embedding layer converts token IDs ->vectors\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        # LSTM unidirectional 4 layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Output layer converts hidden state to vocabulry probabilities\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_id, hidden, cell):\n",
        "\n",
        "        #Forward pass of decoder (one step at a time)\n",
        "\n",
        "\n",
        "        # Embed the input token\n",
        "        embedded = self.embedding(input_id)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # Pass through LSTM\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        logits = self.fc_out(output.squeeze(1))\n",
        "\n",
        "        return logits, hidden, cell\n",
        "\n",
        "\n",
        "# seq2seq MODEL\n",
        "\n",
        "class Seq2SeqNMT(nn.Module):\n",
        "\n",
        "\n",
        "    #Combines BiLSTM Encoder (2 layers) and LSTM Decoder (4 layers) for translating Urdu to Roman Urdu\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2SeqNMT, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src_ids, src_lengths, tgt_ids, teacher_forcing_ratio=0.5):\n",
        "\n",
        "\n",
        "        batch_size = src_ids.size(0)\n",
        "        tgt_seq_length = tgt_ids.size(1)\n",
        "        tgt_vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        # Encode the Urdu text\n",
        "        encoder_outputs, encoder_hidden, encoder_cell = self.encoder(src_ids, src_lengths)\n",
        "\n",
        "\n",
        "        # Initialize decoder hidden and cell states from encoder\n",
        "\n",
        "        decoder_hidden, decoder_cell = self._init_decoder_states(\n",
        "            encoder_hidden, encoder_cell, batch_size\n",
        "        )\n",
        "\n",
        "        # Prepare storage for outputs\n",
        "        outputs = torch.zeros(batch_size, tgt_seq_length, tgt_vocab_size).to(self.device)\n",
        "\n",
        "        #  Start with <strt> token usually token ID 2\n",
        "        decoder_input = torch.full((batch_size, 1), 2, dtype=torch.long).to(self.device)\n",
        "\n",
        "        #  Decode step by step\n",
        "        for t in range(tgt_seq_length):\n",
        "            # Get decoder output for current step\n",
        "            logits, decoder_hidden, decoder_cell = self.decoder(\n",
        "                decoder_input, decoder_hidden, decoder_cell\n",
        "            )\n",
        "\n",
        "            # Store the output\n",
        "            outputs[:, t, :] = logits\n",
        "\n",
        "            # Decide what to feed to decoder for next step\n",
        "            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n",
        "\n",
        "            if use_teacher_forcing:\n",
        "                # Use ground truth token as next input\n",
        "                decoder_input = tgt_ids[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                # Use models best prediction\n",
        "                decoder_input = logits.argmax(1).unsqueeze(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _init_decoder_states(self, encoder_hidden, encoder_cell, batch_size):\n",
        "\n",
        "\n",
        "        # Average bidirectional hidden states\n",
        "        encoder_hidden = encoder_hidden.view(\n",
        "            self.encoder.num_layers,\n",
        "            2,\n",
        "            batch_size,\n",
        "            self.encoder.hidden_size\n",
        "        )\n",
        "\n",
        "        encoder_hidden = encoder_hidden.mean(dim=1)\n",
        "\n",
        "        encoder_cell = encoder_cell.view(\n",
        "            self.encoder.num_layers,\n",
        "            2,\n",
        "            batch_size,\n",
        "            self.encoder.hidden_size\n",
        "        )\n",
        "        encoder_cell = encoder_cell.mean(dim=1)\n",
        "\n",
        "        # Expand from 2 encoder layers to 4 decoder layers\n",
        "        decoder_hidden_list = []\n",
        "        decoder_cell_list = []\n",
        "\n",
        "        for layer_idx in range(self.decoder.num_layers):\n",
        "            if layer_idx < encoder_hidden.size(0):\n",
        "                h = encoder_hidden[layer_idx]  # (batch_size, hidden_size)\n",
        "                c = encoder_cell[layer_idx]    # (batch_size, hidden_size)\n",
        "            else:\n",
        "                # Repeat last encoder layer for remaining decoder layers\n",
        "                h = encoder_hidden[-1]\n",
        "                c = encoder_cell[-1]\n",
        "\n",
        "            decoder_hidden_list.append(h.unsqueeze(0))\n",
        "            decoder_cell_list.append(c.unsqueeze(0))\n",
        "\n",
        "        # Concatenate all layers\n",
        "        decoder_hidden = torch.cat(decoder_hidden_list, dim=0)\n",
        "        decoder_cell = torch.cat(decoder_cell_list, dim=0)\n",
        "\n",
        "        return decoder_hidden, decoder_cell\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing Model Architecture\")\n",
        "\n",
        "    # Device setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Hyperparameters\n",
        "    URDU_VOCAB_SIZE = 560\n",
        "    ROMAN_VOCAB_SIZE = 534\n",
        "    EMBEDDING_DIM = 256\n",
        "    HIDDEN_SIZE = 256\n",
        "    DROPOUT = 0.3\n",
        "\n",
        "    print(f\"\\nHyperparameters:\")\n",
        "    print(f\"Urdu Vocab Size: {URDU_VOCAB_SIZE} ::: \" + f\"Roman Vocab Size: {ROMAN_VOCAB_SIZE}\")\n",
        "    print(f\"Embedding Dimension: {EMBEDDING_DIM}\")\n",
        "    print(f\"Hidden Size: {HIDDEN_SIZE}\")\n",
        "    print(f\"Dropout: {DROPOUT}\")\n",
        "\n",
        "    # Create encoder\n",
        "    print(\"Creating Encoder BiLSTM with 2 layers\")\n",
        "    encoder = Encoder(\n",
        "        vocab_size=URDU_VOCAB_SIZE,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=2,\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "    print(f\"Encoder created\")\n",
        "    print(f\"Encoder parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
        "\n",
        "    # Create decoder\n",
        "    print(\"Creating Decoder LSTM with 4 layers\")\n",
        "    decoder = Decoder(\n",
        "        vocab_size=ROMAN_VOCAB_SIZE,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_layers=4,\n",
        "        dropout=DROPOUT\n",
        "    ).to(device)\n",
        "    print(f\"Decoder created\")\n",
        "    print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters()):,}\")\n",
        "\n",
        "    # Create full seq2seq model\n",
        "    print(\"Creating Complete Seq2Seq Model\")\n",
        "    model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "    print(f\"Seq2Seq model created\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nTotal model parameters: {total_params:,}\")\n",
        "\n",
        "    # Test forward pass\n",
        "    print(\"Testing forward pass\")\n",
        "\n",
        "    batch_size = 4\n",
        "    src_seq_len = 15\n",
        "    tgt_seq_len = 18\n",
        "\n",
        "    # Create dummy input\n",
        "    src_ids = torch.randint(0, URDU_VOCAB_SIZE, (batch_size, src_seq_len)).to(device)\n",
        "    src_lengths = torch.tensor([15, 12, 14, 13]).to(device)\n",
        "    tgt_ids = torch.randint(0, ROMAN_VOCAB_SIZE, (batch_size, tgt_seq_len)).to(device)\n",
        "\n",
        "    print(f\"Input shape - src_ids: {src_ids.shape}, src_lengths: {src_lengths.shape}\")\n",
        "    print(f\"Target shape - tgt_ids: {tgt_ids.shape}\")\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(src_ids, src_lengths, tgt_ids, teacher_forcing_ratio=0.5)\n",
        "\n",
        "    print(f\"Output shape: {outputs.shape}\")\n",
        "    print(f\"Expected shape: ({batch_size}, {tgt_seq_len}, {ROMAN_VOCAB_SIZE})\")\n",
        "\n",
        "    if outputs.shape == (batch_size, tgt_seq_len, ROMAN_VOCAB_SIZE):\n",
        "        print(\"\\n Forward pass done\")\n",
        "    else:\n",
        "        print(\"\\n Forward pass failed\")\n",
        "\n",
        "    print(\"Model Architecture Testing donee\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qu4_kR_9M-K"
      },
      "source": [
        "**model testing - Components:**\n",
        "* Encoder: 2-Layer Bidirectional LSTM\n",
        "* Attention Mechanism: Additive (Concatenative)\n",
        "* Decoder: 4-Layer LSTM with Attention\n",
        "* Framework: PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1AVcIkl_LSd",
        "outputId": "fad2865e-2334-4e1c-a640-98ecf790c393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing model architecture\n",
            "device used: cuda\n",
            "urdu vocab: 560\n",
            "roman vocab: 534\n",
            "embedding dim: 256\n",
            "hidden size: 256\n",
            "creating encoder bilstm 2 layers\n",
            "encoder params: 2772992\n",
            "creating decoder lstm 4 layers with attention\n",
            "decoder params: 2904343\n",
            "creating full seq2seq model\n",
            "total params: 5677335\n",
            "input shape: torch.Size([4, 15])\n",
            "target shape: torch.Size([4, 18])\n",
            "output shape: torch.Size([4, 18, 534])\n",
            "forward pass done successfully\n",
            "model testing done\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# encoder part - bilstm with 2 layers\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=2, dropout=0.3):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # embedding layer to convert token ids to vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # bilstm to process input in both directions\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, lengths):\n",
        "        # embed input tokens\n",
        "        embedded = self.embedding(input_ids)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # pack padded sequence\n",
        "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # bilstm forward pass\n",
        "        packed_out, (hidden, cell) = self.bilstm(packed)\n",
        "\n",
        "        # unpack sequence\n",
        "        outputs, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
        "\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "\n",
        "# attention mechanism\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, encoder_output_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention = nn.Linear(hidden_size + encoder_output_size, 1)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs, src_lengths):\n",
        "        batch_size, src_seq_len, encoder_output_size = encoder_outputs.size()\n",
        "\n",
        "        # expand decoder hidden to match encoder length\n",
        "        dec_hidden_exp = decoder_hidden.unsqueeze(1).expand(batch_size, src_seq_len, -1)\n",
        "\n",
        "        # concat decoder hidden and encoder outputs\n",
        "        combined = torch.cat([dec_hidden_exp, encoder_outputs], dim=2)\n",
        "\n",
        "        # energy scores\n",
        "        energy = self.attention(combined).squeeze(2)\n",
        "\n",
        "        # mask for padding\n",
        "        mask = torch.arange(src_seq_len, device=encoder_outputs.device).unsqueeze(0) < src_lengths.unsqueeze(1)\n",
        "        energy = energy.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "        # softmax attention weights\n",
        "        attn_weights = torch.softmax(energy, dim=1)\n",
        "        attn_weights = attn_weights.masked_fill(~mask, 0)\n",
        "\n",
        "        # context vector\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "\n",
        "        return context, attn_weights\n",
        "\n",
        "\n",
        "# decoder part - lstm with 4 layers + attention\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, encoder_hidden_size, num_layers=4, dropout=0.3):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        # attention mechanism\n",
        "        self.attention = Attention(hidden_size, encoder_hidden_size * 2)\n",
        "\n",
        "        # lstm layer takes embedding + context\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim + encoder_hidden_size * 2,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_id, hidden, cell, encoder_outputs, src_lengths):\n",
        "        # embed token\n",
        "        embedded = self.embedding(input_id)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # attention part\n",
        "        dec_hidden_attn = hidden[-1]\n",
        "        context, attn_weights = self.attention(dec_hidden_attn, encoder_outputs, src_lengths)\n",
        "\n",
        "        # concat embedding and context\n",
        "        lstm_input = torch.cat([embedded.squeeze(1), context], dim=1).unsqueeze(1)\n",
        "\n",
        "        # lstm forward\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "\n",
        "        # output projection\n",
        "        logits = self.fc_out(output.squeeze(1))\n",
        "\n",
        "        return logits, hidden, cell\n",
        "\n",
        "\n",
        "# main seq2seq model with attention\n",
        "class Seq2SeqNMT(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2SeqNMT, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src_ids, src_lengths, tgt_ids, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src_ids.size(0)\n",
        "        tgt_len = tgt_ids.size(1)\n",
        "        vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        # encoder forward\n",
        "        enc_outputs, enc_hidden, enc_cell = self.encoder(src_ids, src_lengths)\n",
        "\n",
        "        # init decoder states\n",
        "        dec_hidden, dec_cell = self._init_decoder_states(enc_hidden, enc_cell, batch_size)\n",
        "\n",
        "        # prepare output\n",
        "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(self.device)\n",
        "\n",
        "        # start token id 2\n",
        "        dec_input = torch.full((batch_size, 1), 2, dtype=torch.long, device=self.device)\n",
        "\n",
        "        # decode loop\n",
        "        for t in range(tgt_len):\n",
        "            logits, dec_hidden, dec_cell = self.decoder(dec_input, dec_hidden, dec_cell, enc_outputs, src_lengths)\n",
        "            outputs[:, t, :] = logits\n",
        "\n",
        "            # teacher forcing\n",
        "            if torch.rand(1).item() < teacher_forcing_ratio:\n",
        "                dec_input = tgt_ids[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                dec_input = logits.argmax(1).unsqueeze(1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _init_decoder_states(self, enc_hidden, enc_cell, batch_size):\n",
        "        # reshape and avg bidirectional states\n",
        "        enc_hidden = enc_hidden.view(self.encoder.num_layers, 2, batch_size, self.encoder.hidden_size).mean(dim=1)\n",
        "        enc_cell = enc_cell.view(self.encoder.num_layers, 2, batch_size, self.encoder.hidden_size).mean(dim=1)\n",
        "\n",
        "        # map to decoder layers\n",
        "        dec_h, dec_c = [], []\n",
        "        for i in range(self.decoder.num_layers):\n",
        "            idx = i % self.encoder.num_layers\n",
        "            dec_h.append(enc_hidden[idx].unsqueeze(0))\n",
        "            dec_c.append(enc_cell[idx].unsqueeze(0))\n",
        "\n",
        "        return torch.cat(dec_h, dim=0), torch.cat(dec_c, dim=0)\n",
        "\n",
        "\n",
        "# test run\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"testing model architecture\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"device used:\", device)\n",
        "\n",
        "    # hyper params\n",
        "    URDU_VOCAB = 560\n",
        "    ROMAN_VOCAB = 534\n",
        "    EMBED_DIM = 256\n",
        "    HIDDEN = 256\n",
        "    DROP = 0.3\n",
        "\n",
        "    print(\"urdu vocab:\", URDU_VOCAB)\n",
        "    print(\"roman vocab:\", ROMAN_VOCAB)\n",
        "    print(\"embedding dim:\", EMBED_DIM)\n",
        "    print(\"hidden size:\", HIDDEN)\n",
        "\n",
        "    # encoder\n",
        "    print(\"creating encoder bilstm 2 layers\")\n",
        "    encoder = Encoder(URDU_VOCAB, EMBED_DIM, HIDDEN, num_layers=2, dropout=DROP).to(device)\n",
        "    print(\"encoder params:\", sum(p.numel() for p in encoder.parameters()))\n",
        "\n",
        "    # decoder\n",
        "    print(\"creating decoder lstm 4 layers with attention\")\n",
        "    decoder = Decoder(ROMAN_VOCAB, EMBED_DIM, HIDDEN, encoder_hidden_size=HIDDEN, num_layers=4, dropout=DROP).to(device)\n",
        "    print(\"decoder params:\", sum(p.numel() for p in decoder.parameters()))\n",
        "\n",
        "    # seq2seq model\n",
        "    print(\"creating full seq2seq model\")\n",
        "    model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "    print(\"total params:\", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "    # sample input test\n",
        "    batch = 4\n",
        "    src_len = 15\n",
        "    tgt_len = 18\n",
        "    src_ids = torch.randint(0, URDU_VOCAB, (batch, src_len)).to(device)\n",
        "    src_lens = torch.tensor([15, 12, 14, 13]).to(device)\n",
        "    tgt_ids = torch.randint(0, ROMAN_VOCAB, (batch, tgt_len)).to(device)\n",
        "\n",
        "    print(\"input shape:\", src_ids.shape)\n",
        "    print(\"target shape:\", tgt_ids.shape)\n",
        "\n",
        "    out = model(src_ids, src_lens, tgt_ids, teacher_forcing_ratio=0.5)\n",
        "\n",
        "    print(\"output shape:\", out.shape)\n",
        "    print(\"forward pass done successfully\")\n",
        "    print(\"model testing done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgVHXX7Xb7kQ"
      },
      "source": [
        "**training + experiment management**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2wiHCLYAlku",
        "outputId": "43b1979b-eb87-4ce8-e6c9-5ccbbaffd977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using: cuda\n",
            "ready to train\n",
            "cross check your loaders + model before running\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"using:\", device)\n",
        "\n",
        "# experiments setup\n",
        "experiments = {\n",
        "    'Exp1_Baseline': {\n",
        "        'embedding_dim': 256,\n",
        "        'hidden_size': 256,\n",
        "        'learning_rate': 1e-3,\n",
        "        'dropout': 0.3,\n",
        "        'batch_size': 32\n",
        "    },\n",
        "    'Exp2_BiggerModel': {\n",
        "        'embedding_dim': 512,\n",
        "        'hidden_size': 256,\n",
        "        'learning_rate': 5e-4,\n",
        "        'dropout': 0.5,\n",
        "        'batch_size': 32\n",
        "    },\n",
        "    'Exp3_SmallAndFast': {\n",
        "        'embedding_dim': 128,\n",
        "        'hidden_size': 512,\n",
        "        'learning_rate': 1e-4,\n",
        "        'dropout': 0.1,\n",
        "        'batch_size': 64\n",
        "    }\n",
        "}\n",
        "\n",
        "# one epoch train\n",
        "def train_one_epoch(model, train_loader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"train\", leave=False):\n",
        "        src = batch['urdu'].to(device)\n",
        "        src_len = batch['urdu_lengths'].to(device)\n",
        "        tgt = batch['roman'].to(device)\n",
        "\n",
        "        preds = model(src, src_len, tgt, teacher_forcing_ratio=0.5)\n",
        "\n",
        "        loss = loss_fn(preds.reshape(-1, preds.size(-1)), tgt.reshape(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    perp = math.exp(avg_loss)\n",
        "    return avg_loss, perp\n",
        "\n",
        "\n",
        "# validation\n",
        "def validate_model(model, val_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"val\", leave=False):\n",
        "            src = batch['urdu'].to(device)\n",
        "            src_len = batch['urdu_lengths'].to(device)\n",
        "            tgt = batch['roman'].to(device)\n",
        "\n",
        "            preds = model(src, src_len, tgt, teacher_forcing_ratio=0.0)\n",
        "            loss = loss_fn(preds.reshape(-1, preds.size(-1)), tgt.reshape(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    perp = math.exp(avg_loss)\n",
        "    return avg_loss, perp\n",
        "\n",
        "\n",
        "# run one experiment\n",
        "def run_one_experiment(exp_name, exp_config, model, train_loader, val_loader, test_loader, device):\n",
        "    print(f\"\\n>>> running {exp_name}\")\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=exp_config['learning_rate'])\n",
        "\n",
        "    results = {'train_loss': [], 'train_ppl': [], 'val_loss': [], 'val_ppl': [], 'epochs': []}\n",
        "\n",
        "    best_val = float('inf')\n",
        "    best_epoch = 0\n",
        "    num_epochs = 20\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\nepoch {epoch}\")\n",
        "\n",
        "        tr_loss, tr_ppl = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
        "        val_loss, val_ppl = validate_model(model, val_loader, loss_fn, device)\n",
        "\n",
        "        results['epochs'].append(epoch)\n",
        "        results['train_loss'].append(tr_loss)\n",
        "        results['train_ppl'].append(tr_ppl)\n",
        "        results['val_loss'].append(val_loss)\n",
        "        results['val_ppl'].append(val_ppl)\n",
        "\n",
        "        print(f\"train: loss={tr_loss:.4f}, ppl={tr_ppl:.4f}\")\n",
        "        print(f\"val:   loss={val_loss:.4f}, ppl={val_ppl:.4f}\")\n",
        "\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            best_epoch = epoch\n",
        "            torch.save(model.state_dict(), f'/content/drive/MyDrive/ANLP/project_1/best_model_{exp_name}.pth')\n",
        "            print(\"saved best model\")\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            torch.save(model.state_dict(), f'/content/drive/MyDrive/ANLP/project_1/checkpoint_{exp_name}_epoch{epoch}.pth')\n",
        "\n",
        "    print(\"testing...\")\n",
        "    test_loss, test_ppl = validate_model(model, test_loader, loss_fn, device)\n",
        "    print(f\"test: loss={test_loss:.4f}, ppl={test_ppl:.4f}\")\n",
        "\n",
        "    results['best_val'] = best_val\n",
        "    results['best_epoch'] = best_epoch\n",
        "    results['test_loss'] = test_loss\n",
        "    results['test_ppl'] = test_ppl\n",
        "\n",
        "    with open(f'results_{exp_name}.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(\"saved results json\")\n",
        "    return results\n",
        "\n",
        "\n",
        "# compare all\n",
        "def compare_all_experiments(all_results, names):\n",
        "    print(\"\\n--- comparison ---\")\n",
        "    print(f\"{'exp':<20} {'best val':<10} {'test loss':<10} {'test ppl':<10}\")\n",
        "\n",
        "    for name, res in zip(names, all_results):\n",
        "        best_val = min(res['val_loss'])\n",
        "        print(f\"{name:<20} {best_val:<10.4f} {res['test_loss']:<10.4f} {res['test_ppl']:<10.4f}\")\n",
        "\n",
        "\n",
        "# plot results\n",
        "def plot_results(all_results, names):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    for res, name in zip(all_results, names):\n",
        "        ax1.plot(res['epochs'], res['train_loss'], label=f\"{name} train\", marker='o')\n",
        "        ax1.plot(res['epochs'], res['val_loss'], label=f\"{name} val\", linestyle='--')\n",
        "\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('loss')\n",
        "    ax1.set_title('loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    for res, name in zip(all_results, names):\n",
        "        ax2.plot(res['epochs'], res['train_ppl'], label=f\"{name} train\", marker='o')\n",
        "        ax2.plot(res['epochs'], res['val_ppl'], label=f\"{name} val\", linestyle='--')\n",
        "\n",
        "    ax2.set_xlabel('epoch')\n",
        "    ax2.set_ylabel('ppl')\n",
        "    ax2.set_title('perplexity')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/ANLP/project_1/experiment_comparison.png', dpi=200)\n",
        "    print(\"saved comparison graph\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# main\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ready to train\")\n",
        "    print(\"cross check your loaders + model before running\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrxso7JKyzFQ",
        "outputId": "aa1c6bc7-0940-40ad-c408-cc955e26c8ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking prerequisites...\n",
            "train_loader exists: True\n",
            "val_loader exists: True\n",
            "test_loader exists: True\n",
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if these exist in your notebook\n",
        "print(\"Checking prerequisites...\")\n",
        "print(f\"train_loader exists: {'train_loader' in dir()}\")\n",
        "print(f\"val_loader exists: {'val_loader' in dir()}\")\n",
        "print(f\"test_loader exists: {'test_loader' in dir()}\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xd4CGlr3D8RG",
        "outputId": "9be0117d-39dd-4ee3-be81-6fc310c55cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Exp1:Baseline\n",
            "\n",
            ">>> running Exp1_Baseline\n",
            "\n",
            "epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=5.5576, ppl=259.2112\n",
            "val:   loss=5.1378, ppl=170.3350\n",
            "saved best model\n",
            "\n",
            "epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.8146, ppl=123.3033\n",
            "val:   loss=4.5207, ppl=91.8962\n",
            "saved best model\n",
            "\n",
            "epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.2095, ppl=67.3243\n",
            "val:   loss=4.1634, ppl=64.2927\n",
            "saved best model\n",
            "\n",
            "epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.7516, ppl=42.5894\n",
            "val:   loss=3.8762, ppl=48.2397\n",
            "saved best model\n",
            "\n",
            "epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.4061, ppl=30.1466\n",
            "val:   loss=3.6679, ppl=39.1701\n",
            "saved best model\n",
            "\n",
            "epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.1258, ppl=22.7773\n",
            "val:   loss=3.4873, ppl=32.6964\n",
            "saved best model\n",
            "\n",
            "epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.8969, ppl=18.1172\n",
            "val:   loss=3.3441, ppl=28.3352\n",
            "saved best model\n",
            "\n",
            "epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.7047, ppl=14.9500\n",
            "val:   loss=3.2269, ppl=25.2024\n",
            "saved best model\n",
            "\n",
            "epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.5460, ppl=12.7557\n",
            "val:   loss=3.1621, ppl=23.6192\n",
            "saved best model\n",
            "\n",
            "epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.3867, ppl=10.8781\n",
            "val:   loss=3.1394, ppl=23.0889\n",
            "saved best model\n",
            "\n",
            "epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.2744, ppl=9.7219\n",
            "val:   loss=3.0618, ppl=21.3659\n",
            "saved best model\n",
            "\n",
            "epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.1559, ppl=8.6360\n",
            "val:   loss=3.0152, ppl=20.3932\n",
            "saved best model\n",
            "\n",
            "epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.0578, ppl=7.8286\n",
            "val:   loss=2.9774, ppl=19.6374\n",
            "saved best model\n",
            "\n",
            "epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=1.9690, ppl=7.1633\n",
            "val:   loss=2.9457, ppl=19.0238\n",
            "saved best model\n",
            "\n",
            "epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=1.8825, ppl=6.5698\n",
            "val:   loss=2.9478, ppl=19.0631\n",
            "\n",
            "epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=1.8021, ppl=6.0623\n",
            "val:   loss=2.9304, ppl=18.7344\n",
            "saved best model\n",
            "\n",
            "epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=1.7321, ppl=5.6524\n",
            "val:   loss=2.9522, ppl=19.1472\n",
            "\n",
            "epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=1.6644, ppl=5.2823\n",
            "val:   loss=2.9146, ppl=18.4423\n",
            "saved best model\n",
            "\n",
            "epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=1.6129, ppl=5.0175\n",
            "val:   loss=2.9164, ppl=18.4753\n",
            "\n",
            "epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=1.5430, ppl=4.6786\n",
            "val:   loss=2.9170, ppl=18.4856\n",
            "testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test: loss=2.8804, ppl=17.8216\n",
            "saved results json\n",
            "Exp1 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "print(\"\\nRunning Exp1:Baseline\")\n",
        "\n",
        "exp_name = 'Exp1_Baseline'\n",
        "cfg = experiments[exp_name]\n",
        "\n",
        "encoder = Encoder(\n",
        "    vocab_size=560,\n",
        "    embedding_dim=cfg['embedding_dim'],\n",
        "    hidden_size=cfg['hidden_size'],\n",
        "    num_layers=2,\n",
        "    dropout=cfg['dropout']\n",
        ").to(device)\n",
        "\n",
        "decoder = Decoder(\n",
        "    vocab_size=534,\n",
        "    embedding_dim=cfg['embedding_dim'],\n",
        "    hidden_size=cfg['hidden_size'],\n",
        "    encoder_hidden_size=cfg['hidden_size'],\n",
        "    num_layers=4,\n",
        "    dropout=cfg['dropout']\n",
        ").to(device)\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "\n",
        "res1 = run_one_experiment(exp_name, cfg, model, train_loader, val_loader, test_loader, device)\n",
        "print(\"Exp1 done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N4MiSPiMEBR7",
        "outputId": "adf71a8d-372c-4acd-8ab3-87c74709dd5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run Exp2: Bigger Model\n",
            "\n",
            ">>> running Exp2_BiggerModel\n",
            "\n",
            "epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=5.6960, ppl=297.6622\n",
            "val:   loss=5.4196, ppl=225.7979\n",
            "saved best model\n",
            "\n",
            "epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=5.2030, ppl=181.8220\n",
            "val:   loss=4.9607, ppl=142.6924\n",
            "saved best model\n",
            "\n",
            "epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.8086, ppl=122.5600\n",
            "val:   loss=4.6332, ppl=102.8472\n",
            "saved best model\n",
            "\n",
            "epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.4974, ppl=89.7834\n",
            "val:   loss=4.3875, ppl=80.4396\n",
            "saved best model\n",
            "\n",
            "epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.2149, ppl=67.6864\n",
            "val:   loss=4.2117, ppl=67.4685\n",
            "saved best model\n",
            "\n",
            "epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.9697, ppl=52.9696\n",
            "val:   loss=4.0720, ppl=58.6767\n",
            "saved best model\n",
            "\n",
            "epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.7736, ppl=43.5383\n",
            "val:   loss=3.9218, ppl=50.4909\n",
            "saved best model\n",
            "\n",
            "epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.6063, ppl=36.8298\n",
            "val:   loss=3.8140, ppl=45.3307\n",
            "saved best model\n",
            "\n",
            "epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.4560, ppl=31.6902\n",
            "val:   loss=3.7126, ppl=40.9598\n",
            "saved best model\n",
            "\n",
            "epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.3171, ppl=27.5816\n",
            "val:   loss=3.6016, ppl=36.6578\n",
            "saved best model\n",
            "\n",
            "epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.1929, ppl=24.3585\n",
            "val:   loss=3.5401, ppl=34.4697\n",
            "saved best model\n",
            "\n",
            "epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.0878, ppl=21.9290\n",
            "val:   loss=3.4537, ppl=31.6175\n",
            "saved best model\n",
            "\n",
            "epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.9838, ppl=19.7623\n",
            "val:   loss=3.3919, ppl=29.7219\n",
            "saved best model\n",
            "\n",
            "epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.8862, ppl=17.9254\n",
            "val:   loss=3.3460, ppl=28.3901\n",
            "saved best model\n",
            "\n",
            "epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.7951, ppl=16.3649\n",
            "val:   loss=3.2975, ppl=27.0450\n",
            "saved best model\n",
            "\n",
            "epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.7196, ppl=15.1741\n",
            "val:   loss=3.2478, ppl=25.7334\n",
            "saved best model\n",
            "\n",
            "epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.6438, ppl=14.0659\n",
            "val:   loss=3.2180, ppl=24.9789\n",
            "saved best model\n",
            "\n",
            "epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.5760, ppl=13.1445\n",
            "val:   loss=3.1664, ppl=23.7212\n",
            "saved best model\n",
            "\n",
            "epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.4972, ppl=12.1485\n",
            "val:   loss=3.1348, ppl=22.9841\n",
            "saved best model\n",
            "\n",
            "epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=2.4388, ppl=11.4594\n",
            "val:   loss=3.0904, ppl=21.9851\n",
            "saved best model\n",
            "testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test: loss=3.0773, ppl=21.7008\n",
            "saved results json\n",
            "Exp2 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "print(\"\\nRun Exp2: Bigger Model\")\n",
        "\n",
        "exp_name = 'Exp2_BiggerModel'\n",
        "cfg = experiments[exp_name]\n",
        "\n",
        "encoder = Encoder(\n",
        "    vocab_size=560,\n",
        "    embedding_dim=cfg['embedding_dim'],\n",
        "    hidden_size=cfg['hidden_size'],\n",
        "    num_layers=2,\n",
        "    dropout=cfg['dropout']\n",
        ").to(device)\n",
        "\n",
        "decoder = Decoder(\n",
        "    vocab_size=534,\n",
        "    embedding_dim=cfg['embedding_dim'],\n",
        "    hidden_size=cfg['hidden_size'],\n",
        "    encoder_hidden_size=cfg['hidden_size'],\n",
        "    num_layers=4,\n",
        "    dropout=cfg['dropout']\n",
        ").to(device)\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "\n",
        "res2 = run_one_experiment(exp_name, cfg, model, train_loader, val_loader, test_loader, device)\n",
        "print(\"Exp2 done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pIUcTFEEFEU",
        "outputId": "21e3ef3e-bc3a-4985-ba0d-4d7a7597cae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run Exp3: Small & Fast \n",
            "\n",
            ">>> running Exp3_SmallAndFast\n",
            "\n",
            "epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=5.7834, ppl=324.8645\n",
            "val:   loss=5.6942, ppl=297.1411\n",
            "saved best model\n",
            "\n",
            "epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=5.5961, ppl=269.3670\n",
            "val:   loss=5.4812, ppl=240.1464\n",
            "saved best model\n",
            "\n",
            "epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=5.3407, ppl=208.6489\n",
            "val:   loss=5.2486, ppl=190.2952\n",
            "saved best model\n",
            "\n",
            "epoch 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=5.1125, ppl=166.0870\n",
            "val:   loss=5.0656, ppl=158.4827\n",
            "saved best model\n",
            "\n",
            "epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.9192, ppl=136.8864\n",
            "val:   loss=4.9083, ppl=135.4027\n",
            "saved best model\n",
            "\n",
            "epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.7524, ppl=115.8636\n",
            "val:   loss=4.7788, ppl=118.9587\n",
            "saved best model\n",
            "\n",
            "epoch 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.5977, ppl=99.2554\n",
            "val:   loss=4.6694, ppl=106.6351\n",
            "saved best model\n",
            "\n",
            "epoch 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.4544, ppl=86.0057\n",
            "val:   loss=4.5578, ppl=95.3738\n",
            "saved best model\n",
            "\n",
            "epoch 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.3177, ppl=75.0158\n",
            "val:   loss=4.4667, ppl=87.0657\n",
            "saved best model\n",
            "\n",
            "epoch 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.1877, ppl=65.8720\n",
            "val:   loss=4.3733, ppl=79.3018\n",
            "saved best model\n",
            "\n",
            "epoch 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=4.0645, ppl=58.2374\n",
            "val:   loss=4.2894, ppl=72.9259\n",
            "saved best model\n",
            "\n",
            "epoch 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.9405, ppl=51.4448\n",
            "val:   loss=4.2141, ppl=67.6320\n",
            "saved best model\n",
            "\n",
            "epoch 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.8234, ppl=45.7583\n",
            "val:   loss=4.1509, ppl=63.4934\n",
            "saved best model\n",
            "\n",
            "epoch 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.7068, ppl=40.7235\n",
            "val:   loss=4.0707, ppl=58.6004\n",
            "saved best model\n",
            "\n",
            "epoch 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.5906, ppl=36.2554\n",
            "val:   loss=4.0116, ppl=55.2358\n",
            "saved best model\n",
            "\n",
            "epoch 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.4806, ppl=32.4777\n",
            "val:   loss=3.9682, ppl=52.8893\n",
            "saved best model\n",
            "\n",
            "epoch 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.3671, ppl=28.9944\n",
            "val:   loss=3.9065, ppl=49.7251\n",
            "saved best model\n",
            "\n",
            "epoch 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.2584, ppl=26.0083\n",
            "val:   loss=3.8718, ppl=48.0297\n",
            "saved best model\n",
            "\n",
            "epoch 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.1552, ppl=23.4580\n",
            "val:   loss=3.8267, ppl=45.9120\n",
            "saved best model\n",
            "\n",
            "epoch 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: loss=3.0512, ppl=21.1403\n",
            "val:   loss=3.8176, ppl=45.4939\n",
            "saved best model\n",
            "testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test: loss=3.8031, ppl=44.8386\n",
            "saved results json\n",
            "Exp3 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "print(\"\\nRun Exp3: Small & Fast \")\n",
        "\n",
        "exp_name = 'Exp3_SmallAndFast'\n",
        "cfg = experiments[exp_name]\n",
        "\n",
        "encoder = Encoder(\n",
        "    vocab_size=560,\n",
        "    embedding_dim=cfg['embedding_dim'],\n",
        "    hidden_size=cfg['hidden_size'],\n",
        "    num_layers=2,\n",
        "    dropout=cfg['dropout']\n",
        ").to(device)\n",
        "\n",
        "decoder = Decoder(\n",
        "    vocab_size=534,\n",
        "    embedding_dim=cfg['embedding_dim'],\n",
        "    hidden_size=cfg['hidden_size'],\n",
        "    encoder_hidden_size=cfg['hidden_size'],\n",
        "    num_layers=4,\n",
        "    dropout=cfg['dropout']\n",
        ").to(device)\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "\n",
        "res3 = run_one_experiment(exp_name, cfg, model, train_loader, val_loader, test_loader, device)\n",
        "print(\"Exp3 done\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrSxGlKGK81W"
      },
      "source": [
        "blu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qKkq0BY4DDF",
        "outputId": "02ac3480-f773-4f76-a8ed-9c3dbee1d8e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing editdistance...\n",
            "already installed, cool!\n",
            "\n",
            "Starting evaluation...\n",
            "functions created!\n",
            "translation func done!\n",
            "\n",
            "loading best model--\n",
            "model loaded!\n",
            "\n",
            "testing model on examples...\n",
            "testing 20 examples...\n",
            "\n",
            "done 5 examples...\n",
            "done 10 examples...\n",
            "done 15 examples...\n",
            "done 20 examples...\n",
            "\n",
            "testing complete!\n",
            "conclusion\n",
            "\n",
            "Average BLEU: 0.0147\n",
            "Average CER:  3.8597\n",
            "Perplexity:   17.53\n",
            "\n",
            "what these mean:\n",
            "BLEU = 0.0147 (closer to 1.0 is better)\n",
            "CER = 3.8597 (closer to 0.0 is better)\n",
            "transaltion\n",
            "\n",
            "Example 1:\n",
            "Urdu:      کچھ تو یہ ہے کہ مری راہ جدا ہے تجھ سے\n",
            "Expected:  kuchh to ye hai ki miri raah juda hai tujh se\n",
            "Got:       kuchh to ye hai ki ki jajuda hai tujh se se se se se tha tha tha tha tha tha hain hain hain tha tha haath hain tha haath haath tha tha haath haath tha tha haath haath tha tha haath haath tha haath haath tha haath haath\n",
            "BLEU: 0.095, CER: 4.067\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 2:\n",
            "Urdu:      غم ترا جلوہ گہہ کون و مکاں ہے کہ جو تھا\n",
            "Expected:  gham tira jalva-gah-e-kaun-o-makan hai ki jo tha\n",
            "Got:       gham jaljaltugaah guah kamaman hai ki ki tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha aaya tha aaya tha\n",
            "BLEU: 0.000, CER: 3.271\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 3:\n",
            "Urdu:      ہائے کیا اچھی کہی ظالم ہوں میں جاہل ہوں میں\n",
            "Expected:  haae kya achchhi kahi zalim huun main jahil huun main\n",
            "Got:       haahi hi hi hi hi hi iim main hi jajahuun main main main main huun main huun huun huun huun men huun huun men huun huun men huun huun men huun huun huun men huun huun huun huun huun huun huun men huun huun gi\n",
            "BLEU: 0.000, CER: 3.226\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 4:\n",
            "Urdu:      جام داغ شعلہ اندود چراغ کشتہ ہے\n",
            "Expected:  jaam dagh-e-shola-andud-e-charagh-e-kushta hai\n",
            "Got:       jam-e-m-e-m-e-jum-e-andad-e-dugh-e-dugh-e-kushta hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai haath hai hai haath hai hai haath hai hai haath hai hai haath hai\n",
            "BLEU: 0.000, CER: 3.587\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 5:\n",
            "Urdu:      شکل انسان کی ہو چال بھی انسان کی ہو\n",
            "Expected:  shakl insan ki ho chaal bhi insan ki ho\n",
            "Got:       shikasan san san ki ho chasan ki chasan ki ho ki ho ho ka ho ho ka ho ho ka ho gai gai gai o o o o o o o o oo o ooa o ooah o toah\n",
            "BLEU: 0.000, CER: 2.513\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 6:\n",
            "Urdu:      میں اپنا جان و دل قرباں کروں اوس پر سیتی ناجی\n",
            "Expected:  main apna jaan o dil qurban karun uus par seti naji\n",
            "Got:       main apna daro darqaaon men asss sii hai saji ji i i i i kahin kahin i i i i i kahin nain in i i i i in in in i i nahin nain in\n",
            "BLEU: 0.000, CER: 1.863\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 7:\n",
            "Urdu:      مجلس میں شیخ صاحب کچھ کود جانتے ہیں\n",
            "Expected:  majlis men shaikh-sahib kuchh kuud jante hain\n",
            "Got:       lls men shaikh iikh men kuchh jante jante hain hain hain hain hain hain hain tin hain tte hain lam hain qi qi hain aaya aaya aaya aaya aaya aaya aaya aaya aaya hain lam m hain lam hain m\n",
            "BLEU: 0.000, CER: 3.489\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 8:\n",
            "Urdu:      کہتے ہو نہ دیں گے ہم دل اگر پڑا پایا\n",
            "Expected:  kahte ho na denge ham dil agar pada paaya\n",
            "Got:       kahte ho ho dange ham ham dil kyuun papaha de ya ya ya hai ya hai mire mire hai hai hai hai hai hai hai hai haath hai haath hai haath haath hai haath haath hai haath haath hai haath haath hai haath ahi hai haath ahi\n",
            "BLEU: 0.000, CER: 4.439\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 9:\n",
            "Urdu:      رشک کہتا ہے کہ اس کا غیر سے اخلاص حیف\n",
            "Expected:  rashk kahta hai ki us ka ghair se ikhlas haif\n",
            "Got:       rushta kahta hai ki us ka khakhalalalas hahaf f f f hai magar magar magar hai magar magar hai n men nahin n nahin n bhi nahin kam n nahin n n nahin kam n nahin kam n nahin n\n",
            "BLEU: 0.086, CER: 2.978\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 10:\n",
            "Urdu:      ان سے اپنا غم بیاں اب ہم کو کرنا ہی نہیں\n",
            "Expected:  un se apna gham bayan ab ham ko karna hi nahin\n",
            "Got:       un se apna apna ghain ham ham ko karna nahin nahin dekha nahin hai nahin nahin nahin nahin nahin nahin nahin nahin main nahin nahin main main hai main m men ti men ham men hain ham men kiya kiya men kiya kiya men kiya kiya men\n",
            "BLEU: 0.000, CER: 4.065\n",
            "------------------------------------------------------------\n",
            "\n",
            "saving results...\n",
            "saved to eval_results.json\n",
            "its donee!\n",
            "\n",
            "Your scores:\n",
            "BLEU: 0.0147\n",
            "CER:  3.8597\n"
          ]
        }
      ],
      "source": [
        "# okay so first i need to install this package for character errors\n",
        "print(\"installing editdistance...\")\n",
        "try:\n",
        "    import editdistance\n",
        "    print(\"already installed, cool!\")\n",
        "except:\n",
        "    !pip install editdistance\n",
        "    import editdistance\n",
        "\n",
        "import math\n",
        "import json\n",
        "\n",
        "print(\"\\nStarting evaluation...\")\n",
        "\n",
        "# BLEU Score Function this checks how good our translations are\n",
        "\n",
        "def calculate_bleu(correct, predicted):\n",
        "\n",
        " #   BLEU score - basically checks how many words match\n",
        "#    1.0 = perfect, 0.0 = completely wrong\n",
        "\n",
        "\n",
        "    # make everything lowercase and split into words\n",
        "    correct_words = correct.lower().split()\n",
        "    predicted_words = predicted.lower().split()\n",
        "\n",
        "    # if we didn't predict anything, score is zero\n",
        "    if len(predicted_words) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # check 1-word, 2-word, 3-word, 4-word matches\n",
        "    scores = []\n",
        "\n",
        "    for n in [1,2,3,4]:  # n-gram sizes\n",
        "        # get all n-grams from correct answer\n",
        "        correct_ngrams = []\n",
        "        for i in range(len(correct_words) - n + 1):\n",
        "            ngram = tuple(correct_words[i:i+n])\n",
        "            correct_ngrams.append(ngram)\n",
        "\n",
        "        # get all n-grams from our prediction\n",
        "        pred_ngrams = []\n",
        "        for i in range(len(predicted_words) - n + 1):\n",
        "            ngram = tuple(predicted_words[i:i+n])\n",
        "            pred_ngrams.append(ngram)\n",
        "\n",
        "        # count matches\n",
        "        matches = 0\n",
        "        correct_copy = correct_ngrams.copy()\n",
        "        for ngram in pred_ngrams:\n",
        "            if ngram in correct_copy:\n",
        "                matches += 1\n",
        "                correct_copy.remove(ngram)  # dont count twice\n",
        "\n",
        "        # calculate precision for this n\n",
        "        if len(pred_ngrams) > 0:\n",
        "            precision = matches / len(pred_ngrams)\n",
        "        else:\n",
        "            precision = 0\n",
        "\n",
        "        scores.append(precision)\n",
        "\n",
        "    # if any score is 0, bleu is 0\n",
        "    if min(scores) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # calculate geometric mean (dont worry about the math)\n",
        "    bleu = 1.0\n",
        "    for s in scores:\n",
        "        bleu = bleu * s\n",
        "    bleu = bleu ** (1/4)  # fourth root\n",
        "\n",
        "    return bleu\n",
        "\n",
        "\n",
        "def calculate_cer(correct, predicted):\n",
        "\n",
        "\n",
        "    distance = editdistance.eval(correct, predicted)\n",
        "\n",
        "    if len(correct) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    cer = distance / len(correct)\n",
        "    return cer\n",
        "\n",
        "print(\"functions created!\")\n",
        "\n",
        "# takes urdu tokens and gives back roman urdu text\n",
        "\n",
        "def translate(model, urdu_tokens, roman_bpe, device):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # prepare input\n",
        "        src = torch.tensor([urdu_tokens]).to(device)\n",
        "        src_len = torch.tensor([len(urdu_tokens)]).to(device)\n",
        "\n",
        "        # encode urdu\n",
        "        enc_outputs, enc_hidden, enc_cell = model.encoder(src, src_len)\n",
        "\n",
        "        # initialize decoder\n",
        "        dec_h, dec_c = model._init_decoder_states(enc_hidden, enc_cell, 1)\n",
        "\n",
        "        # start token\n",
        "        current = torch.tensor([[2]]).to(device)  # 2 = <SOS>\n",
        "\n",
        "        # generate tokens\n",
        "        output_tokens = []\n",
        "        for _ in range(50):  # max 50 tokens\n",
        "            logits, dec_h, dec_c = model.decoder(current, dec_h, dec_c, enc_outputs, src_len)\n",
        "            next_token = logits.argmax(-1).item()\n",
        "\n",
        "            # stop if end token or padding\n",
        "            if next_token == 3 or next_token == 0:\n",
        "                break\n",
        "\n",
        "            output_tokens.append(next_token)\n",
        "            current = torch.tensor([[next_token]]).to(device)\n",
        "\n",
        "        # decode to text\n",
        "        try:\n",
        "            text = roman_bpe.decode(output_tokens)\n",
        "        except:\n",
        "            text = \"\"\n",
        "\n",
        "        return text\n",
        "\n",
        "print(\"translation func done!\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nloading best model--\")\n",
        "\n",
        "# create encoder\n",
        "encoder = Encoder(\n",
        "    vocab_size=560,\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "# create decoder\n",
        "decoder = Decoder(\n",
        "    vocab_size=534,\n",
        "    embedding_dim=256,\n",
        "    hidden_size=256,\n",
        "    num_layers=4,\n",
        "    dropout=0.3,\n",
        "    encoder_hidden_size=256 # Added the missing argument\n",
        ").to(device)\n",
        "\n",
        "# combine them\n",
        "model = Seq2SeqNMT(encoder, decoder, device).to(device)\n",
        "\n",
        "# load trained weights\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.eval()\n",
        "\n",
        "print(\"model loaded!\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\ntesting model on examples...\")\n",
        "\n",
        "# im testing on 20 examples (can change this)\n",
        "num_test = 20\n",
        "\n",
        "bleu_scores = []\n",
        "cer_scores = []\n",
        "examples = []  # save some examples to show\n",
        "\n",
        "print(f\"testing {num_test} examples...\\n\")\n",
        "\n",
        "for i in range(num_test):\n",
        "\n",
        "    # get test example\n",
        "    sample = test_dataset[i]\n",
        "    urdu_toks = sample['urdu'].tolist()\n",
        "    roman_toks = sample['roman'].tolist()\n",
        "\n",
        "    # clean tokens (remove padding etc)\n",
        "    urdu_clean = [t for t in urdu_toks if t not in [0,2,3]]\n",
        "    roman_clean = [t for t in roman_toks if t not in [0,2,3]]\n",
        "\n",
        "    # decode to text\n",
        "    try:\n",
        "        urdu_text = urdu_bpe.decode(urdu_clean)\n",
        "        correct_roman = roman_bpe.decode(roman_clean)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    if not correct_roman.strip():\n",
        "        continue\n",
        "\n",
        "    # translate using model\n",
        "    predicted_roman = translate(model, urdu_toks, roman_bpe, device)\n",
        "\n",
        "    # calculate scores\n",
        "    bleu = calculate_bleu(correct_roman, predicted_roman)\n",
        "    cer = calculate_cer(correct_roman, predicted_roman)\n",
        "\n",
        "    bleu_scores.append(bleu)\n",
        "    cer_scores.append(cer)\n",
        "\n",
        "    # save first 10 for showing later\n",
        "    if len(examples) < 10:\n",
        "        examples.append({\n",
        "            'urdu': urdu_text,\n",
        "            'correct': correct_roman,\n",
        "            'predicted': predicted_roman,\n",
        "            'bleu': bleu,\n",
        "            'cer': cer\n",
        "        })\n",
        "\n",
        "    # show progress every 5 examples\n",
        "    if (i+1) % 5 == 0:\n",
        "        print(f\"done {i+1} examples...\")\n",
        "\n",
        "print(f\"\\ntesting complete!\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"conclusion\")\n",
        "\n",
        "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "avg_cer = sum(cer_scores) / len(cer_scores)\n",
        "\n",
        "print(f\"\\nAverage BLEU: {avg_bleu:.4f}\")\n",
        "print(f\"Average CER:  {avg_cer:.4f}\")\n",
        "print(f\"Perplexity:   17.53\")\n",
        "\n",
        "print(f\"\\nwhat these mean:\")\n",
        "print(f\"BLEU = {avg_bleu:.4f} (closer to 1.0 is better)\")\n",
        "print(f\"CER = {avg_cer:.4f} (closer to 0.0 is better)\")\n",
        "\n",
        "print(\"transaltion\")\n",
        "\n",
        "for idx, ex in enumerate(examples, 1):\n",
        "    print(f\"\\nExample {idx}:\")\n",
        "    print(f\"Urdu:      {ex['urdu']}\")\n",
        "    print(f\"Expected:  {ex['correct']}\")\n",
        "    print(f\"Got:       {ex['predicted']}\")\n",
        "    print(f\"BLEU: {ex['bleu']:.3f}, CER: {ex['cer']:.3f}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "\n",
        "print(\"\\nsaving results...\")\n",
        "\n",
        "results = {\n",
        "    'model': 'Exp1_Baseline',\n",
        "    'bleu': avg_bleu,\n",
        "    'cer': avg_cer,\n",
        "    'perplexity': 17.53,\n",
        "    'num_examples': len(bleu_scores),\n",
        "    'examples': examples\n",
        "}\n",
        "\n",
        "# save to file\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/eval_results.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"saved to eval_results.json\")\n",
        "\n",
        "print(\"its donee!\")\n",
        "print(f\"\\nYour scores:\")\n",
        "print(f\"BLEU: {avg_bleu:.4f}\")\n",
        "print(f\"CER:  {avg_cer:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lGEAeGYJK-YI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d88f0a8-2163-46f7-d699-ec04b5cb93dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BPE tokenizer objects...\n",
            "BPE tokenizer objects loaded!\n",
            "creating model with baseline settings...\n",
            "loading best model...\n",
            "model loaded successfully!\n",
            "\n",
            "calculating bleu score...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.61\n",
            "\n",
            "calculating cer...\n",
            "CER: 380.02%\n",
            "Sample Translations:\n",
            "\n",
            "1.\n",
            "Urdu:      کچھ تو یہ ہے کہ مری راہ جدا ہے تجھ سے\n",
            "Actual:    kuchh to ye hai ki miri raah juda hai tujh se\n",
            "Predicted: kuchh to ye hai ki ki jajuda hai tujh se se se se se tha tha tha tha tha tha hain hain hain tha tha haath hain tha haath haath tha tha haath haath tha tha haath haath tha tha haath haath tha haath haath tha haath haath\n",
            "\n",
            "2.\n",
            "Urdu:      غم ترا جلوہ گہہ کون و مکاں ہے کہ جو تھا\n",
            "Actual:    gham tira jalva-gah-e-kaun-o-makan hai ki jo tha\n",
            "Predicted: gham jaljaltugaah guah kamaman hai ki ki tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha tha aaya tha aaya tha\n",
            "\n",
            "3.\n",
            "Urdu:      ہائے کیا اچھی کہی ظالم ہوں میں جاہل ہوں میں\n",
            "Actual:    haae kya achchhi kahi zalim huun main jahil huun main\n",
            "Predicted: haahi hi hi hi hi hi iim main hi jajahuun main main main main huun main huun huun huun huun men huun huun men huun huun men huun huun men huun huun huun men huun huun huun huun huun huun huun men huun huun gi\n",
            "\n",
            "4.\n",
            "Urdu:      جام داغ شعلہ اندود چراغ کشتہ ہے\n",
            "Actual:    jaam dagh-e-shola-andud-e-charagh-e-kushta hai\n",
            "Predicted: jam-e-m-e-m-e-jum-e-andad-e-dugh-e-dugh-e-kushta hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai hai haath hai hai haath hai hai haath hai hai haath hai hai haath hai\n",
            "\n",
            "5.\n",
            "Urdu:      شکل انسان کی ہو چال بھی انسان کی ہو\n",
            "Actual:    shakl insan ki ho chaal bhi insan ki ho\n",
            "Predicted: shikasan san san ki ho chasan ki chasan ki ho ki ho ho ka ho ho ka ho ho ka ho gai gai gai o o o o o o o o oo o ooa o ooah o toah\n"
          ]
        }
      ],
      "source": [
        "# install library\n",
        "!pip install python-Levenshtein -q\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import Levenshtein\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Load the trained BPE tokenizer objects\n",
        "print(\"Loading BPE tokenizer objects...\")\n",
        "urdu_bpe_tokenizer = BPETokenizer()\n",
        "roman_bpe_tokenizer = BPETokenizer()\n",
        "\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/urdu_tokenizer.pkl', 'rb') as f:\n",
        "    urdu_tokenizer_data = pickle.load(f)\n",
        "urdu_bpe_tokenizer.vocab = urdu_tokenizer_data['vocablry']\n",
        "urdu_bpe_tokenizer.merges = urdu_tokenizer_data['merges']\n",
        "urdu_bpe_tokenizer.special_tokens = urdu_tokenizer_data['special tokens']\n",
        "\n",
        "with open('/content/drive/MyDrive/ANLP/project_1/roman_tokenizer.pkl', 'rb') as f:\n",
        "    roman_tokenizer_data = pickle.load(f)\n",
        "roman_bpe_tokenizer.vocab = roman_tokenizer_data['vocablry']\n",
        "roman_bpe_tokenizer.merges = roman_tokenizer_data['merges']\n",
        "roman_bpe_tokenizer.special_tokens = roman_tokenizer_data['special tokens']\n",
        "print(\"BPE tokenizer objects loaded!\")\n",
        "\n",
        "\n",
        "# Function to translate a sequence of Urdu token IDs to Roman Urdu text\n",
        "def translate(model, urdu_token_ids, roman_bpe_tokenizer_obj, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = torch.tensor([urdu_token_ids]).to(device)\n",
        "        src_len = torch.tensor([len(urdu_token_ids)]).to(device)\n",
        "\n",
        "        # Encode Urdu\n",
        "        enc_outputs, enc_hidden, enc_cell = model.encoder(src, src_len)\n",
        "\n",
        "        # Initialize decoder states using the correct method from Seq2SeqNMT\n",
        "        dec_h, dec_c = model._init_decoder_states(enc_hidden, enc_cell, 1)\n",
        "\n",
        "        # Start token ID for Roman Urdu (assuming '<strt>' is ID 2 in BPETokenizer vocab)\n",
        "        start_token_id = roman_bpe_tokenizer_obj.vocab.get('<strt>', 2) # Default to 2 if not found\n",
        "        current_token_id = torch.tensor([[start_token_id]]).to(device)\n",
        "\n",
        "        output_tokens = []\n",
        "        max_seq_len = 50 # Max length for generated sequence (can be adjusted)\n",
        "\n",
        "        for _ in range(max_seq_len):\n",
        "            # Pass encoder_outputs and src_len to the decoder (required for attention)\n",
        "            logits, dec_h, dec_c = model.decoder(current_token_id, dec_h, dec_c, enc_outputs, src_len)\n",
        "            next_token_id = logits.argmax(-1).item()\n",
        "\n",
        "            # Stop if <end> token (3) or <pad> token (0) is predicted\n",
        "            end_token_id = roman_bpe_tokenizer_obj.vocab.get('<end>', 3) # Default to 3 if not found\n",
        "            pad_token_id = roman_bpe_tokenizer_obj.vocab.get('<pad>', 0) # Default to 0 if not found\n",
        "\n",
        "            if next_token_id == end_token_id or next_token_id == pad_token_id:\n",
        "                break\n",
        "\n",
        "            output_tokens.append(next_token_id)\n",
        "            current_token_id = torch.tensor([[next_token_id]]).to(device)\n",
        "\n",
        "        # Decode the list of token IDs back to a string\n",
        "        return roman_bpe_tokenizer_obj.decode(output_tokens)\n",
        "\n",
        "\n",
        "# calculate bleu score\n",
        "def calculate_bleu(model, test_urdu_token_lists, test_roman_token_lists, urdu_bpe_tokenizer_obj, roman_bpe_tokenizer_obj, device):\n",
        "    model.eval()\n",
        "    bleu_scores = []\n",
        "    # Evaluate on a subset of the test data (e.g., first 100 samples) for efficiency\n",
        "    for i in range(min(100, len(test_urdu_token_lists))):\n",
        "        src_token_ids = test_urdu_token_lists[i]\n",
        "        tgt_token_ids = test_roman_token_lists[i]\n",
        "\n",
        "        # Translate using the model\n",
        "        predicted_roman_text = translate(model, src_token_ids, roman_bpe_tokenizer_obj, device)\n",
        "\n",
        "        # Decode the actual Roman Urdu token IDs to text for comparison\n",
        "        correct_roman_text = roman_bpe_tokenizer_obj.decode(tgt_token_ids)\n",
        "\n",
        "        if not correct_roman_text.strip(): # Skip if the reference is empty after decoding\n",
        "            continue\n",
        "\n",
        "        # NLTK's sentence_bleu expects lists of words\n",
        "        reference = [correct_roman_text.split()]\n",
        "        hypothesis = predicted_roman_text.split()\n",
        "\n",
        "        # Handle empty hypothesis to avoid errors in sentence_bleu\n",
        "        if not hypothesis:\n",
        "            score = 0.0\n",
        "        else:\n",
        "            score = sentence_bleu(reference, hypothesis)\n",
        "        bleu_scores.append(score)\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
        "    return avg_bleu * 100 # Return as percentage\n",
        "\n",
        "\n",
        "# calculate cer\n",
        "def calculate_cer(model, test_urdu_token_lists, test_roman_token_lists, urdu_bpe_tokenizer_obj, roman_bpe_tokenizer_obj, device):\n",
        "    model.eval()\n",
        "    total_distance = 0\n",
        "    total_length = 0\n",
        "\n",
        "    for i in range(min(100, len(test_urdu_token_lists))):\n",
        "        src_token_ids = test_urdu_token_lists[i]\n",
        "        tgt_token_ids = test_roman_token_lists[i]\n",
        "\n",
        "        # Translate using the model\n",
        "        predicted_roman_text = translate(model, src_token_ids, roman_bpe_tokenizer_obj, device)\n",
        "\n",
        "        # Decode the actual Roman Urdu token IDs to text for comparison\n",
        "        correct_roman_text = roman_bpe_tokenizer_obj.decode(tgt_token_ids)\n",
        "\n",
        "        if not correct_roman_text.strip(): # Skip if the reference is empty after decoding\n",
        "            continue\n",
        "\n",
        "        # Calculate Levenshtein distance on strings\n",
        "        distance = Levenshtein.distance(predicted_roman_text, correct_roman_text)\n",
        "        total_distance += distance\n",
        "        total_length += len(correct_roman_text)\n",
        "\n",
        "    cer = (total_distance / total_length) * 100 if total_length > 0 else 0.0\n",
        "    return cer\n",
        "\n",
        "\n",
        "# create model with EXACT same settings as Exp1_Baseline\n",
        "print(\"creating model with baseline settings...\")\n",
        "embedding_dim = 256\n",
        "hidden_size = 256\n",
        "dropout = 0.3\n",
        "\n",
        "# Use the vocab sizes from the loaded tokenizer objects\n",
        "encoder = Encoder(\n",
        "    vocab_size=len(urdu_bpe_tokenizer.vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=2,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "decoder = Decoder(\n",
        "    vocab_size=len(roman_bpe_tokenizer.vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=4,\n",
        "    dropout=dropout,\n",
        "    encoder_hidden_size=hidden_size # Ensure this argument is passed as per model definition\n",
        ")\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, device)\n",
        "\n",
        "# load saved weights\n",
        "print(\"loading best model...\")\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ANLP/project_1/best_model_Exp1_Baseline.pth'))\n",
        "model.to(device)\n",
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "print(\"model loaded successfully!\")\n",
        "\n",
        "# calculate metrics\n",
        "print(\"\\ncalculating bleu score...\")\n",
        "# Pass the tokenized test data lists and tokenizer objects\n",
        "bleu = calculate_bleu(model, test_urdu, test_roman, urdu_bpe_tokenizer, roman_bpe_tokenizer, device)\n",
        "print(f\"BLEU Score: {bleu:.2f}\")\n",
        "\n",
        "print(\"\\ncalculating cer...\")\n",
        "# Pass the tokenized test data lists and tokenizer objects\n",
        "cer = calculate_cer(model, test_urdu, test_roman, urdu_bpe_tokenizer, roman_bpe_tokenizer, device)\n",
        "print(f\"CER: {cer:.2f}%\")\n",
        "\n",
        "# show some translations\n",
        "print(\"Sample Translations:\")\n",
        "\n",
        "for i in range(5):\n",
        "    src_token_ids = test_urdu[i]\n",
        "    tgt_token_ids = test_roman[i]\n",
        "\n",
        "    # Translate using the model\n",
        "    predicted_text = translate(model, src_token_ids, roman_bpe_tokenizer, device)\n",
        "\n",
        "    # Decode actual Urdu and Roman from token IDs to text for display\n",
        "    src_text = urdu_bpe_tokenizer.decode(src_token_ids)\n",
        "    actual_text = roman_bpe_tokenizer.decode(tgt_token_ids)\n",
        "\n",
        "    print(f\"\\n{i+1}.\")\n",
        "    print(f\"Urdu:      {src_text}\")\n",
        "    print(f\"Actual:    {actual_text}\")\n",
        "    print(f\"Predicted: {predicted_text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lC95CwHgRIFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf2258d-00e8-46ca-b2f9-2287f070fbad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Urdu vocab keys:\n",
            "['<pad>', '<unk>', '<strt>', '<end>', '!', \"'\", '</w>', '،', '؟', 'ء']\n",
            "\n",
            "Roman vocab keys:\n",
            "['<pad>', '<unk>', '<strt>', '<end>', '!', ',', '-', '-e-', '-e-a', '-o-']\n"
          ]
        }
      ],
      "source": [
        "# check your vocab keys\n",
        "print(\"Urdu vocab keys:\")\n",
        "print(list(urdu_vocab.keys())[:10])\n",
        "\n",
        "print(\"\\nRoman vocab keys:\")\n",
        "print(list(roman_vocab.keys())[:10])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}